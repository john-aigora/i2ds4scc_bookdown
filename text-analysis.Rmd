```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```
# Text Analysis {#text-analysis}

Humans exchange information through the use of languages. There is of course a very large number of different languages, each of them having their own specificity. The science that studies languages per se is called *linguistics*: It focuses on areas such as phonetics, phonology, morphology, syntax, semantics, and pragmatics.

Natural Language Processing (NLP) is a sub-field of linguistics, computer science, and artificial intelligence. It connects computers to human language by processing, analyzing, and modeling large amounts of natural language data. One of the main goals of NLP is to "understand" the contents of documents, and to extract accurately information and insights from those documents.

In Sensory and Consumer Research, *Text Analysis* usually refers to NLP.

Since the fields of linguistics and NLP are widely studied, a lot of documentations is already available [REF REF REF?]. 
The objective of this chapter is to provide sufficient information for you to be familiar with textual data, and to give you the keys to run the most useful analyses in Sensory and Consumer Research. For those who would like to dive deeper into NLP, we recommend reading (@Silge2017, @Becue-Bertaut2019), and (@Hvitfeldt2021) for more advanced techniques.

### Applicaion of Text Analysis in Sensory and Consumer Science

In recent years, open-ended comments have gained interest as it is the fastest, safest, most unbiased (and sometimes cheapest) way to collect spontaneous data from participants (REF Betina's chapter in Julien's book). 

At first, most S&C questionnaires primarily relied on closed questions, to which open-ended questions were added to uncover the consumers' reasons for liking or disliking products. In practice, these open-ended questions were positioned right after liking questions, and aimed in partially understanding why a product may or may not be liked, and to give the consumers the chance to reduce their frustration by explaining their responses to certain questions. Hence, these questions were not deeply analyzed.

With the development of the so-called *rapid* methods, the benefits of open-ended questions became more apparent as they provide a new way to uncover the perception of respondents. In practice, respondents are asked to freely list a number of terms that describe their sensory perception of the products. Such questions can either be coupled to intensity or ranking questions, (e.g. Free Choice Profile, Flash Profile), or justify the similarities and dissimilarities between products perceived by the respondents (e.g. Free Sorting Task, and Ultra Flash Profile  as an extension of Napping). Since the textual responses are now an integral part of the questionnaire, its analysis can no longer be ignored.

The importance of open-ended questions increased further as it has been shown that respondents can describe in their own words their full experience (perception, emotion, or any other sort of association) with products. Recently, Mahieu et al. showed the benefits of using open-ended questions over CATA^[CATA can be seen as a simplified version of free-comments in the sense that respondents also associate products to words, however they lose the freedom of using their own as they need to select them from a pre-defined list.]. In this study, consumers were asked to describe with their own words both the products and their ideal. Similarly, Luc et al. proposed an alternative to Just About Right (JAR) called free-JAR in which consumers described the samples using their own words, by still following a JAR terminology (too little, too much, or JAR, etc.)

The inclusion of open-ended questions as one of the primary elements of sensory and consumer tasks blurs the line with other fields, including psychology and sociology where these qualitative methods originated. More recently, advances in the technology opened new doors (web-scraping, social listening, etc.) that brought us closer to other fields such as marketing for instance. In these applications, although the amount of data can be considerably larger, the aim of the analysis stays the same: extracting information from text/comments. 


### Objectives of Text Analysis


Open-ended comments, and more generally textual responses in questionnaires, are by definition qualitative. This means that the primary analysis should be qualitative, which could consist in reading all these comments, and eventually summarizing the information gathered. 

But as the number of comments increases, such approach quickly becomes too time/energy consuming for the analysts. How can we transform such qualitative data into some quantitative measures, that digest and summarize the information contained in these comments, without losing the overall meaning of the messages (context)?

As one can imagine, such solution can easily be done by simply counting how often a certain word is being used in a certain context (e.g. how often the word `sweet` is being associated to each product evaluated.) However, if such solution is a reasonable one to start with, we will show some alternatives that allow going deeper into the understanding of textual inputs.
This is the objective of the textual analysis and NLP that we are going to tackle in the next sections.


### Warnings


Languages are complex, as many aspects can influence the meaning of a message. For instance, in spoken languages, the intonation is as important as the message itself. In written languages, non-word items (e.g. punctuation, emojis) can also change completely the meaning of a sentence (e.g.irony). Worst, some words have different meanings depending on their use (e.g. *like*), and the context of the message provides its meaning. Unfortunately, the full *context* is only available when analyzed manually (e.g. when the analyst reads all the comments), meaning that automating analyses do not always allow capturing it properly. However, reading all the comments is also not a solution, as it quickly becomes too tedious to be done by hand.

This is why we suggest to automate the analysis to extract as much information as possible, before going back to the raw text to ensure that the conclusions drawn actually match the data. 


## Illustration using Sorting Task Data


To illustrate this chapter, the data set that we are using was kindly shared by Dr. Jacob Lahne. It is part of a study that aimed in developing a CATA lexicon for Virginia Hard (Alcoholic) Ciders (REF.). The data can be found in *cider_data.xlsx*.


## Data Pre-processing


Before starting, it is important to mention that there are a large variety of packages in R that handle textual data. Amongst others, we can mention the `{tm}` package for text mining, `{tokenizers}` to transform strings into tokens, `{SnowballC}` for text stemming, `{SpacyR}` for Natural Language Processing, or `{Xplortext}` for deep understanding and analysis of textual data. 

<!-- More relevant package to add? -->

However, to ensure a continuity with the rest of the book, we will emphasize the use of the `{stringr}` package (it is part of the `tidyverse`) for handling strings (here text) combined with the `{tidytext}` package as it applies the `{tidyverse}` philosophy to textual data. 

For the more curious readers, you can also have a look at the project IRaMuTeQ^[See [http://www.iramuteq.org/](http://www.iramuteq.org/), which is a free software dedicated to text analysis and developed in R and Python. 


### Introduction to working with strings (`{stringr}`)


As mentioned earlier, `{stringr}` is one of the packages included in the `{tidyverse}`. This package brings a large set of tools that allow working with strings. Most functions included in `{stringr}` start with `str_*()`. Amongst the most convenient functions, we can mention:

  - `str_length()` to extract the strength of the string;
  - `str_c()` to combine multiple strings into one;
  - `str_detect()` to search for a pattern in a string, and `str_which()` find the position of a pattern within the string;
  - `str_extract()` and `str_extract_all()` to extract the first (or all) matching pattern from a string;
  - `str_remove()` and `str_remove_all()` to remove the first (or all) matching pattern from a string;
  - `str_replace()`, `str_replace_all()`, to replace the first (or all) matching pattern with another one.
  
It also includes *formatting* options that can be applied to strings, including:

  - `str_to_upper()` and `str_to_lower()` to convert strings to uppercase or lowercase;
  - `str_trim()` and `str_squish()` to remove white spaces;
  - `str_order` to order the element of a character vector.
  
Examples of application of some of these functions will be provided in the next sections.
  
  
### Tokenization


The analysis of textual data starts with defining the statistical unit of interest, also known as *token*. This can either be a single word, a group of words, a sentence, a paragraph, a whole document etc. The procedure to transform the document into tokens is called *tokenization*. 

Let's have a look at our data here:


```{r}
library(tidyverse)
library(readxl)
library(here)

file_path <- here("data","cider_data.xlsx") 
cider_og <- read_xlsx(file_path) %>% 
  mutate(sample = as.character(sample))

```


We notice here that for each sample evaluated, respondents are providing a set of descriptions, which can be a single word (e.g. `yeasty`) or a group of words (`like it will taste dry and acidic`). This dataset is fairly well structure since the tokens are separated by a `;` or `,`.

Let's transform this text into tokens using `unnest_tokens()` from the `{tidytext}` package. The function `unnest_tokens()` already proposes different options for the tokenization, including "words", "ngrams", or "sentences" for instance. However, since we want to use a specific character to separate the tokens (here `;`, `,` etc.), we are using "regex" by specifying the different patterns.


```{r}
(cider <- cider_og %>% 
  unnest_tokens(tokens, comments, token="regex", pattern="[;|,|:|.|/]", to_lower=FALSE))
```


This procedure already provides some interesting information as we already could count how often the word *apple* for instance is used to describe each samples. However, a deeper look at the data shows some inconsistencies, since some words starts with a space, or have capital letters (remember that R is case-sensitive!).


### Simple Transformation (lowercase)


To improve the analysis, let's standardize the text by removing all the white spaces (*irrelevant* spaces in the text, e.g. at  the start/end, double spaces, etc.), transforming everything to lower case (we could have done directly through `unnest_tokens()` already), removing some special letters, replacing some misplaced characters etc.^[This process is done in iterations: the more you clean your document, the more you find some small things to fix...until you're set!]


```{r}
(cider <- cider %>% 
  mutate(tokens = str_to_lower(tokens)) %>% 
  mutate(tokens = str_trim(tokens)) %>% 
  mutate(tokens = str_squish(tokens)) %>% 
  mutate(tokens = str_remove_all(tokens, pattern="[(|)|?|!]")) %>% 
  mutate(tokens = str_remove_all(tokens, pattern="[ó|ò]")) %>% 
  mutate(tokens = str_replace_all(tokens, pattern="õ", replacement="'")))
```


Let's produce the list of tokens generated here (and its corresponding frequency):


```{r}
cider %>% 
  count(tokens) %>% 
  arrange(desc(n)) %>%
  View()
```


At first, it seems that the most used words to describe the ciders are `sweet` (55 occurrences), `fruity` (33 occurrences), and `sour` (32 occurrences). However, a deeper look at this list highlights a few things that still need to get tackled: 

  - The same concept can be described in different ways: `spicy`, `spices`, and `spiced` may all refer to the same concept, yet they are written differently and hence are considered as different tokens. We will see how we can handle this in a later stage.
  - Multiple concepts are still joined (and hence considered separately: `sour and sweet` is currently neither associated to `sour`, nor to `sweet`, and we may want to merge them.
  - There could be some typo: Is `sweat` a typo and should read `sweet`? Or did that respondent really perceived the cider as `sweat`?
  - Although most tokens are made of one (or few) words, some others are defined as a whole sentence (e.g. `this has a very lovely floral and fruity smell`)


### Stopwords


*Stop words* refer to *common* words that do not carry much (if at all) information. In general, stop words include words (in English) such as *I*, *you*, *or*, *of*, *and*, *is*, *has*, etc. It is hence common practice to remove such stop words before any analysis, as they would *pollute* the results with unnecessary information.

Building lists of stop words can be tedious. Fortunately, it is possible to find some pre-defined lists, and to eventually adjust them to our own needs by adding and/or removing words. In particular, the package `{stopwords}` contains a comprehensive collection of stop word lists:


```{r}
library(stopwords)
length(stopwords(source="snowball"))
length(stopwords(source="stopwords-iso"))
```


Here, we can see that English Snowball list contains 175 terms, whereas the English list from the Stopwords ISO collection contains 1298 words.

A deeper look at these lists (and particularly to the Stopwords ISO list) shows that certain words including *like*, *not* and *don't* (just to name a few) are considered as stop words. If we would use this list blindly, we would remove these words from our comments. 

Although using such list on our current example would have a limited impact on the analysis (most comments are just few descriptive words), it would have a more critical impact on other studies in which consumers give their opinion on samples. Indeed, the analysis of the two following comments *I like Sample A* and *I don't like Sample B* would be lost although they provide some relevant information.

It is hence important to remember that although a lot of stop words are common, there are also a lot of them that are topic specific, and that should be (or should not be) used in certain contexts. Hence inspecting and adapting these lists before use is strongly recommended.

For further cleaning, let's go one step further and split the remaining tokens into words by using the space as separator. To ensure that we can still recover which words belong to the same token (as defined previously, as this information can be relevant later for *bigrams*), we are adding an increasing number next to each token for each assessor using `row_number()`:


```{r}
cider <- cider %>% 
  relocate(subject, .before=sample) %>% 
  group_by(subject, sample) %>% 
  mutate(num = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(tokens, tokens, token="regex", pattern=" ")

head(cider)
```


We can see here that for `J1` and `182`, the first token is now separated into three words: `hard`, `cider`, and `smell`.

We can re-count how often each word has been used, using the exact same code as previously. 
It appears now that `sweet` appears 91 times, and `apple` 80 times. Interestingly, terms such as `a`, `like`, `the`, `of`, `and` etc. also appear fairly frequently.

Since we have a small set of text, let's use the *SnowBall Stopword* list as a start, and look at the terms that are shared in both our list and this stopword list:


```{r}
stopword_list <- stopwords(source="snowball")
word_list <- cider %>% 
  count(tokens) %>% 
  pull(tokens)

intersect(stopword_list, word_list)
```


As we can see, some words such as *off*, *not*, *no*, *too*, and *very* would be removed. Since we prefer to keep them, we will remove them from the original list. Similarly, we can look at the words from our data that we would not remove. We can then track the words that we would not consider relevant and add them to the list:


```{r}
word_list[!word_list %in% stopword_list]
```


For instance, words such as *like*, *sample*, *just*, *think*, or *though* for instance do not seem to bring any relevant information here. Hence, we propose to remove these as well:


```{r}
stopword_list <- stopword_list[!stopword_list %in% c("off","no","not","too","very")]
stopword_list <- c(stopword_list, c("accompany","amount","anything","considering","despite","expected",
                                    "just","like","neither","one","order","others","products",
                                    "sample","seems","something","thank","think","though","time","way",
                                    "-"))
```


Finally, we clean our data by removing all the words stored in `stopword_list`. This can easily be done either using `filter()` (we keep tokens that are not contained in `stopword_list`), or by using `anti_join()`^[Note that if we were using the original list of stopwords, `anti_join()` can directly be associated to `get_stopwords(source="snowball")`.]:


```{r}
cider <- cider %>% 
  anti_join(tibble(tokens = stopword_list), by="tokens")
```


### Stemming and Lemmatization


After removing the stop words, we finally have a list of 334 different words. However a closer look at this list shows that it is still not optimal, as for instance `apple` (80 occurrences) and `apples` (24 occurrences) are considered as two separate words although we could argue that they refer to the same concept.

To further *clean* the data, two similar approaches can be considered: **stemming** and **lemmatization**. 

The procedure of stemming consists in performing a step-by-step algorithm that reduces each word to its base word (or *stem*). The most used algorithm is the one introduced by REF (Porter, 1980) which is available in the `{SnowballC}` package through the `wordStem()` function:


```{r}
cider <- cider %>% 
  mutate(stem = wordStem(tokens))

cider %>% 
  count(stem) %>% 
  arrange(desc(n))
```


The stemming reduced further the list to 309 words. In particular, `apple` and `apples` have been combined into `appl` (104 occurrences). However, due to the way the algorithm works, the final tokens are no longer English^[Different algorithms for different languages exist, so we are not limited to stemming English words.] words.

Alternatively, we can *lemmatize* words. Lemmatization is similar to stemming except that it does not cut words to their stems: Instead it uses knowledge about the language's structure to reduce words down to their dictionary form (also called *lemma*). Such approach is implemented in the `{spacyr}` package^[spaCy is a library written in Python: for the `{spacyr}` package to work, you'll need to go through a series of steps that are described here: (https://cran.r-project.org/web/packages/spacyr/readme/README.html)[https://cran.r-project.org/web/packages/spacyr/readme/README.html]] and the `spacy_parse()` function:


```{r}
library(spacyr)

spacy_initialize(entity=FALSE)
lemma <- spacy_parse(cider$tokens) %>% 
  as_tibble() %>% 
  dplyr::select(tokens=token, lemma) %>% 
  unique()

cider <- full_join(cider, lemma, by="tokens")

```


As can be seen, as opposed to stems, lemmas consist in *regular* words. Here, the grouping provides similar number of terms (309 vs. 307) yet some differences can be noticed:


```{r}
cider %>% count(stem)
cider %>% count(lemma)
```


In the case of lemmatization, `acid`, `acidity`, and `acidic` are still considered as separate words whereas they are all grouped under `acid` with the stemming procedure. This particular example shows the advantage and disadvantage of each method, as it may (resp. may not) group words that are not (resp. that are) meant to be grouped. Hence, the use of lemmatization/stemming procedures should be thought carefully.

It should also be said that neither the lemmatization nor the stemming procedure will combine words that are different, yet with similar meanings. For instance, the words `moldy` and `rotten` have been used, and some researchers may decide to group them if they consider them equivalent (same could apply to `acid`, `acidity`, and `acidic` from the lemmatization procedure). Such third step of grouping can be done using `str_replace()`:


```{r}
cider %>% 
  count(lemma) %>% 
  filter(lemma %in% c("moldy","rotten"))

cider %>% 
  mutate(lemma = str_replace(lemma, "moldy", "rotten")) %>% 
  count(lemma) %>% 
  filter(lemma %in% c("moldy","rotten"))
```


As can be seen here, originally, `moldy` was stated twice whereas `rotten` was stated 5 times. After re-placing `moldy` by `rotten`, the newer version contains 7 occurrences of `rotten` and none of `modly`. 

Doing such transformation can quickly be tedious to do directly in R. As an alternative solution, we propose to export the list of words in Excel, create a new column with the new grouping names, and merge the newly acquired names to the previous file. This is the approach we used to create the file entitled *Example of word grouping.xlsx*. In this example, one can notice that we limited the grouping to a strict minimum for most words except `bubble` that we also combined to `bubbly`, `carbonate`, `champagne`, `moscato`, `fizzy`, and `sparkle`:


```{r}
new_list <- read_xlsx("temp/Example of word grouping.xlsx")
cider <- cider %>% 
  full_join(new_list, by="lemma") %>% 
  mutate(lemma = ifelse(is.na(`new name`), lemma, `new name`)) %>% 
  dplyr::select(-`new name`)

cider %>% count(lemma)

```


This last *cleaning* approach reduces further the number of words to 279.


## Text Analysis

Now the text has been sufficiently cleaned, we can run some analysis and actually compare the samples in the way they have been described by the respondents. To do so, let's start with simple analyses. 

### Raw Frequencies and Visualization

In the previous sections, we have already shown how to count the number of occurrences of each word. We can reproduce this and show the top 10 words the most used to describe our ciders:


```{r}
cider %>% 
  group_by(lemma) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n>=10, !is.na(lemma)) %>% 
  ggplot(aes(x=reorder(lemma, n), y=n))+
  geom_col()+
  theme_minimal()+
  xlab("")+
  ylab("")+
  theme(axis.line = element_line(colour="grey80"))+
  coord_flip()+
  ggtitle("List of words mentioned at least 10 times")
```


As seen previously, the most mentioned words are `apple`, `sweet`, `fruity`, and `sour`.

Let's now assess the number of time each word has been used to characterize each product.


```{r}
cider %>% 
  filter(!is.na(lemma), !is.na(sample)) %>% 
  group_by(sample, lemma) %>% 
  count() %>% 
  ungroup() %>% 
  pivot_wider(names_from=lemma, values_from=n, values_fill=0)
```


A first look at the contingency table shows that `apple` has been used 24 times to characterize sample `365` while it has only be used 9 times to characterize sample `401`.

Since the list of terms is quite large, we can visualize these frequencies in different ways: First, we could re-adapt the histogram produced previously overall but per product. This could give a good overview on which words characterize each sample:


```{r}
prod_term <- cider %>% 
  filter(!is.na(lemma), !is.na(sample)) %>% 
  group_by(sample, lemma) %>% 
  count() %>% 
  ungroup() %>% 
  split(.$sample) %>% 
  map(function(data){
    
    data %>% 
      arrange(desc(n)) %>% 
      filter(n>=5) %>% 
      ggplot(aes(x=reorder(lemma, n), y=n))+
      geom_col()+
      theme_minimal()+
      xlab("")+
      ylab("")+
      theme(axis.line = element_line(colour="grey80"))+
      coord_flip()+
      ggtitle(paste0("List of words mentioned at least 5 times for ", 
                     data %>% pull(sample) %>% unique()))
    
  })

```


Another approach consists in visualizing the association between the samples and the words in a multiple way using Correspondence Analysis (CA). Since the CA can be sensitive to low frequencies (Add REF), we reduce the frequency table by only keeping terms that were at least mentioned 5 times across all samples. We then use the `CA()` function from `{FactoMineR}` to build the CA map:


```{r}
cider_ct <- cider %>% 
  filter(!is.na(lemma), !is.na(sample)) %>% 
  group_by(sample, lemma) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 5) %>% 
  pivot_wider(names_from=lemma, values_from=n, values_fill=0) %>% 
  as.data.frame() %>% 
  column_to_rownames(var="sample")

library(FactoMineR)
cider_CA <- CA(cider_ct)

```


As can be seen, sample `731` is more associated to alcoholic terms such as `alcohol` or `wine`, and colors (`red`, `green`). Samples `239` and `401` are more associated to `sour` and `bitter` (and `pear` for `239`), whereas samples `519` and `182` are more described by terms such as `fruity`, and `sweet` (`floral` is also used to characterize `182`).


These frequencies can also be visualized using wordclouds, which can easily be done using the `{ggwordcloud}` package which has the advantage to build such representation in a `{ggplot2}` format. Such wordcloud (here one per product) can be obtained using the following code:

```{r}
cider_wc <- cider %>% 
  filter(!is.na(lemma), !is.na(sample)) %>% 
  group_by(sample, lemma) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 5)

library(ggwordcloud)
ggplot(cider_wc, aes(x=sample, colour=sample, label=lemma, size=n))+
  geom_text_wordcloud(eccentricity = 2.5)+
  xlab("")+
  theme_minimal()

```


In these wordclouds, we notice that `apple` and `sweet` appear big for (almost) all the samples, which can make the comparison quite difficult between samples. Fortunately, the `geom_text_wordcloud()` function provides an interesting parameter in its aesthetics called `angle_group` which allows controlling the position of the words. To illustrate this, let's apply the following rule: for a given sample, if the proportion of association of a word is larger than 1/6 (as we have 6 samples), the word will be printed in the upper part of its wordcloud, and in the lower part otherwise. To facilitate the readability, the color code used follow the same rule.


```{r}
cider_wc %>% 
  group_by(lemma) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup() %>% 
  ggplot(aes(colour= prop<1/2, label=lemma, size=n, angle_group = prop < 1/2))+
  geom_text_wordcloud(eccentricity = 2.5)+
  xlab("")+
  theme_minimal()+
  facet_wrap(~sample)
```


As can be seen, the term `apple` is more frequently (i.e. more than 1/6) used to characterize samples `182`, `239`, `365`, and `731`. The term `sweet` is more frequently used to characterize samples `182` and `519`. Such conclusions would have been more difficult to reach based on the previous *unstructured* wordcloud.


### Bigrams, *n*-grams


In the previous set of analyses, we defined each word as a token. This procedure disconnects words from each others, hence discarding the context of the word. Although this approach is common, it can lead to misinterpretation since a product that would often be associated to (say) *not sweet* would in the end be characterized as *not* and *sweet*. A comparison of samples based on the sole word *sweet* could suggest that the previous product is often characterized by it although it should be the opposite.

To avoid this misinterpretation, two solutions exist:

1. Replace *not sweet* by *not_sweet*, so that it is considered as one token rather than two;
2. Look at groups of words, i.e. at words within their surroundings.

The latter option requires us introducing the notion of bi-grams (groups of 2 following words), tri-grams (groups of 3 following words), or more generally *n*-grams (groups of *n* following words). More precisely, we are applying the same frequency count as before except that we are no longer considering one word as a token, but a sequence of 2, 3, or more generally *n* words as a token. Such grouping can be obtained by the `unnest_tokens()` from `{tidytext}` in which `token='ngrams'`, with `n` defining the number of words to consider.

For simplicity, let's apply this to the original data, although it could be applied to the cleaned version (here we consider bi-grams).

```{r}
cider_2grams <- cider_og %>% 
  unnest_tokens(bigrams, comments, token="ngrams", n=2)

cider_2grams %>% 
  count(bigrams) %>% 
  arrange(desc(n))
```


In overall, the stronger 2-words association is between `sweet fruity`. Other relevant associations are `green apple`, `sweet apple`, or `very sweet`.
Of course, such bi-grams can also be obtained per product:

```{r}
cider_2grams %>% 
  group_by(sample) %>% 
  count(bigrams) %>% 
  ungroup() %>% 
  arrange(desc(n)) %>% 
  filter(sample == "182")

```

For sample `182`, `not sweet` appears 3 times which can be surprising since it was one of the sample the most associated to `sweet` with 22 occurrences.


### Word Embedding


The previous section introduces the concept of context, as words are associated to their direct neighbors. Another approach called *word embedding* goes one step further by looking at connections between words present within a certain window: for instance, how often are *not* and *sweet* connected within a window of 3, 5, or 7 words? Such approach is not presented here as it is only relevant for longer text documents. 

In the previous sections, we already introduced the notion of *term frequency* (*tf*), which corresponds to the number of times a word is being used in a document. When a collection of documents are analyzed and compared, it is also interesting to look at the *inverse document frequency* (*idf*), which consists in highlighting words that discriminate between documents by reducing the weight of common words and by increasing the weight of words that are specific to certain documents only. In practice, both concepts are associated (by multiplication) to compute a term's *tf-idf*, which measures the frequency of a term adjusted for its rarity in use.


### Sentiment Analysis


In our current example, the textual data we analyze is descriptive. In other words, the items that we analyze have no particular valence (i.e. they are neither negative, nor positive). When text data are more spontaneous (e.g. social media such as tweets, or consumers' responses to open-ended questions), they can be the charged with positive or negative connotations. A good way to measure the overall valence of a message is through *Sentiment Analysis*. 

To perform *Sentiment Analysis*, we start by deconstructing the message into words (tokenization approach considered previously). Then, in a similar approach to the stop words, we can combine our list of words with a pre-defined list that defines which words should be considered as positive or negative (the rest being neutral). Ultimately, all the scores associated to each message can be summed, hence providing the overall valence score of a message. 

To get examples of *sentiment* list, the `get_sentiments()` function from the `{tidytext}` package can be used. This function proposes 4 potential lists: `"bing"`, `"afinn"`, `"loughran"`, and `"nrc"` (REFERENCES). Of course, such lists can be modified and adapted to your own needs in case they do not fit perfectly.


## To go further...


Text Mining and Natural Language Processing is a topic that has been (and is still being) studied for a very long time. Recently, it has made a lot of progress thanks to the advances in technology, and has gain even more interest with the abundance of text through social media, websites, blogs, etc. It is hence no surprise that a lot of machine learning models use text data (topic modelling, classification of emails to spam, etc.). Even current handy additions to simplify our life are based on text analysis (e.g. suggestions in emails, translation, etc.) 

In case you would want to go further on this topic, we strongly recommend the following books:

 - Text Mining with R
 - Supervised Machine Learning for Text Analysis in R
 - Textual Data Science with R
 - R for Data Science (introduction to web-scrapping etc.)

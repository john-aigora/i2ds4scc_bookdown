```{r include=FALSE}
knitr::opts_chunk$set(echo=FALSE, eval=FALSE)
```
# Data Preparation {#data-prep}

```{r}
library(tidyverse)
```

*Data Preparation*, which consists of *data inspection* and *data cleaning*, is a critical step before any further *Data Manipulation* or *Data Analysis*. Having a good data preparation procedure ensures a good understanding of the data, and avoids what could be very critical mistakes.

To illustrate the importance of the later point, let's imagine a study in which the samples are defined by their 3-digits code. 
During importation, R would recognize them as number, and hence defines the *Product* column as numerical. Without inspection and correction, any ANOVA that include the product effect would be replaced by a linear regression (or analysis of covariance) which of course does not provide the results required (although the analysis would run without error). Worst, if this procedure is automated and the *p-value* associated to the product effect is extracted, the conclusions would rely on the wrong analysis! A good data preparation procedure is hence important to avoid such unexpected results. 

So what consists of data preparation, and how does that differ from data manipulation? 
There is clearly a very line between data preparation (and particularly *data cleaning*) and data manipulation, many procedures being used in both (same applies to data manipulation and data analysis for instance). For this book, we decided to follow this rule:

*Data Preparation* includes all the required steps to ensure that the data is matching its intrinsic nature. These steps include inspecting the data at hand (usually through simple descriptive statistics of the data as a whole, with no *interpretation mindset*) and cleaning the data by eventually correcting importation errors (imputation of missing data is also included here). Although some descriptive statistics are being produced for data inspection, these analyses have no interpretation value besides ensuring that the data are in the right range, or following the right distribution. For instance, with our sensory data, we would ensure that all our sensory scores are included between 0 and 100 (negative scores would not be permitted), but we would not look at the mean or the distribution of the score per product which would belong to data analyses and would require interpretation (e.g. P01 is sweeter than P02).

The *Data Manipulation* is an optional step that adjust or convert the data into a structure that is usable for further analysis. This of course may lead to *interpretation* of the results as it may involve some analyses.

The *Data Analysis* step ultimately converts the data into results (through values, graphics, tables, etc.) that provide more insights (through interpretation) about the data at hand.

The data used in this chapter includes the *Sensory Profile.xlsx* that you imported in Section \@ref(data-collection). Another version of the sensory profiles that includes missing values is also being used (*Sensory Profile (NA).xlsx*).

## Inspect {#inspect}

### Data Inspection {#data-inspection}

To inspect the data, different steps can be used. 
First, since `read_xlsx()` returns a tibble, let's take advantage of its printing properties to get a fill of the data at hand:

```{r TFEQ_data}
sensory
```

Other informative solutions consists in printing a summary of the data through the `summary()` or `glimpse()` function:

```{r}
summary(sensory)

glimpse(sensory)
```

These functions provide relevant yet basic views of each variable present in the data including the type of variable, the range, mean, and median, as well as the first values of each variables. 

Such view might be sufficient for some first conclusions (e.g. Are my panelists considered as numerical or nominal data? Do I have missing values?), yet it is not sufficient to fully ensure that the data is ready for analysis. For the latter, more extensive analyses can be performed automatically in different ways. These analyses include looking at the distribution of some variables, or the frequencies of character levels. 

A first solution comes from the `{skimr}` package and its `skim()` function. By applying it to data, an automated extended summary is directly printed on screen by separating `character` type variables from `numeric` type variables:

```{r TFEQ_data_skim}
library(skimr)
skim(sensory)
```

Another approach consists in generating automatically an html report with some pre-defined analyses using `create_report()` from the `{DataExplorer}` package.

```{r}
library(DataExplorer)
create_report(sensory)
```

Unless specified otherwise through `output_file`, `output_dir`, and `output_format`, the report will be saved as an html file on your active directory as *report.html*. This report provides many statistics on your data, including some simple statistics (e.g. raw counts, percentages), informs you on the structure of your data, as well as on eventual missing data. It also generates graphics to describe your variables (e.g. univariate distribution, correlation and PCA). 

Note that the analyses performed to build this report can be called directly within R: `introduce()` and `plot_intro()` generates the first part of the report, whereas `plot_missing()` and `profile_missing()` provide information regarding missing data for instance.

### Missing Data

In the previous section on \@ref(data-inspection), it can be seen that the data set contain missing values. It concerns for instance the attribute `Light`, for which one missing value has been detected.
So how to handle missing values?

#### Ignore missing values

A first solution is to simply *ignore* the presence of missing values, as many analyses handle them well. For instance, an ANOVA could be run for such attributes, and results are being produced:

```{r}
broom::tidy(aov(Light ~ Product + Judge, data=sensory))
```

This solution may work fine when the number of missing values is small, but be aware that it can also provide erroneous results in case they are not handled the way the analyst is expecting them to be handled.

For some other analyses, *ignoring* the presence of missing values may simply provide results. To illustrate this, let's compute the simple mean per product for `Light` 

```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light)) %>% 
  ungroup()
```

As can be seen, since `P04` contains missing values, its corresponding mean is defined as `NA`. To enforce the mean to be computed, we need to inform R to remove missing values when computing the mean. This is done by adding within the `mean()` function the parameter `na.rm=TRUE`:

```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light, na.rm=TRUE)) %>% 
  ungroup()
```

Using `na.rm=TRUE` is equivalent to removing rows containing missing values from the data before performing the analysis.

#### Remove missing values

The solution using `na.rm=TRUE` is a first way to handle missing values by removing or ignoring them. However, it is not the only strategy, and we can consider other approaches that are more *sensory and consumer studies* driven.

For instance, missing values can be removed systematically by deleting the entire row of data. Let's take the example of `Sour` which contains 10 missing values: 

```{r}
sensory %>% 
  filter(!is.na(Sour))
```

Unfortunately, this solution is not satisfactory as it also deletes real data since the dataset went from 99 to 89 rows. This means that for variables that did not have missing values for instance, existing data have been removed. 

Another approach suggests to first rotate using `pivot_longer()` the data before removing missing values:

```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores)) %>% 
  group_by(Product, Variables) %>% 
  summarize(Means = mean(Scores)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = Variables, values_from = Means) %>% 
  dplyr::select(Product, Sour, Light)
```

If this solution seems satisfactory as the means were computed without using `na.rm=TRUE` for both `Sour` and `Light` (who contained missing values), its use is limited since converting the data to its original format (i.e. performing `pivot_wider()` after `pivot_longer()` without computing the mean in between) will reintroduce the missing values^[Missing values do not need to be visible to exist: Incomplete designs are a good example showing that although the data do not have empty cells, it does contain a lot of missing data (the samples that were not evaluated by each panelist).].  

Also removing missing values has the impact of unbalancing the data. By taking the example of `Light` and `Sour`, let's print the number of panelist evaluating each product:

```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores),
         Variables %in% c("Light","Sour")) %>%
  group_by(Product, Variables) %>% 
  count() %>% 
  ungroup() %>% 
  pivot_wider(names_from=Variables, values_from=n)
```

Here for example, the only missing value detected for `Light` is related to `P04`. For `Sour`, `P02`, `P07`, and `P09` only have 7 observations out of 9. 

Another strategy consists in removing attributes, products, or panelists that have missing data. The procedure presented below show the procedure on how to remove attributes with missing data, but could easily be adapted to panelists or products:

```{r}
sensory_long <- sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores")

(attr_rmv <- sensory_long %>% 
  filter(is.na(Scores)) %>% 
  pull(Variables) %>% 
  unique())

sensory_clean <- sensory_long %>% 
  filter(!(Variables %in% attr_rmv)) %>% 
  pivot_wider(names_from=Variables, values_from=Scores)

```

This procedure removed the 7 attributes that contained missing values, leading to a table with 99 rows and 29 columns (instead of 36).

#### Impute missing values

Rather than removing missing data, another (better?) strategy is to impute missing values. Here again, many strategies can be considered, starting with replacing them with a fixed value. Such approach is usually not the most suitable one, yet it can be relevant in certain cases. For instance, in a CATA task, missing values would often be replaced by 0 (not ticked).

To replace missing values with a fixed value, the `replace_na()` can be used. When applied to a tibble, this function requires you defining using `list()` the columns to apply it to, and which values to use (each column being treated separately). 

For convenience, let's apply it to `sensory` by replacing missing values for `Sour` by the value `888` and for `Light` with `999` (we use these extreme values to track changes more easily):

```{r}
sensory %>% 
  replace_na(list(Sour = 888, Light = 999)) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

When dealing with intensity scale, it is more frequent to replace missing values by the mean score for that product and attribute. When the test is duplicated, the mean provided by this *panelist x product x attribute* across the different repetitions available is even preferred as it maintains individual variability (e.g. scale usage) within the scores. 

Such approach is then can be done in two steps:
  1. Compute the mean (since we do not have duplicates, we use the mean per product);
  2. Combine it to the data

For simplicity, `sensory_long` is used as starting point:

```{r}
prod_mean <- sensory_long %>% 
  group_by(Product, Variables) %>% 
  summarize(Mean = mean(Scores, na.rm=TRUE)) %>% 
  ungroup()

sensory_long %>% 
  full_join(prod_mean, by=c("Product","Variables")) %>% 
  mutate(Scores = ifelse(is.na(Scores), Mean, Scores)) %>% 
  dplyr::select(-"Mean") %>% 
  pivot_wider(names_from=Variables, values_from=Scores) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, the missing value associated to `J01` for `Light` and `P04` has been replaced by `40.7`. In fact, any missing values related to `P04` and `Light` would automatically be replaced by `40.7` here. For other products (and other attributes), their respective means would be used.

When the model used to impute missing values is fairly simple (here, replacing by the mean correspond to a simple 1-way ANOVA), the imputation can be done directly through the `impute_lm()` function from the `{simputation}` package. To mimic the previous approach, the one-way ANOVA is being used^[It is worth noticing that the individual differences could also be included by simple adding the Judge effect in the model.]. Here, missing data for both `Sour` and `Light` are being imputed independently using the same model:

```{r}
library(simputation)

sensory %>% 
  impute_lm(Sour + Light ~ Product) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, this procedure provides the same results as before, but in less steps!

It can be noted that in some situations, implementing missing values using such ANOVA (or regression) model can lead to aberrations. It is for instance the case when the imputed values falls outside the scale boundaries. To avoid such situations, `{simputation}` also provides other more advanced alternatives including (amongst others) `impute_rf()` which uses random forest to impute the missing values.

Last but not least, imputation of missing values could also be done in a multivariate way, by using the structure of the data (e.g. correlation) to predict the missing values. This is the approach proposed in the `{missMDA}` package. Since our data are numeric, the imputation is done through PCA with the `imputePCA()` function. Note that here, the imputed values are stored in the object `.$completeObs` (here, `sensory` is used):

```{r}
library(missMDA)
imputePCA(sensory, quali.sup=1:4, method="EM")$completeObs %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

In this case, it can be seen that the missing value for `J01`-`P04`-`Light` has been replaced by the value `31.8`.

#### Limitations

Different strategies for handling missing values exist, and the one used should be adapted to the data.

Since most imputation methods are based on modeling, using such technique on variables with high missing values rate can lead to introducing bias in data. For example, in a situation in which half the data is missing (e.g. incomplete design in which assessors evaluated 1/2 the product set), each prediction is proportionally based on one value only. This means that further analyses would be based on 1/2 measured - 1/2 *fictive* data. 

It is hence important to understand the structure of missing values before handling them.

### Design Inspection

As part of the data inspection, it is relevant to check that all the data are in line with their intrinsic nature, look for outliers, etc. 
Another point of interest - quite specific to sensory and consumer data - is to ensure that the design is well balanced, and handles the first-order and carry-over effects. This step is particularly important for those who analyze the data but were not involved from the start in that study (and hence were not involved when the test was set-up).

Let's show a simple procedure that would check part of the quality of a design.
Since neither in *Sensory Profile.xlsx*, nor in *TFEQ.xlsx* the design is not available, the `sensochoc` data stored in the `{SensoMineR}` is used. 

To load (and clean) the data, let's run these lines of code:

```{r}
library(SensoMineR)

data(chocolates)

dataset <- sensochoc %>% 
  as_tibble() %>% 
  mutate(across(c(Panelist, Session, Rank, Product), as.character))

```

The data consist in 6 products (`Product`) evaluated by 29 panelists (`Panelist`) in duplicates (`Session`). The presentation order is stored in `Rank`. 

To evaluate whether the products have been equally presented at each position, a simple cross-count between `Product` and `Rank` is done. This can easily be done using the `xtabs()` function:

```{r}
xtabs(~Product + Rank, data=dataset)
```

Such table can also be obtained using `group_by()` and `count()` to get the results in a tibble:

```{r}
dataset %>% 
  group_by(Product) %>% 
  count(Rank) %>% 
  ungroup() %>% 
  pivot_wider(names_from=Rank, values_from=n)
```

As we can see, the design is not perfectly balanced, as `choc2` is evaluated 11 times in the 1st, 4th, and 6th position, but only 7 times in the 5th position.

The next step then consists in evaluating how often each product is assessed before the other products (carry-over effect). Since this information is not directly available in the data, it needs to be added. 

Let's start with extracting the information available, i.e. the order of each product for each panelist and session:

```{r}
current <- dataset %>% 
  dplyr::select(Panelist, Product, Session, Rank) %>% 
  mutate(Rank = as.numeric(Rank))

```

An easy way to add the `Previous` product information as a new column in the data is by replacing `Rank` by `Rank + 1` in `current` (all new positions larger than the number of products is filtered). 

```{r}
previous <- current %>% 
  rename(Previous = Product) %>% 
  mutate(Rank = Rank + 1) %>% 
  filter(Rank <= length(unique(dataset$Product)))

```


This new data is merged to `current` by `Panelist`, `Session`, and `Rank`:

```{r}
(cur_prev <- current %>% 
  left_join(previous, by=c("Panelist", "Session", "Rank")))
```

As can be seen, the products that are evaluated first get `NA` in `Previous`, and for each rank r (r > 1), `Previous` gets the product that was evaluated at rank r-1.

To evaluate the carry-over effect, the only thing to do is to cross-count `Product` and `Previous` (here, the results are split per `Session`):

```{r}
cur_prev %>% 
  group_by(Session, Product, Previous) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(Product = factor(Product, levels=paste0("choc",1:6)),
         Previous = factor(Previous, levels=c("NA",paste0("choc",1:6)))) %>%
  arrange(Previous) %>% 
  pivot_wider(names_from=Previous, values_from=n, values_fill=0) %>% 
  arrange(Product) %>% 
  split(.$Session)

```

As expected, the table shows that a product is never evaluated twice in a row (the diagonal contains 0). Here again, the design is not optimal since `choc1` has been evaluated 3 times before `choc2` and 6 times before `choc5` in the first session.

Finally, the `NA` column refers to the time that products did not have a product tested before, meaning that they were evaluated first.

## Clean {#clean}

### Renaming Variables

renaming columns using `rename()` or `select()`, tackled in data manipulation

### Handling Data Type

In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another.

Remember that when your dataset is a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables' type can be assessed at any moment. 

In case the dataset is not in a tibble, the use of the `str()` function used previously becomes handy as it provides this information.

In sensory and consumer research, the four most common types are:

 - Numerical (incl. integer or `int`, decimal or `dcl`,  and double or `dbl`);
 - Logical or `lgl`;
 - Character or `char`;
 - Factor or `fct`.

R still has plenty of other types, for more information please visit: https://tibble.tidyverse.org/articles/types.html

#### Numerical Data

Since a large proportion of the research done is quantitative, it is no surprise that our dataset are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g. 1, 2, -16, etc.), or decimal value (or double, e.g. 1.6, 2.333333, -3.2 etc.).
By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double.

**Do we want to show how to format R wrt the number of decimals? (e.g. options(digits=2))**

#### Binary Data

Another common type that seem to be numerical in appearance, but that has additional properties is the binary type. 
Binary data is data that takes two possible values (`TRUE` or `FALSE`), and are often the results of a *test* (e.g. is `x>3`? Or is `MyVar` numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires.

Note: Intrinsically, binary data is *numerical*, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the `sum()` function, as shown in the simple example below:


```{r example_logical}
set.seed(123456)
# Generating 10 random values between 1 and 10 using the uniform distribution
x <- runif(10, 1, 10)
x

# Test whether the values generated are strictly larger than 5
test <- x>5
test

# Counting the number of values strictly larger than 5
sum(test)

```


#### Nominal Data

Nominal data is any data that is not numerical. In most cases, nominal data are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes: In that case, it is clear that respondent 2 is not twice larger than respondent 1 for instance. But since the software cannot guess that those numbers are *identifiers* rather than *numbers*, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables will be explained in the next section. 

For nominal data, two particular types of data are of interest: 

 - Character or `char`;
 - Factor or `fct`.
  
Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels: 

 - For `character`, there are no particular structure, and the variables can take any values (e.g. open-ended question);
 - For `factor`, the inputs of the variables are structured into `levels`.
 
To evaluate the number of levels, different procedure are required:

 - For `character`, one should count the number of unique element using `length()` and `unique()`;
 - For `factor`, the levels and the number of levels are direcly provided by `levels()` and `nlevels()`.
 
Let's compare a variable set as `factor` and `character` by using the `Judge` column from `TFEQ_data`:


```{r char_vs_fctr1}
example <- TFEQ_data %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge))
summary(example)

unique(example$Judge)
length(unique(example$Judge))

levels(example$Judge_fct)
nlevels(example$Judge_fct)

```


Although `Judge` and `Judge_fct` look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or building tables and graphs.

When set as `character`, the number of levels of a variable is directly read from the data, and its levels' order would either match the way they appear in the data, or are ordered alphabetically. This means that any data collected using a structured scale will lose its natural order. 

When set as `factor`, the number and order of the factor levels are informed, and does not depend on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. 

To illustrate this, let's re-arrange the levels from `Judge_fct` by ordering them numerically in such a way `J2` follows `J1` rather than `J10`.



```{r sorting_judge}
judge <- str_sort(levels(example$Judge_fct),numeric=TRUE)
judge
levels(example$Judge_fct) <- judge

```



Now the levels are sorted, let's 'remove' some respondents by only keeping the 20 first ones (J1 to J20, as J18 does not exist), and re-run the previous code:


```{r char_vs_fctr2}
example <- TFEQ_data %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge)) %>% 
  filter(Judge %in% paste0("J",1:20))
dim(example)

unique(example$Judge)
length(unique(example$Judge))

levels(example$Judge_fct)
nlevels(example$Judge_fct)

```



After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements, whereas the column set as factor still contains the 107 respondents (most of them not having any recordings). This property can be seen as an advantage or a disadvantage depending on the situation:

 - For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of `factor`).
 - For hypothesis testing (e.g. ANOVA) on subset of data (e.g. the data being split by gender), the `Judge` variable set as `character` would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would use 106 degrees of freedom in all cases!

The latter point is particularly critical since the analysis is incorrect and will either return an error or worse return erroneous results!

Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily. 
Let's consider the `Living area` variable from `TFEQ_data` as example. From the original excel file, it can be seen that it has three levels, `1` corresponding to *urban area*, `2` to *rurban area*, and `3` to *rural area*.
Let's start by renaming this variable accordingly:


```{r area_recode1}
example = TFEQ_data %>% 
  mutate(Area = factor(`Living area`, levels=c(1,2,3), labels=c("Urban", "Rurban", "Rural")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```


As can be seen, the variable `Area` is the factor version (including its labels) of `Living area`.
If we would also consider that `Rurban` should be combined with `Rural`, and that `Rural` should appear before `Urban`, we can simply modify the code as such:


```{r area_recode2}
example = TFEQ_data %>% 
  mutate(Area = factor(`Living area`, levels=c(2,3,1), labels=c("Rural", "Rural", "Urban")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```


This approach of renaming and re-ordering factor levels is very important as it simplifies the readability of tables and figures.
Some other transformations can be applied to factors thanks to the `{forcats}` package. Particular attention can be given to the following functions:

 - `fct_reorder`/`fct_reorder2` and `fct_relevel` reorder the levels of a factor;
 - `fct_recode` renames the factor levels (as an alternative to `factor` used in the previous example);
 - `fct_collapse` and `fct_lump` aggregate different levels together (`fct_lump` regroups automatically all the rare levels).

Although it hasn't been done here, manipulating strings is also possible through the `{stringr}` package, which provides interesting functions such as:

 - `str_to_upper`/`str_to_lower` to convert strings to uppercase or lowercase;
 - `str_c`, `str_sub` combine or subset strings;
 - `str_trim` and `str_squish` remove white spaces;
 - `str_extract`, `str_replace`, `str_split` extract, replace, or split strings or part of the strings.
  
 
### Converting between Types

When importing data, variables may not always be associated to the right type. For instance, when respondents or products are numerically coded, they will be defined as integers rather than strings. Additionally, each variable type has its own property. To take full advantage of the different variable types, and to avoid wrong analyses (e.g considering a variable that is numerically coded as numeric when it is not), we need to convert them to other types.  

In the following sections, we will `mutate()` a variable to create a new variable that corresponds to the original one after being converted to its new type (as in the previous example with `Area`). In case we want to overwrite a variable by only changing the type, the same name is used within `mutate()`.  

Based on our variable types of interest, there are two main conversions to run:
 - From numerical to character/factor;
 - From character/factor to numerical.
 
The conversion from numerical to character or factor is simply done using `as.character()` and `as.factor()` respectively. Note however that `as.factor()` only converts into factors without allowing to chose the order of the levels, nor to rename them. Alternatively, the use of `factor()` allows specifying the `levels` (and hence the order of the levels) and their corresponding `labels`. An example in the use of `as.character()` and `as.factor()` was provided in the previous section when we converted the `Respondent` variables to character and factor. The use of `factor()` was also used earlier when the variable `Living area` was converted from numerical to factor (called `Area`) with labels.

To illustrate the following points, let's start with creating a tibble with two variables, one containing strings made of numbers, and one containing strings made of text.


```{r simple_example}
example <- tibble(Numbers = c("2","4","9","6","8","12","10"),
                  Text = c("Data","Science","4","Sensory","and","Consumer","Research"))
```



The conversion from character to numerical is straight forward and requires the use of the function `as.numeric()`:


```{r char_2_num}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```



As can be seen, when strings are made of numbers, the conversion works fine. However, the text are not converted properly and returns NAs.

Now let's apply the same principle to a variable of the type factor. To do so, we will take the same example but first convert the variables from character to factor:


```{r char_2_fctr}
example <- example %>% 
  mutate(Numbers = as.factor(Numbers)) %>% 
  mutate(Text = factor(Text, levels=c("Data","Science","4","Sensory","and","Consumer","Research")))
```



Let's apply as.numeric() to these variables:



```{r fctr_2_num}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```



We can notice here that the outcome is not really as expected as the numbers 2-4-9-6-8-12-10 becomes 3-4-7-5-6-2-1, and Data-Science-4-Sensory-and-Consumer-Research becomes 1-2-3-4-5-6-7. The rationale behind this conversion is that the numbers do not reflects the string itself, but the position of that level within the factor level structure.

To convert properly numerical factor levels to number, the variable should first be converted as character:


```{r fctr_2_char_2_num}
example %>%
  mutate(Numbers = as.numeric(as.character(Numbers)))
```


#### Conditional Renaming?

`mutate()` and `ifelse()`

### Handling Missing Values


Ignoring, removing, imputing


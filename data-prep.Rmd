```{r include=FALSE}
knitr::opts_chunk$set(echo=FALSE, eval=FALSE)
```
# Data Preparation {#data-prep}

```{r}
library(tidyverse)
```

*Data Preparation*, which consists of *data inspection* and *data cleaning*, is a critical step before any further *Data Manipulation* or *Data Analysis*. Having a good data preparation procedure ensures a good understanding of the data, and avoids what could be very critical mistakes.

To illustrate the importance of the later point, let's imagine a study in which the samples are defined by their 3-digits code. 
During importation, R would recognize them as number, and hence defines the *Product* column as numerical. Without inspection and correction, any ANOVA that include the product effect would be replaced by a linear regression (or analysis of covariance) which of course does not provide the results required (although the analysis would run without error). Worst, if this procedure is automated and the *p-value* associated to the product effect is extracted, the conclusions would rely on the wrong analysis! A good data preparation procedure is hence important to avoid such unexpected results. 

So what consists of data preparation, and how does that differ from data manipulation? 
There is clearly a very line between data preparation (and particularly *data cleaning*) and data manipulation, many procedures being used in both (same applies to data manipulation and data analysis for instance). For this book, we decided to follow this rule:

*Data Preparation* includes all the required steps to ensure that the data is matching its intrinsic nature. These steps include inspecting the data at hand (usually through simple descriptive statistics of the data as a whole, with no *interpretation mindset*) and cleaning the data by eventually correcting importation errors (imputation of missing data is also included here). Although some descriptive statistics are being produced for data inspection, these analyses have no interpretation value besides ensuring that the data are in the right range, or following the right distribution. For instance, with our sensory data, we would ensure that all our sensory scores are included between 0 and 100 (negative scores would not be permitted), but we would not look at the mean or the distribution of the score per product which would belong to data analyses and would require interpretation (e.g. P01 is sweeter than P02).

The *Data Manipulation* is an optional step that adjust or convert the data into a structure that is usable for further analysis. This of course may lead to *interpretation* of the results as it may involve some analyses.

The *Data Analysis* step ultimately converts the data into results (through values, graphics, tables, etc.) that provide more insights (through interpretation) about the data at hand.

The data used in this chapter includes the *Sensory Profile.xlsx* that you imported in Section \@ref(data-collection). Another version of the sensory profiles that includes missing values is also being used (*Sensory Profile (NA).xlsx*).

## Inspect {#inspect}

### Data Inspection {#data-inspection}

To inspect the data, different steps can be used. 
First, since `read_xlsx()` returns a tibble, let's take advantage of its printing properties to get a fill of the data at hand:

```{r TFEQ_data}
sensory
```

Other informative solutions consists in printing a summary of the data through the `summary()` or `glimpse()` function:

```{r}
summary(sensory)

glimpse(sensory)
```

These functions provide relevant yet basic views of each variable present in the data including the type of variable, the range, mean, and median, as well as the first values of each variables. 

Such view might be sufficient for some first conclusions (e.g. Are my panelists considered as numerical or nominal data? Do I have missing values?), yet it is not sufficient to fully ensure that the data is ready for analysis. For the latter, more extensive analyses can be performed automatically in different ways. These analyses include looking at the distribution of some variables, or the frequencies of character levels. 

A first solution comes from the `{skimr}` package and its `skim()` function. By applying it to data, an automated extended summary is directly printed on screen by separating `character` type variables from `numeric` type variables:

```{r TFEQ_data_skim}
library(skimr)
skim(sensory)
```

Another approach consists in generating automatically an html report with some pre-defined analyses using `create_report()` from the `{DataExplorer}` package.

```{r}
library(DataExplorer)
create_report(sensory)
```

Unless specified otherwise through `output_file`, `output_dir`, and `output_format`, the report will be saved as an html file on your active directory as *report.html*. This report provides many statistics on your data, including some simple statistics (e.g. raw counts, percentages), informs you on the structure of your data, as well as on eventual missing data. It also generates graphics to describe your variables (e.g. univariate distribution, correlation and PCA). 

Note that the analyses performed to build this report can be called directly within R: `introduce()` and `plot_intro()` generates the first part of the report, whereas `plot_missing()` and `profile_missing()` provide information regarding missing data for instance.

### Missing Data

In the previous section on \@ref(data-inspection), it can be seen that the data set contain missing values. It concerns for instance the attribute `Light`, for which one missing value has been detected.
So how to handle missing values?

#### Ignore missing values

A first solution is to simply *ignore* the presence of missing values, as many analyses handle them well. For instance, an ANOVA could be run for such attributes, and results are being produced:

```{r}
broom::tidy(aov(Light ~ Product + Judge, data=sensory))
```

This solution may work fine when the number of missing values is small, but be aware that it can also provide erroneous results in case they are not handled the way the analyst is expecting them to be handled.

For some other analyses, *ignoring* the presence of missing values may simply provide results. To illustrate this, let's compute the simple mean per product for `Light` 

```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light)) %>% 
  ungroup()
```

As can be seen, since `P04` contains missing values, its corresponding mean is defined as `NA`. To enforce the mean to be computed, we need to inform R to remove missing values when computing the mean. This is done by adding within the `mean()` function the parameter `na.rm=TRUE`:

```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light, na.rm=TRUE)) %>% 
  ungroup()
```

Using `na.rm=TRUE` is equivalent to removing rows containing missing values from the data before performing the analysis.

#### Remove missing values

The solution using `na.rm=TRUE` is a first way to handle missing values by removing or ignoring them. However, it is not the only strategy, and we can consider other approaches that are more *sensory and consumer studies* driven.

For instance, missing values can be removed systematically by deleting the entire row of data. Let's take the example of `Sour` which contains 10 missing values: 

```{r}
sensory %>% 
  filter(!is.na(Sour))
```

Unfortunately, this solution is not satisfactory as it also deletes real data since the dataset went from 99 to 89 rows. This means that for variables that did not have missing values for instance, existing data have been removed. 

Another approach suggests to first rotate using `pivot_longer()` the data before removing missing values:

```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores)) %>% 
  group_by(Product, Variables) %>% 
  summarize(Means = mean(Scores)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = Variables, values_from = Means) %>% 
  dplyr::select(Product, Sour, Light)
```

If this solution seems satisfactory as the means were computed without using `na.rm=TRUE` for both `Sour` and `Light` (who contained missing values), its use is limited since converting the data to its original format (i.e. performing `pivot_wider()` after `pivot_longer()` without computing the mean in between) will reintroduce the missing values^[Missing values do not need to be visible to exist: Incomplete designs are a good example showing that although the data do not have empty cells, it does contain a lot of missing data (the samples that were not evaluated by each panelist).].  

Also removing missing values has the impact of unbalancing the data. By taking the example of `Light` and `Sour`, let's print the number of panelist evaluating each product:

```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores),
         Variables %in% c("Light","Sour")) %>%
  group_by(Product, Variables) %>% 
  count() %>% 
  ungroup() %>% 
  pivot_wider(names_from=Variables, values_from=n)
```

Here for example, the only missing value detected for `Light` is related to `P04`. For `Sour`, `P02`, `P07`, and `P09` only have 7 observations out of 9. 

Another strategy consists in removing attributes, products, or panelists that have missing data. The procedure presented below show the procedure on how to remove attributes with missing data, but could easily be adapted to panelists or products:

```{r}
sensory_long <- sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores")

(attr_rmv <- sensory_long %>% 
  filter(is.na(Scores)) %>% 
  pull(Variables) %>% 
  unique())

sensory_clean <- sensory_long %>% 
  filter(!(Variables %in% attr_rmv)) %>% 
  pivot_wider(names_from=Variables, values_from=Scores)

```

This procedure removed the 7 attributes that contained missing values, leading to a table with 99 rows and 29 columns (instead of 36).

#### Impute missing values

Rather than removing missing data, another (better?) strategy is to impute missing values. Here again, many strategies can be considered, starting with replacing them with a fixed value. Such approach is usually not the most suitable one, yet it can be relevant in certain cases. For instance, in a CATA task, missing values would often be replaced by 0 (not ticked).

To replace missing values with a fixed value, the `replace_na()` can be used. When applied to a tibble, this function requires you defining using `list()` the columns to apply it to, and which values to use (each column being treated separately). 

For convenience, let's apply it to `sensory` by replacing missing values for `Sour` by the value `888` and for `Light` with `999` (we use these extreme values to track changes more easily):

```{r}
sensory %>% 
  replace_na(list(Sour = 888, Light = 999)) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

When dealing with intensity scale, it is more frequent to replace missing values by the mean score for that product and attribute. When the test is duplicated, the mean provided by this *panelist x product x attribute* across the different repetitions available is even preferred as it maintains individual variability (e.g. scale usage) within the scores. 

Such approach is then can be done in two steps:
  1. Compute the mean (since we do not have duplicates, we use the mean per product);
  2. Combine it to the data

For simplicity, `sensory_long` is used as starting point:

```{r}
prod_mean <- sensory_long %>% 
  group_by(Product, Variables) %>% 
  summarize(Mean = mean(Scores, na.rm=TRUE)) %>% 
  ungroup()

sensory_long %>% 
  full_join(prod_mean, by=c("Product","Variables")) %>% 
  mutate(Scores = ifelse(is.na(Scores), Mean, Scores)) %>% 
  dplyr::select(-"Mean") %>% 
  pivot_wider(names_from=Variables, values_from=Scores) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, the missing value associated to `J01` for `Light` and `P04` has been replaced by `40.7`. In fact, any missing values related to `P04` and `Light` would automatically be replaced by `40.7` here. For other products (and other attributes), their respective means would be used.

When the model used to impute missing values is fairly simple (here, replacing by the mean correspond to a simple 1-way ANOVA), the imputation can be done directly through the `impute_lm()` function from the `{simputation}` package. To mimic the previous approach, the one-way ANOVA is being used^[It is worth noticing that the individual differences could also be included by simple adding the Judge effect in the model.]. Here, missing data for both `Sour` and `Light` are being imputed independently using the same model:

```{r}
library(simputation)

sensory %>% 
  impute_lm(Sour + Light ~ Product) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, this procedure provides the same results as before, but in less steps!

It can be noted that in some situations, implementing missing values using such ANOVA (or regression) model can lead to aberrations. It is for instance the case when the imputed values falls outside the scale boundaries. To avoid such situations, `{simputation}` also provides other more advanced alternatives including (amongst others) `impute_rf()` which uses random forest to impute the missing values.

Last but not least, imputation of missing values could also be done in a multivariate way, by using the structure of the data (e.g. correlation) to predict the missing values. This is the approach proposed in the `{missMDA}` package. Since our data are numeric, the imputation is done through PCA with the `imputePCA()` function. Note that here, the imputed values are stored in the object `.$completeObs` (here, `sensory` is used):

```{r}
library(missMDA)
imputePCA(sensory, quali.sup=1:4, method="EM")$completeObs %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

In this case, it can be seen that the missing value for `J01`-`P04`-`Light` has been replaced by the value `31.8`.

#### Limitations

As we have seen, there are different ways to implement missing values, and the different algorithms will likely impute them with different values. Hence, the overall results can be affected, and there is no way to know which solution is the most suitable for our study. Still, it is recommended to treat the missing values, and to chose the right strategy that is the most adapted to the data. 

However, since most imputation methods involve modeling, applying them to variables with a high missing values rate can introduce bias in the data. Let's consider a situation in which assessors are evaluating half the product set using a BIB. The data hence collected contained 1/2 missing values. By imputing the missing values, each prediction is proportionally based on one unique value. And ultimately, any further analyses on this data would be based on half measured and half *fictive* data.

### Design Inspection

As part of the data inspection, it is relevant to check that all the data are in line with their intrinsic nature, look for outliers, etc. 
Another point of interest - quite specific to sensory and consumer data - is to ensure that the design is well balanced, and handles the first-order and carry-over effects. This step is particularly important for those who analyze the data but were not involved from the start in that study (and hence were not involved when the test was set-up).

Let's show a simple procedure that would check part of the quality of a design.
Since neither in *Sensory Profile.xlsx*, nor in *TFEQ.xlsx* the design is not available, the `sensochoc` data stored in the `{SensoMineR}` is used. 

To load (and clean) the data, let's run these lines of code:

```{r}
library(SensoMineR)

data(chocolates)

dataset <- sensochoc %>% 
  as_tibble() %>% 
  mutate(across(c(Panelist, Session, Rank, Product), as.character))

```

The data consist in 6 products (`Product`) evaluated by 29 panelists (`Panelist`) in duplicates (`Session`). The presentation order is stored in `Rank`. 

To evaluate whether the products have been equally presented at each position, a simple cross-count between `Product` and `Rank` is done. This can easily be done using the `xtabs()` function:

```{r}
xtabs(~Product + Rank, data=dataset)
```

Such table can also be obtained using `group_by()` and `count()` to get the results in a tibble:

```{r}
dataset %>% 
  group_by(Product) %>% 
  count(Rank) %>% 
  ungroup() %>% 
  pivot_wider(names_from=Rank, values_from=n)
```

As we can see, the design is not perfectly balanced, as `choc2` is evaluated 11 times in the 1st, 4th, and 6th position, but only 7 times in the 5th position.

The next step then consists in evaluating how often each product is assessed before the other products (carry-over effect). Since this information is not directly available in the data, it needs to be added. 

Let's start with extracting the information available, i.e. the order of each product for each panelist and session:

```{r}
current <- dataset %>% 
  dplyr::select(Panelist, Product, Session, Rank) %>% 
  mutate(Rank = as.numeric(Rank))

```

An easy way to add the `Previous` product information as a new column in the data is by replacing `Rank` by `Rank + 1` in `current` (all new positions larger than the number of products is filtered). 

```{r}
previous <- current %>% 
  rename(Previous = Product) %>% 
  mutate(Rank = Rank + 1) %>% 
  filter(Rank <= length(unique(dataset$Product)))

```


This new data is merged to `current` by `Panelist`, `Session`, and `Rank`:

```{r}
(cur_prev <- current %>% 
  left_join(previous, by=c("Panelist", "Session", "Rank")))
```

As can be seen, the products that are evaluated first get `NA` in `Previous`, and for each rank r (r > 1), `Previous` gets the product that was evaluated at rank r-1.

To evaluate the carry-over effect, the only thing to do is to cross-count `Product` and `Previous` (here, the results are split per `Session`):

```{r}
cur_prev %>% 
  group_by(Session, Product, Previous) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(Product = factor(Product, levels=paste0("choc",1:6)),
         Previous = factor(Previous, levels=c("NA",paste0("choc",1:6)))) %>%
  arrange(Previous) %>% 
  pivot_wider(names_from=Previous, values_from=n, values_fill=0) %>% 
  arrange(Product) %>% 
  split(.$Session)

```

As expected, the table shows that a product is never evaluated twice in a row (the diagonal contains 0). Here again, the design is not optimal since `choc1` has been evaluated 3 times before `choc2` and 6 times before `choc5` in the first session.

Finally, the `NA` column refers to the time that products did not have a product tested before, meaning that they were evaluated first.

## Clean {#clean}

As mentioned in the introduction of this chapter, there is a thin line between *Data Inspection* and *Data Manipulation*, as both steps share many common practices. Here, we are limiting ourselves on handling variables and their type. For a full overview, we encourage the readers to look at (REF DATA MANIPULATION) to see other practices on how to handle data.

### Handling Data Type

The data used in this section is stored in *TFEQ.xlsx*. Let's import this file as suggested in (REFERENCE).

```{r}
library(readxl)
library(here)

file_path <- here("Data", "TFEQ.xlsx")

demo_var <- read_xlsx(file_path, sheet="Variables") %>% 
  dplyr::select(Code, Name)

demo_lev <- read_xlsx(file_path, sheet="Levels") %>% 
  dplyr::select(Question, Code, Levels) %>% 
  inner_join(demo_var, by=c("Question"="Code")) %>% 
  dplyr::select(-Question)

demographic <- read_xlsx(file_path, sheet="Data", skip=1, col_names=unlist(demo_var$Name))

```

In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another.

Remember that when your data set is stored in a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables' type can be assessed at any moment. 

```{r}
demographic
```

In case the data is not in a tibble, the use of the `str()` function becomes handy as it provides this information.

```{r}
str(demographic)
```

In sensory and consumer research, the four most common types are:

 - Numerical (incl. integer or `int`, decimal or `dcl`,  and double or `dbl`);
 - Logical or `lgl`;
 - Character or `char`;
 - Factor or `fct`.

R still has plenty of other types, for more information please visit: https://tibble.tidyverse.org/articles/types.html

#### Numerical Data

Since a large proportion of the research done is quantitative, it is no surprise that our data are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g. 1, 2, -16, etc.), or decimal value (or double, e.g. 1.6, 2.333333, -3.2 etc.).
By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double.

#### Binary Data

Another common type that seems to be numerical in appearance, but that has additional properties is the binary type. 
Binary data is data that takes two possible values (`TRUE` or `FALSE`), and are often the results of a *test* (e.g. is `x>3`? Or is `MyVar` numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires.

Note: Intrinsically, binary data is *numerical*, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the `sum()` function, as shown in the simple example below:

```{r example_logical}
set.seed(123456)
# Generating 10 random values between 1 and 10 using the uniform distribution
x <- runif(10, 1, 10)
x

# Test whether the values generated are strictly larger than 5
test <- x>5
test

# Counting the number of values strictly larger than 5
sum(test)

```

#### Nominal Data

Nominal data is any data that is not numerical nor binary. In most cases, nominal data are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes. But since the software cannot guess that those numbers are *identifiers* rather than *numbers*, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables is explained in the next section. 

For nominal data, two particular types of data are of interest: 

 - Character or `char`;
 - Factor or `fct`.
  
Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels: 

 - For `character`, there are no particular structure, and the variables can take any values (e.g. open-ended question);
 - For `factor`, the inputs of the variables are structured into `levels`.
 
To evaluate the number of levels, different procedures are required:

 - For `character`, one should count the number of unique element using `length()` and `unique()`;
 - For `factor`, the levels and the number of levels are directly provided by `levels()` and `nlevels()`.
 
Let's compare a variable set as `factor` and `character` by using a simple hand-made example:

```{r char_vs_fctr1}

(example <- demographic %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge)))

summary(example)

unique(example$Judge)
length(unique(example$Judge))

levels(example$Judge_fct)
nlevels(example$Judge_fct)

```

Although `Judge` and `Judge_fct` look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or building tables and graphs.

When set as `character`, the number of levels of a variable is directly read from the data, and its levels' order matches the way they appear in the data. This means that any data collected using a structured scale will often lose its natural order. 

When set as `factor`, the factor levels (including their order) are informed, and does not depend necessarily on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. In our case, the levels are read from the data an are automatically being reordered alphabetically (note that `J10` and `J100` appear before `J2` for instance.)

To illustrate this, let's re-arrange the levels from `Judge_fct` by ordering them numerically in such a way `J2` follows `J1` rather than `J10`.

```{r sorting_judge}
example <- demographic %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = factor(Judge, str_sort(Judge, numeric=TRUE)))
levels(example$Judge_fct)

```

Now the levels are sorted, let's 'remove' some respondents by only keeping the 20 first ones (J1 to J20), and re-run the previous code:

```{r char_vs_fctr2}
(example_reduced <- example %>%  
  filter(Judge %in% paste0("J",1:20)))

unique(example_reduced$Judge)
length(unique(example_reduced$Judge))

levels(example_reduced$Judge_fct)
nlevels(example_reduced$Judge_fct)

```

After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements (`J18` doesn't exist in the data), whereas the column set as factor still contains the 107 entries (most of them not having any recordings). 

```{r}
example_reduced %>% 
  count(Judge, .drop=FALSE)

example_reduced %>% 
  count(Judge_fct, .drop=FALSE)
```

This property can be seen as an advantage or a disadvantage depending on the situation:

 - For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of `factor`).
 - For hypothesis testing (e.g. ANOVA) on subset of data (e.g. the data being split by gender), the `Judge` variable set as `character` would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would still use the original count (so 106 here)!

The latter point is particularly critical since the analysis is incorrect and will either return an error or (worse!) return erroneous results!

Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily. 
Let's consider the `Living area` variable from `demographic` as example. From the original excel file, it can be seen that it has three levels, `1` corresponding to *urban area*, `2` to *rurban area*, and `3` to *rural area*. Let's start by renaming its levels:

```{r area_recode1}
example = demographic %>% 
  mutate(Area = factor(`Living area`, levels=c(1,2,3), labels=c("Urban", "Rurban", "Rural")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```

As can be seen, the variable `Area` is the factor version (including its labels) of `Living area`.
Let's now regroup `Rurban` and `Rural` together under `Rural`, and change the order by ensure that `Rural` appears before `Urban`:

```{r area_recode2}
example = demographic %>% 
  mutate(Area = factor(`Living area`, levels=c(2,3,1), labels=c("Rural", "Rural", "Urban")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```

This approach of renaming and re-ordering factor levels is very important as it simplifies the readability of tables and figures.
Some other transformations can be applied to factors thanks to the `{forcats}` package. Particular attention can be given to the following functions:

 - `fct_reorder()`/`fct_reorder2()` and `fct_relevel()` reorder the levels of a factor;
 - `fct_recode()` renames the factor levels (as an alternative to `factor()` used in the previous example);
 - `fct_collapse()` and `fct_lump()` aggregate different levels together (`fct_lump()` regroups automatically all the rare levels);
 - `fct_inorder()` takes uses the same order as read in the data (particularly useful with `pivot_longer()` for instance);
 - `fct_rev()` reverses the order of the levels (particularly useful in graphs).

Although it hasn't been done here, manipulating strings is also possible through the `{stringr}` package, which provides interesting functions such as:

 - `str_to_upper()`/`str_to_lower()` to convert strings to uppercase or lowercase;
 - `str_c()`, `str_sub()` combine or subset strings;
 - `str_trim()` and `str_squish()` remove white spaces;
 - `str_extract()`, `str_replace()`, `str_split()` extract, replace, or split strings or part of the strings;
 - `str_sort()` to order alphabetically (or by respecting numbers, as shown previously) its elements.
 
### Converting between Types

When importing data, variables may not always be associated to the right type (remember, each type has its own properties). For example, respondents or products coded numerically would be defined as integers rather than strings after importation. Without manual correction, this would lead to the wrong analyses! Hence, they need to be converted to the right type.  

In the following sections, the function `mutate()` is used to to create a new variable that corresponds to the original one after being converted to its new type (as in the previous example with `Area`). By renaming the new variable with the same name as the original one, we overwrite it by simply changing its type, here.  

Based on our variable types of interest, there are two main conversions to run:
 - From numerical to character/factor;
 - From character/factor to numerical.
 
The conversion from numerical to character or factor is simply done using `as.character()` and `as.factor()` respectively. Note however that `as.factor()` only converts into factors without allowing to chose the order of the levels, nor to rename them. Alternatively, the use of `factor()` allows specifying the `levels` (and hence the order of the levels) and their corresponding `labels`. An example in the use of `as.character()` and `as.factor()` was provided in the previous section when we converted the `Respondent` variables to character and factor. The use of `factor()` was also used earlier when the variable `Living area` was converted from numerical to factor (called `Area`) with labels.

To illustrate the following points, let's start with creating a tibble with two variables, one containing strings made of numbers, and one containing strings made of text.

```{r simple_example}
example <- tibble(Numbers = c("2","4","9","6","8","12","10"),
                  Text = c("Data","Science","4","Sensory","and","Consumer","Research"))
```

The conversion from character to numerical is straight forward and requires the use of the function `as.numeric()`:

```{r char_2_num}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```

As can be seen, when strings are made of numbers, the conversion works fine. However, the text are not converted properly and returns NAs.

Now let's apply the same principle to a variable of the type factor. To do so, we will take the same example but first convert the variables from character to factor:


```{r char_2_fctr}
example <- example %>% 
  mutate(Numbers = as.factor(Numbers)) %>% 
  mutate(Text = factor(Text, levels=c("Data","Science","4","Sensory","and","Consumer","Research")))
```

Let's apply as.numeric() to these variables:

```{r fctr_2_num}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```

We can notice here that the outcome is not really as expected as the numbers 2-4-9-6-8-12-10 becomes 3-4-7-5-6-2-1, and Data-Science-4-Sensory-and-Consumer-Research becomes 1-2-3-4-5-6-7. The rationale behind this conversion is that the numbers do not reflects the string itself, but the position of that level in the factor level order.

To convert properly numerical factor levels to number, the variable should first be converted as character:

```{r fctr_2_char_2_num}
example %>%
  mutate(Numbers = as.numeric(as.character(Numbers)))
```

### Conclusions

As can be seen, it is very important to verify (and correct!) the type of each variable as it can lead to running the wrong analysis otherwise. When using tibbles, such inspection is quite straightforward since printing the table on screen automatically shows the type of each variable. However, when presented as a matrix or as a data frame, such information is not directly visible:

```{r}
demographic %>% 
  as.data.frame()
```

Since each type has its advantages and drawbacks, it is convenient to regularly transit from one to another. This procedure will occur very often in the next sections, which will give you the chance to get accustomed to the procedure.
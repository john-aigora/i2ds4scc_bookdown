```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```
# Data Analysis {#data-analysis}

In this chapter, the different techniques presented in \@ref(data-manip) will be applied to our biscuit data.
The analyses presented are non-exhaustive, but tackle some well-known analyses often used in sensory and consumer science.

The following sections are divided based on the type of data to consider and their corresponding analysis.


## Sensory Data


Let's start with the analysis of our sensory data. If not done already, import the file *Sensory Profile.xlsx*.
To do so, we need the `{here}` and `{readxl}` packages. Since most of the analyses we are going to perform require tools from the `{tidyverse}`, we also load this group of packages.


```{r}
library(tidyverse)
library(here)
library(readxl)

file_path <- here("data", "Sensory Profile.xlsx") 
p_info <- read_xlsx(file_path, sheet = "Product Info") %>% 
  dplyr::select(-Type)

sensory <- read_xlsx(file_path, sheet="Data") %>% 
  inner_join(p_info, by="Product") %>% 
  relocate(Protein:Fiber, .after=Product)

```


The first analysis often done consists in evaluate whether there are differences between samples for the different attributes. This is done through Analysis of Variance (ANOVA) and can be done using the `lm()` or `aov()` functions.
If we would want to run the ANOVA for `Shiny`, the code would look like this (here we consider the 2-way ANOVA evaluating the Product and Assessor effects):


```{r}
shiny_aov <- lm(Shiny ~ Product + Judge, data=sensory)
anova(shiny_aov)
```


We could duplicate this code for each single attributes, but 1. this would be quite tedious if the number of attributes increase, 2. it is sensitive to the way the variables are being called, meaning that 3. it would usually not be applicable to other dataset. Instead, we propose two solutions, one using `split()` combined with `map()` and one involving `nest_by()` to run this analysis automatically.

For both these solutions, the dataset should be in the long and short form, which can be obtained using `pivot_longer()`:

```{r}
senso_aov_data <- sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Score")
```


From this structure, the first approach consists in splitting the data by attribute. Once done, we run for each sub-data the ANOVA (the model is then defined as `Score ~ Product + Judge`), and we extract the results of interest using the `{broom}` package^[The `{broom}` package is a very useful package that convert statistical objects into tidy tables.] 
Ultimately, the results can be combined again using `enframe()` and `unnest()`.


```{r}
senso_aov1 <- senso_aov_data %>% 
  split(.$Attribute) %>% 
  map(function(data){
    
    res <- broom::tidy(anova(lm(Score ~ Product + Judge, data=data)))
    return(res)
    
  }) %>% 
  enframe(name="Attribute", value="res") %>% 
  unnest(res)

```


The second approach nests the analysis by attribute (meaning the analysis is done for each attribute separately, a bit like `group_by()`): in this case, we create a new variable called `mod` which takes the results of the ANOVA.
Once the analysis is done, we summarize the info stored in `mod` by converting into a tibble using `{broom}`:


```{r}
senso_aov2 <- senso_aov_data %>% 
  nest_by(Attribute) %>% 
  mutate(mod = list(lm(Score ~ Product + Judge, data=data))) %>% 
  summarise(broom::tidy(anova(mod))) %>% 
  ungroup()

```


The two approaches return the exact same results. Let's dig into the results by checking for which attributes there are no significant differences at 5%. Since the `tidy()` function from `{broom}` tidies the data, all the usual data transformation can be performed. Let's filter only the `Product` effect under `term`, and let's order decreasingly the `p.value`:


```{r}
senso_aov1 %>% 
  filter(term == "Product") %>% 
  dplyr::select(Attribute, statistic, p.value) %>% 
  arrange(desc(p.value)) %>% 
  mutate(p.value = round(p.value, 3))
```


As can be seen, the products do not show any significant differences at 5% for 4 attributes: `Cereal flavor` (p=0.294), `Roasted odor` (p=0.193), `Astrigent` (p=0.116), and `Sticky` (p=0.101).

To visualize graphically the results of the ANOVA, a barchart representing the F-value can be generated. We recommend to order the attributes based on their decreasing F-values, and by colour-coding them based on their significance:


```{r}
senso_aov1 %>% 
  filter(term == "Product") %>% 
  dplyr::select(Attribute, statistic, p.value) %>% 
  mutate(Signif = ifelse(p.value <= 0.05, "Signif.", "Not Signif.")) %>% 
  mutate(Signif = factor(Signif, levels=c("Signif.", "Not Signif."))) %>% 
  ggplot(aes(x=reorder(Attribute, statistic), y=statistic, fill=Signif))+
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("Signif."="forestgreen", "Not Signif."="orangered2"))+
  ggtitle("Sensory Attriubtes","(The attributes are sorted according to the F-values)")+
  theme_bw()+
  xlab("")+
  ylab("F-values")+
  coord_flip()

```


It appears that the different biscuits differ the most (top 5) for `Initial hardness`, `Shiny`, `Dairy flavor`, `External color intensity`, and `Thickness`. 

Note that as an alternative, we could use the `decat()` function from the `{SensoMineR}` package. This function performs ANOVA on a set of attributes (presented in subsequent columns), followed by some t-test that highlights which samples are significantly more (or less) intense than average for each attribute.


Once the significant differences have been checked, a follow-up analysis consists in visualizing these differences. Such visualization is often done through Principal Component Analysis (PCA).

In practice, PCA is performed on the mean table crossing the products in rows, and the sensory attributes in columns. In \@ref(data-manip), many different approaches to generate such table have been proposed:


```{r}
senso_mean <- sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Score") %>% 
  dplyr::select(-Judge) %>% 
  pivot_wider(names_from=Attribute, values_from=Score, values_fn=mean)

```


Such table can be submitted to PCA. To do so, the `PCA()` function from the `{FactoMineR}` is used. 
Few important remarks: The `PCA()` function usually contains only numerical data, the product names being often set as row names in a data frame. This is something that can easily be taken care of through the `column_to_rownames()` function from the `{tibble}` package. Additionally, one might notice that we still have two qualitative variables: `Protein` and `Fiber`. These 2 variables can either be removed prior to running the analysis, or better, projected as supplementary variables through the `quali.sup` parameter from `PCA()`. 


```{r}
library(FactoMineR)

senso_pca <- senso_mean %>% 
  as.data.frame() %>% 
  column_to_rownames(var="Product") %>% 
  PCA(., quali.sup=1:2, graph=TRUE)

```


The `PCA()` function can generate the plots either in `{base}` R language, or in `{ggplot2}`. However, we like to use a complementary package called `{factoextra}` which re-creates any plot from `{FactoMineR}` (and more) as a `ggplot()` object. for `PCA()`, the main function used to re-create the plots are `fviz_pca_ind()` for the products' map, `fviz_pca_var()` for the variables' representation, and `fviz_pca_biplot()` for the biplot combining both.
Since we have two qualitative variables, let's take advantage of them by coloring for each of them the products based on their content. This can easily be done through the `habillage` parameter from `fviz_pca_ind()`, which can either take a numerical value (position) of the name of the qualitative variable.


```{r}
library(factoextra)

fviz_pca_ind(senso_pca, habillage="Protein", repel=TRUE)
fviz_pca_ind(senso_pca, habillage=2, repel=TRUE)
fviz_pca_var(senso_pca)
fviz_pca_biplot(senso_pca)

``` 


On the first dimension, `P10` is opposed to `P09` and `P03` as it is more intense for attributes such as `Sweet`, and `Dairy flavor` for example, and less intense for attributes such as `Dry in mouth` and `External color intensity`. On the second dimension, `P08`, `P06`, and `POpt` are opposed to `P02` and `P07` as they score higher for `Qty of inclusions`, and `Initial hardness`, and score lower for `RawDough flavor` and `Shiny`. 


## Demographic and TFEQ Data


In the *TFEQ.xlsx* file, descriptive (e.g. demographic) and behavioral (e.g. TFEQ) information regarding the consumers is being stored.

A quick look at this file shows that it contains three tabs denoted as *Data*, *Variables*, and *Levels*:

- *Data* contains the data, coded;
- *Variables* provides information (e.g. name, information) related to the different variables present in Data;
- *Levels* provides information about the different levels each variable can take.

Let's start with importing this data set. The importation is done in multiple step as following:


```{r}
file_path <- here("Data", "TFEQ.xlsx")
excel_sheets(file_path)

demo_var <- read_xlsx(file_path, sheet="Variables") %>% 
  dplyr::select(Code, Name)

demo_lev <- read_xlsx(file_path, sheet="Levels") %>% 
  dplyr::select(Question, Code, Levels) %>% 
  inner_join(demo_var, by=c("Question"="Code")) %>% 
  dplyr::select(-Question)

demographic <- read_xlsx(file_path, sheet="Data", skip=1, col_names=unlist(demo_var$Name))

```


### Demographic Data: Frequency and Proportion


For this demographic data file, we are interested to see the partition of consumers for each of the descriptive variables. More precisely, we would like to know the frequency and proportion (in percentage) attached to each level of `Living area`, `Housing`, `Income range`, and `Occupation`. 

To obtain such table, let's start by only selecting these particular columns together with `Judge`. 
Since the data is coded, let's decode it using the information stored in `demo_lev`: we could automatically recode each variable using `mutate()` and `factor()`. However, we prefer here to use `pivot_longer()` to create a long thin tibble, and then to merge this table with `demo_lev` by `Question` and `Response` using `inner_join()` instead. Once done, we can aggregate the results by `Question` and `Levels` (since we want to use the level information, not their code) and compute the frequency (`n()`) and the proportion (`N/sum(N)`)^[We use the package `{formattable}` to print the results in percentage using one decimal. As an alternative, we could have used `percent()` from the `{scales}` package.].


```{r}
library(formattable)

demog_reduced <- demographic %>% 
  dplyr::select(Judge, `Living area`, Housing, `Income range`, `Occupation`) %>% 
  pivot_longer(-Judge, names_to="Question", values_to="Response") %>% 
  inner_join(demo_lev, by=c("Question"="Name", "Response"="Code")) %>% 
  group_by(Question, Levels) %>% 
  summarize(N = n()) %>% 
  mutate(Pct = percent(N/sum(N), digits=1L)) %>% 
  ungroup()
```


To better visualize these different proportions, histograms are often considered. Such histograms can be obtained by splitting `demog_reduced` by `Question` and by creating in each case an histogram using either `N` or `Pct` (we are using `Pct`)^[Here, the histograms are ordered decreasingly (`reorder`) and are represented horizontally (`coord_flip()`).]. Of course, we automate this across all questions using `map()`.


```{r}
demog_reduced %>% 
  split(.$Question) %>% 
  map(function(data){
    
    var = as.character(unlist(data$Question))
    
    ggplot(data, aes(x=reorder(Levels, Pct), y=Pct, label=Pct))+
      geom_bar(stat="identity", fill="grey50")+
      geom_text(aes(y = Pct/2), colour="white")+
      xlab("")+
      ylab("")+
      ggtitle(var)+
      theme_bw()+
      coord_flip()
    
  })
```


### Behavioral Data: TFEQ Traits


In the same data set, consumers also answered some behavioral questions that reflects on their relation to food. These questions can be categorized into three groups: Disinhibitor (variables starting with `D`), Restriction (variables starting with `R`), and Hunger (variables starting with `H`).

Let's start by selecting all these variables. We could use `select()` combined with `starts_with("D")`, `starts_with("R")`, and `starts_with("H")`, but this solution is not satisfying since it can also seelct other variables that would start with any of these letters (e.g. Housing). Since the variables are all well structured (the all start with the letters D, R, or H, followed by a number), the use of a regular expression is more appropriate.

In practice, generating regular expression can be quite complex as it is an abstract concept which follow very specific rules. Fortunately, the package `{RVerbalExpression}` is a great assistant as it generates the regular expression for you thanks to understandable functions. First, we initiate the regular expression by calling the function `rx()` to which we add that we first want any variables that start with any of the letter R, D, or H, followed by a number (or more, as values go from 1 to 21). This can be done using the following code:


```{r}
library(RVerbalExpressions)

rdh <- rx() %>% 
  rx_either_of(c("R","D","H")) %>% 
  rx_digit() %>% 
  rx_one_or_more()

rdh

```


`rdh` contains the regular expression that we need. We can then `dplyr::select()` any variable that fits our regular expression by using the function `matches()`.


```{r}
demographic %>% 
  dplyr::select(matches(rdh))
```


Since we would like to create a frequency table for each of these variables, we create a function called `myfreq()` which will automatically compute the frequency for each level, and the corresponding percentage.


```{r}
myfreq <- function(data, info){
  
  var = unique(unlist(data$TFEQ))
  info <- info %>% 
    filter(Name == var)
  
  res <- data %>% 
    mutate(Response = factor(Response, levels=info$Code, labels=info$Levels)) %>% 
    arrange(Response) %>% 
    group_by(Response) %>% 
    summarize(N = n()) %>% 
    mutate(Pct = percent(N/sum(N), digits=1L)) %>% 
    ungroup()
  
  return(res)
}

```


We then apply this function to each variable separately using `map()` after pivoting all these variables of interest (`pivot_longer()`) and splitting the data by `TFEQ` question:


```{r}
TFEQ_freq <- demographic %>% 
  dplyr::select(Judge, matches(rdh)) %>% 
  pivot_longer(-Judge, names_to="TFEQ", values_to="Response") %>% 
  split(.$TFEQ) %>%
  map(myfreq, info=demo_lev) %>% 
  enframe(name = "TFEQ", value="res") %>% 
  unnest(res) %>% 
  mutate(TFEQ = factor(TFEQ, levels=unique(str_sort(.$TFEQ, numeric=TRUE)))) %>% 
  arrange(TFEQ)
```


Similarly, we can create histograms representing the frequency for each variable. But here we limit ourselves to variables related to Disinhibitor. To do so, we start by generating the right regular expression (only selecting variables starting with "D") that we used to filter the results, before creating the corresponding graph:


```{r}
d <- rx() %>% 
  rx_find("D") %>% 
  rx_digit() %>% 
  rx_one_or_more()

TFEQ_freq %>% 
  filter(str_detect(TFEQ, d)) %>% 
  ggplot(aes(x=Response, y=Pct, label=Pct))+
  geom_bar(stat="identity", fill="grey50")+
  geom_text(aes(y = Pct/2), colour="white")+
  theme_bw()+
  facet_wrap(~TFEQ, scales="free")
```


Ultimately, the TFEQ questionnaire consists in a set of structured questions whose respective answers combine to provide a TFEQ score. This TFEQ score translates into certain behavior towards food. Similar structured questionnaire could include IQ test, Food Neophobia, etc.

However, it appears that the TFEQ score is not computed by simply adding the scores of all TFEQ questions together. Instead, it follows certain rules that are stored in the *Variables* sheet in *TFEQ.xlsx*. For each TFEQ question, the rule to follow is provided by `Direction` and `Value`, and works as following: if the condition provided by `Direction` and `Value` is met, then the respondent gets a 1, else a 0. Ultimately, the TFEQ is the sum of all these evaluations.

Let's start by extracting this information (`Direction` and `Value`) from the sheet Variables for all the variables involved in the computation of the TFEQ score. We store this in `var_drh`.


```{r}
var_rdh <- read_xlsx(file_path, sheet="Variables") %>% 
  filter(str_detect(Name, rdh)) %>% 
  dplyr::select(Name, Direction, Value)
```


This information is added to `demographic`. Since we need to evaluate each assessors' answer to the TFEQ questions, we create a new variable `TFEQValue` which takes a 1 if the corresponding condition is met, and a 0 otherwise. Such approach is done through `mutate()` combined with a succession of intertwined `ifelse()` functions.^[The function `ifelse()` takes three parameters: 1. the condition to test, 2. the value or code to run if the condition is met, and 3. the value or code to run if the condition is not met.]

Ultimately, we compute the TFEQ Score by summing across all `TFEQValue` per respondent (here, we compute the sum per category first before computing the grand total per respondent).


```{r}
TFEQ <- demographic %>% 
  dplyr::select(Judge, matches(rdh)) %>% 
  pivot_longer(-Judge, names_to="DHR", values_to="Score") %>% 
  inner_join(var_rdh, by=c("DHR"="Name")) %>% 
  mutate(TFEQValue = ifelse(Direction == "Equal" & Score == Value, 1, 
                        ifelse (Direction == "Superior" & Score > Value, 1,
                                ifelse(Direction == "Inferior" & Score < Value, 1, 0)))) %>% 
  mutate(Factor = ifelse(str_starts(.$DHR, "D"), "Disinhibitor",
                         ifelse(str_starts(.$DHR, "H"), "Hunger", "Restriction"))) %>% 
  mutate(Factor = factor(Factor, levels=c("Restriction","Disinhibitor","Hunger"))) %>%
  group_by(Judge, Factor) %>% 
  summarize(TFEQ = sum(TFEQValue)) %>% 
  mutate(Judge = factor(Judge, levels=unique(str_sort(.$Judge, numeric=TRUE)))) %>% 
  arrange(Judge) %>% 
  pivot_wider(names_from=Factor, values_from=TFEQ) %>% 
  mutate(Total = sum(across(where(is.numeric))))
```


Such results can then be visualized graphically, for instance by representing the distribution of the TFEQ score for the 3 categories of questions (or TFEQ-factor):


```{r}
TFEQ %>% 
  dplyr::select(-Total) %>% 
  pivot_longer(-Judge, names_to="Factor", values_to="Scores") %>% 
  ggplot(aes(x=Scores, colour=Factor))+
  geom_density(lwd=1.5)+
  xlab("TFEQ Score")+
  ylab("")+
  ggtitle("Distribution of the Individual TFEQ-factor Scores")+
  theme_bw()

```


## Consumer Data


The analysis of consumer data involves the same type of analysis as the ones for sensory data (e.g. ANOVA, PCA, etc.), but the way the data is being collected (absence of duplicates) and its underlying nature (affective vs. descriptive) require some adjustments.

So let's start by importing the consumer data that is stored in *Consumer Test.xlsx*. Here we import two sheets, one with the consumption time and number of biscuits (stored in `Nbiscuit`), and one with different consumer evaluations of the samples (stored in `consumer`)


```{r}
file_path <- here("Data","Consumer Test.xlsx")

Nbiscuit <- read_xlsx(file_path, sheet="Time Consumption") %>% 
  mutate(Product = str_c("P", Product)) %>% 
  rename(N = `Nb biscuits`)

consumer <- read_xlsx(file_path, sheet="Biscuits") %>% 
  rename(Judge=Consumer, Product=Samples) %>% 
  mutate(Judge = str_c("J", Judge), Product = str_c("P", Product)) %>% 
  inner_join(Nbiscuit, by=c("Judge", "Product"))

```


Similarly to the sensory data, let's start with computing the mean liking score per product after the first bite (`1stbite_liking`) and at the end of the evaluation (`after_liking`). 


```{r}
consumer %>% 
  dplyr::select(Judge, Product, `1stbite_liking`, `after_liking`) %>% 
  group_by(Product) %>% 
  summarise(across(where(is.numeric), mean))

```


A first glance at the table show that there are clear differences between the samples (within a liking variable), but little difference between liking variables(within a sample).

Ultimately, since we want to see if there are differences between samples, an ANOVA (testing for the product effect) followed up by a paired comparison test (here Tukey's HSD) is performed. To do so, we recommend the `{agricolae}` package as it is simple to use and all their build in tests are working in the same way.


```{r}
library(agricolae)

liking_start <- lm(`1stbite_liking` ~ Product + Judge, data=consumer)
liking_start_hsd <- HSD.test(liking_start, "Product")$groups %>% 
  as_tibble(rownames = "Product")

liking_end <- lm(`after_liking` ~ Product + Judge, data=consumer)
liking_end_hsd <- HSD.test(liking_end, "Product")$groups %>% 
  as_tibble(rownames = "Product")
```


Both at the start and at the end of the evaluation, significant differences (at 5%) in liking between samples are observed according to Tukey's HSD test. 
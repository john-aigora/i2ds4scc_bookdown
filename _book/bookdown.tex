% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{krantz}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\captionsetup[table]{labelsep=space}
\captionsetup[figure]{labelsep=space}
\usepackage[scale=.8]{sourcecodepro}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\usepackage{hyperref}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage{fontspec}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Data Science for Sensory and Consumer Scientists},
  pdfauthor={Thierry Worch, Julien Delarue, Vanessa Rios de Souza, and John Ennis},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Data Science for Sensory and Consumer Scientists}
\author{Thierry Worch, Julien Delarue, Vanessa Rios de Souza, and John Ennis}
\date{2023-01-12}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
To Luca,

To Bella,

...
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Welcome to the website for \emph{Data Science for Sensory and Consumer Scientists}, a book in development and under contract for \href{https://www.routledge.com/}{CRC Press}.

\includegraphics[width=0.9\linewidth]{images/cover_art}

\hypertarget{who-should-read-this-book}{%
\section*{Who Should Read This Book?}\label{who-should-read-this-book}}


This book is for practitioners or students of sensory and consumer science who want to participate in the emerging field of computational sensory science. This book assumes little to no coding experience. Some statistical experience will be helpful to understand the examples discussed.

\hypertarget{how-is-this-book-structured}{%
\section*{How Is This Book Structured}\label{how-is-this-book-structured}}


It is important to start by saying that the aim for this book \textbf{is not} to explain in depth what the different sensory and consumer methods are, nor to explain how the data gathered from these methods should be analyzed. For such topics, other excellent books including \citet{Le2018}, OTHER REFS for example are available.

Instead, the aim is to explain the workflow sensory and consumer scientists can adopt to become more efficient, and to push their analyses further. The workflow proposed includes many steps including:

\begin{itemize}
\tightlist
\item
  Setting up the test both from an experimental design and an analysis perspective (e.g.~setting up projects, collaboration tools, etc.);
\item
  Data collection and data processing through data cleaning, data manipulation and transformation, and data analysis;
\item
  Communication of the results (e.g.~visualization, reporting, communication).
\end{itemize}

For this journey, R is used as the preferred platform to assist us in this data science journey. Moreover, this book is written following a restaurant' card' menu with four main sections:

\begin{itemize}
\tightlist
\item
  \emph{Apéritifs} provides tips, tricks, and tools for the readers to get accustomed to the overall style of this book, an excellent way to make you salivate on what's about to come!
\item
  \emph{Hors D'Oeuvres} aims in describing in more depth some of the tools that will be used frequently all along the book. Just a neat way to open your appetite!
\item
  \emph{Bon Appétit} is an illustration of the workflow proposed applied to a real-case study on biscuits. This is the main course that should satisfy your cravings!
\item
  \emph{Haute Cuisine} is an introduction of the current workflow to other types of data and/or more advanced analysis, a particularly tasty chapter for the most \emph{gourmand}.
\item
  \emph{Digestifs} extends to other relevant topics that did not fit within this book's scope, but can be relevant for many readers. This is a perfect way to finish the journey and hopefully give a good taste on new possibilities!
\end{itemize}

\hypertarget{how-to-use-this-book}{%
\section*{How To Use This Book}\label{how-to-use-this-book}}


This book is meant to be interactive, with the reader ideally typing and running all of the code presented in the book. Computer languages are like human languages in that they need to be practiced to be learned, so we highly recommend the reader actually typing all of the code from these parts, running it, and verifying they obtain the results shown in the book. To help with this practice, we have created a \href{https://github.com/aigorahub/data_science_for_sensory_sample_code}{special GitHub repository} that contains folders called \texttt{work\_along} and \texttt{sample\_code}. Please see Chapter \ref{start-R} for guidance on how to get started with R and GitHub. In the \texttt{sample\_code} folder, we have included the code as presented in the book, while in the \texttt{work\_along} folder we have provided blank files for each chapter that load the libraries\footnote{You may need to load these libraries before the code will run, please see Appendix \ref{start-R} for more information on this topic as well.}.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


First and foremost we all thank our partners for their continual support. Arkadi Avanesyan, Bartosz Smulski, Jakub Kwiecien, Tian Yu, and Veerle van Leemput all contributed to the content in various important ways.

\hypertarget{about-the-author}{%
\chapter*{About the Author}\label{about-the-author}}


Frida Gomam is a famous lady. Police will always let her go.

\mainmatter

\hypertarget{bienvenue}{%
\chapter{Bienvenue!}\label{bienvenue}}

\hypertarget{why-data-science-for-sensory-and-consumer-science}{%
\section{Why Data Science for Sensory and Consumer Science?}\label{why-data-science-for-sensory-and-consumer-science}}

Located at the crossroads of biology, social science, and design, sensory and consumer science (SCS) is definitely the tastiest of all sciences. On the menu is a wide diversity of products and experiences that sensory and consumer scientists approach through the lens of human senses. Thanks to a wide set of refined methods, they have access to rich, complex and mouthwatering data. Delightedly, data science empowers them and leads them to explore new flavorful territories.

\hypertarget{core-principles-in-sensory-and-consumer-science}{%
\subsection{Core principles in Sensory and Consumer Science}\label{core-principles-in-sensory-and-consumer-science}}

Sensory and consumer science is considered as a pillar of food science and technology and is essential to product development, quality control and market research. Most scientific and methodological advances in the field are applied to food. This book makes no exception as we chose a cookie formulation data set as a main thread. However, SCS widely applies to many other consumer goods; so are the content of this book and the principles set out below.

\hypertarget{measuring-and-analyzing-human-responses}{%
\subsubsection{Measuring and analyzing human responses}\label{measuring-and-analyzing-human-responses}}

Sensory and consumer science aims at measuring and understanding consumers' sensory perceptions as well as the judgement, emotions and behaviors that may arise from these perceptions. SCS is thus primarily a science of measurement, although a very particular one that uses human beings and their senses as measuring instruments. In other words, sensory and consumer researchers measure and analyze human responses.

To this end, SCS relies essentially on sensory evaluation which comprises a set of techniques that mostly derive from psychophysics and behavioral research. It uses psychological models to help separate signal from noise in collected data \citep{lee2004, Ennis2016Thurstonian}. Besides, sensory evaluation has developed its own methodological framework that includes most refined techniques for the accurate measurement of product sensory properties while minimizing the potentially biasing effects of brand identity and the influence of other external information on consumer perception \citep{LawlessHeym2010}.

A detailed description of sensory methods is beyond the scope of this book and many textbooks on sensory evaluation methods are available to readers seeking more information. However, just to give a brief overview, it is worth remembering that sensory methods can be roughly divided into three categories, each of them bearing many variants:

\begin{itemize}
\tightlist
\item
  Discrimination tests that aim at detecting subtle differences between products.
\item
  Descriptive analysis (DA), also referred to as `sensory profiling', aims at providing both qualitative and quantitative information about products' sensory properties.
\item
  Affective tests. This category includes hedonic tests that aim at measuring consumers' liking for the tested products or their preferences among a product set.
\end{itemize}

Each test category generates its own type of data and related statistical questions in relation to the objectives of the study. Typically, data from difference tests with forced-choice procedures (e.g.~triangle test, duo-trio, 2-AFC, etc.) consist in series of binary answers (correct/failed) depending on whether judges successfully picked the odd sample(s) among a set of three or more samples \footnote{Other procedures like the \emph{different from control} test or the \emph{degree of difference} test generate rating data}. These data are used to determine whether the number of correct choices is above the level expected by chance \citep[see][ for an overview of these methods, the related theories and experimental factors]{OMahonyRousseau2003}.

Conventional descriptive analysis data consist in intensity scores given by each panelist to evaluated samples on a series of sensory attributes, hence resulting in a product x attribute x panelist dataset (Figure \ref{fig:DAdata}). Note that depending on the DA method, quantifying means other than intensity ratings can be used (ranks, frequency, counts, etc.). Most frequently, each panelist evaluates all the samples in the product set. However, the use of a balanced incomplete design can also be found when the experimenters aim to limit the number of samples evaluated by each subject.

\begin{figure}
\includegraphics[width=0.9\linewidth]{images/DA_data} \caption{Typical structure of a Descriptive Analysis data set.}\label{fig:DAdata}
\end{figure}

Eventually, datasets from hedonic tests consist of hedonic scores (i.e.~degrees of liking, or preference ranks) given by each interviewed consumer to a series of products (Figure \ref{fig:hedodata}). As in the case of DA, each consumer usually evaluates all the samples in the product set, but balanced incomplete designs are sometimes used too. In addition, some companies favor pure monadic evaluation of products (i.e.~between-subject design or independent groups design) which obviously result in unrelated sample datasets.

\begin{figure}
\includegraphics[width=0.9\linewidth]{images/hedo_data} \caption{Two-way hedonic data from a consumer test where *n* consumers have evaluated a series of products.}\label{fig:hedodata}
\end{figure}

Sensory and consumer researchers also borrow methods from other fields, in particular from sociology and experimental psychology. As a result, it is now frequent to collect textual sensory data from open comments and qualitative interviews, or sensory distances or co-occurrences from projective and sorting tasks. Definitely a multidisciplinary area, SCS develops in many directions and reaches disciplines that range from genetics and physiology to social marketing, behavioral economics and computational neuroscience. So have diversified the types of data sensory and consumer scientists must deal with. As in many scientific fields, the development of sophisticated statistical techniques and access to powerful data analysis tools have played an important role in the evolution of sensory \& consumer science. Statisticians and data analysts in SCS have developed their own field of research, coined Sensometrics \citep{Schlich1993, Brockhoff2011, Qannari2017}. Now then, what makes sensory \& consumer science special? And how does it influence the way sensory and consumer data are handled?

\hypertarget{dealing-with-human-diversity}{%
\subsubsection{Dealing with human diversity}\label{dealing-with-human-diversity}}

Sensory evaluation attempts to isolate the sensory properties of foods and provides important and useful information about these properties to product developers, food scientists, and managers \citep{LawlessHeym2010}. However, one should bear in mind that these `sensory properties' actually result from the interaction between the object (the food) and the perceiver of that object (the consumer). In fact, we may very well consider the true object of evaluation in SCS to be mental representations. They are nonetheless very concrete and directly impact behaviors, health and economic decisions \citep{Kahneman2000}.
A direct consequence of this is that sensory data depend both on the product to be evaluated and on the subjects who evaluate the product. Because people are different, individual sensory data are expected to differ accordingly. In its core principle, SCS recognizes the diversity of human beings, biologically, socially and culturally speaking, not to mention the fact that each individual has their own personal history and experience with products. In short, people perceive things differently and like different things. For this reason, SCS only relies on groups of people (i.e.~a panel of judges, a sample of consumers) and never on a single individual's response. Yet, sensory and consumer scientists usually collect individual data and analyze them at a refined level of granularity (individual, subgroups) before considering larger groups (specific populations or marketing targets).

This said, sensory and consumer studies must lead to operational recommendations. They are used to make informed decisions on product development, to launch new product, and sometimes to help define food and health policies. Data science can precisely help sensory and consumer scientists to reach those objectives while taking diversity into account.

For measures of sensory description, sensory and consumer scientists can manage the diversity of human responses to a certain extent by training panels to use a consensual vocabulary, by aligning evaluated concepts and calibrating the quantification of evaluations on scales \citep{Bleibaum2020}. However, this won't eliminate interindividual differences in sensitivity, simply because we are genetically different, on top of differences due to age, physiological state, illness, etc. Nowadays, as the field becomes more and more consumer-oriented, it becomes clear that the use of several subjects in a panel cannot be assimilated to a mere repetition of individual measurements. Accordingly, sensory methods have been developed to allow panelists to better express their own perceptions and to get a more accurate picture of how people perceive products \citep{Varela2012}. These methods yield richer and more complex data that require more advanced analysis techniques to extract relevant and actionable information. Historically, one the first methodological innovations in this direction has been the use of free choice profiling combined to Generalized Procrustes Analysis \citep{Williams1984}. Since then, sensory and data analysis methods have multiplied greatly \citep{Delarue2022}. Naturally, data science has become even more crucial to accompany this evolution.

As regards hedonic tests (liking, acceptance, preference\ldots), the measurements are in essence subjective and participants to such tests are by definition `untrained' consumers. A constant outcome of these tests is to find important interindividual differences and it is very common to find consumers who have opposite sensory preference patterns. Clustering and segmentation techniques are thus routinely applied to consumer data. One difficulty though, is to link these differences in preferences to other consumer variables, should they be sociodemographic, psychographic, or related to usage and attitudes. Most often, one set of variables (e.g.~demographics) is not enough to fully explain preference patterns. In saturated and ever changing markets however, being able to understand individual needs and preferences is critical should one intend to develop customized products. This makes the understanding of consumer segments even more important. Nowadays, these segments go far beyond sensory preferences and must take into account variables that touch environmental awareness and sustainability dimensions.

\hypertarget{specificities-of-data-handled-in-sensory-and-consumer-science}{%
\subsubsection{Specificities of data handled in sensory and consumer science}\label{specificities-of-data-handled-in-sensory-and-consumer-science}}

Sensory and consumer data are usually of relatively small size. Indeed, we often deal with a number of subjects ranging between a dozen (for trained panels) and few hundreds (for consumer hedonic tests). Of course, when multiplied by the number of samples being evaluated by each subject, we would get a much larger numbers of observations, but this will still be relatively modest compared with so-called big data generated everywhere online. The same goes with the number of variables in sensory data sets. Sensory descriptive analysis, for example, typically relies on 10 to 50 attributes, which could be seen as a lot but is in fact much less than in other fields producing experimental data with thousands of variables like chemometrics, genomics, etc.

This being said, it must be stressed that sensory and consumer data are very diverse. Indeed, the need to understand perceptions and preferences often leads sensory and consumer scientists to deal with multiple datasets, each possibly comprising various data types. Most sensory techniques yield quantitative (e.g.~intensity, similarity, hedonic) data collected from rating scales or ranking tasks, but other methods would provide inter-product distances (e.g.~napping), co-occurrences (e.g.~free sorting), citation frequencies (e.g.~CATA), or texts (e.g.~open-ended comments, natural speech). Besides, agreement scores from Likert scales would often be used when sensory studies are combined with usage and attitude surveys or psychometrics questionnaires. To add richer information, but more complexity to this picture, experimenters are sometimes interested the temporal dimension of sensory measurements (by the means of methods like TI, TDS, TCATA) or may simply aim to measure reaction times (e.g.~Implicit Association Test).\\
Eventually, different type of data can result from the same task. For example, this would typically be the case for \emph{free JAR} that yields both categorization data with hedonic valence and textual data \citep{Luc2022}. With the development of all sorts of media and data collection means, such patterns will surely become even more frequent.

\begin{figure}
\includegraphics[width=0.9\linewidth]{images/multiple_data} \caption{Sensory and consumer science studies often yield multiple data set that may all relate to each other.}\label{fig:multipledata}
\end{figure}

As could be expected, sensory and consumer studies are often multifaceted and collected data may all relate to each other when they apply to the same product set and/or to the same consumers . Such links between datasets are usually sought because they allow uncovering consumers' motivations and their drivers of preferences, thanks to modeling techniques (e.g.~preference mapping, PLS regression), segmentation analyses (e.g.~latent variables clustering), and machine learning. As a prerequisite to the application of any of these techniques though, it is critical to understand how these data are structured, and to properly handle them in a reliable and efficient manner. Many examples of such data manipulation are given throughout this book and specific guidance is given in Chapter \ref{data-manip}.

Last, it is worth mentioning that sensory and consumer data are intrinsically subjective. This is of course a good thing because the goal of any sensory study is to capture subjects' point of view. However, it could make some of the usual data quality criteria useless. This is specially true for hedonic data, for which repeatability and reference values could be questionable notions \citep{Koster2003, Kosteretal2003}. Sensory and consumer scientists may nonetheless rely on techniques allowing them to evaluate the degree of consensus of their panel, or tools like jackknife and bootstrap to evaluate the robustness of their data.

\hypertarget{computational-sensory-science}{%
\subsection{Computational Sensory Science}\label{computational-sensory-science}}

We can make an analogy of the future (or maybe the present already) of the sensory and consumer science field with other areas that advanced into the computational field, such as computational neuroscience and computational biology. A quick search in Wikipedia on the definition of those fields and a little about on how those areas evolved or how the term `computational' was introduced will make you realize that is the same path as the consumer and sensory field is moving along.

\begin{itemize}
\item
  \textbf{Computational neuroscience}: ``is a branch of neuroscience which employs mathematical models, computer simulations, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system. The term `computational neuroscience' was introduced to provide a summary of the current status of a field which was referred to by a variety of names, such as neural modeling, brain theory and neural networks.'' \url{https://en.wikipedia.org/wiki/Computational_neuroscience}
\item
  \textbf{Computational biology}: is a branch of biology that ``involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioral, and social systems. Computational biology, which includes many aspects of bioinformatics and much more, is the science of using biological data to develop algorithms or models in order to understand biological systems and relationships.'' \url{https://en.wikipedia.org/wiki/Statistical_model}
\end{itemize}

The sensory and consumer science field although not officially named with the term `computational', is already expanded in this field. The way consumer and sensory data is explored today is extremely advanced and went way beyond the simple statistical analysis performed a few years ago using the data collected from standard consumer or trained panel studies. Nowadays, sensory is getting into the big data field by organizing and putting together years of historical data into a database to answer future business questions and extract meaningful information.

Advances in digital technologies such as the integration of biometrics to assess consumers' physiological and emotional responses, incorporation of virtual, augmented, and mixed reality, and even the use of sensor technologies (\emph{`electronic noses and tongues'}) for sensory analysis are already widely used in the field. Additionally, data is being collected from different sources, such as social media. Those advanced technologies and complex data being extracted require much more advanced tools and computer capabilities to analyze and get meaningful information.

Rapid data acquisition, allied with the need for flexible, customized, and fast results interpretation, is opening a huge way for automation. The urge to deep explore, segment products/consumer, and discover new or hidden patterns and relationships to get the most valuable insights from the data sets is also nurturing the implementation of Artificial Intelligence, particularly Machine Learning.

At this point, we hope to have motivated you even more about the importance of data science for practitioners or students of sensory and consumer science who want to participate in the emerging field of computational sensory science.

Let's get started?

\hypertarget{start-R}{%
\chapter{Getting Started}\label{start-R}}

\hypertarget{introduction-to-r}{%
\section{Introduction to R}\label{introduction-to-r}}

\hypertarget{what-is-r}{%
\subsection{What is R?}\label{what-is-r}}

First released in 1995, R is an open-source programming language and software environment, that is widely used for statistical analyses, graphical representations, and reporting. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and is currently developed by the R Development Core Team \citep{Rmanual}.

R is a scripting language (not a compiled language) that runs the lines of code or commands one by one, in order. It is one of the most popular languages used by statisticians, data analysts, researchers, marketers etc. to retrieve, clean, analyze, visualize and represent data. By the time this book is being written, it is among the most popular programming languages in the world, including the sensory and consumer science field.

\begin{quote}
Do you know why R is called as such?
It seems that the name \emph{R} has two potential origins: It is the first letter of both Ihaka's and Gentleman's first names, but also it is a play on the name of Bell Labs Software called S it originates from (a lot of code that runs in S also run in R).
\end{quote}

\hypertarget{why-learning-r-or-any-programming-language}{%
\subsection{Why Learning R (or any Programming Language)?}\label{why-learning-r-or-any-programming-language}}

There are several reasons why you should learn R, or any programming language for that matter.

First, it gives the user a lot of \textbf{control}. Compared to other statistical software which can be seen as a 1. a \emph{black box} (you do not have necessarily access to the code that runs behind the scene), and 2. are restricted to the features their developers provide, R allows you to see what is happening at each step of your analysis (you can print the code that runs behind each function to ensure that it does what you are expecting\ldots), and allows exploring any type of analysis. This means that users are fully in control, and are only limited by their imagination (and maybe their program skills?). A direct advantage of this way of working helps \textbf{reducing errors}, since you can run the script line by line and see what's happening in each step to ensure that things are working properly the way it is meant to.

Allied to the control it provides, knowing a programming language allows you gaining in \textbf{efficiency} and \textbf{speed}. It may take some time at first to build the required skills to write efficient scripts, but once acquired, it will pay you back exponentially. A simple example showcasing this could be situations in which you have analyzed your data, and either realized that the data should be altered, or a different project with similar type of data also need analyzing. In both scenario, you would traditionally need to re-run the full set of analyses manually, which can be time-consuming. However, with a programming language, you can update all your tables, figures, and reports by simply applying to the new data your previous scripts.

Such solution brings us to the next reason, which is related to \textbf{abstract thinking} and \textbf{problem-solving} mindset. These are two components that are necessary to acquire good programming skills (no worries if you're not confident in having that in you yet, the more you program, the more you'll develop these skills) hence \textbf{increasing your capability} through \textbf{continuous improvement}. In other words, the more you play with your data, try new things etc., the more you'll improve as a programmer, and most importantly the more diverse and flexible you'll become. And quickly enough, you'll discover that each challenge can be solved in various different ways (as you will see in \ref{means}), so be imaginative and don't be afraid to think outside the box.

Last but not least, it \textbf{improves collaboration} and allows for \textbf{reproducible research} as your analyses are made transparent to colleagues if you decide to share your scripts with them. By embedding script, data sets, and results in a single file (we also recommend adding explanations regarding eventual decisions that were made for clarity) you and your colleagues can always track down why you obtain certain results by simply re-reading your script or re-running the analyses. In situations in which multiple users are collaborating on the same project, \textbf{version control} (see \ref{git-and-github}) also allows tracking changes done by the different contributors.

\hypertarget{why-r}{%
\subsection{Why R?}\label{why-r}}

For sensory and consumer scientists, we recommend the R ecosystem for three main reasons.

The first reason is \textbf{cultural}. R has from its inception been oriented more towards statistics than to computer science, making the feeling of programming in R more natural (in our experience) for sensory and consumer scientists than Python for instance. This opinion of experience is not to say that a sensory and consumer scientist shouldn't learn other languages (such as Python) if they are inclined to, or even that other tools aren't sometimes better than their R equivalent. Yet, to our experience, R tools are typically better suited to sensory and consumer science than any other solution we are aware of (especially in programming language).

This leads to our second reason, namely \textbf{availability}. R provides many tools that are suitable and relevant for sensory and consumer science purposes, while also providing many packages (e.g.~\texttt{\{SensoMineR\}} and \texttt{\{FactoMineR\}}, \texttt{\{SensR\}}, \texttt{\{FreeSortR\}}, \texttt{\{cata\}} just to name a few\ldots) that have been specifically developed for the analysis of sensory and consumer data. If you want to learn more about R, especially in the context of analyzing sensory and consumer data, we refer you to \citet{Le2018}.

Finally, the recent work done by the RStudio company, and especially the exceptional work of Hadley Wickham, has lead to a very low barrier to entry for programming within R. This is supplemented by the strong support provided by an active online community via numerous forums and websites, and by the several books, courses, and other educational materials made available.

\hypertarget{rstudio}{%
\subsection{Why RStudio/Posit?}\label{rstudio}}

\textbf{RStudio} (now renamed as Posit) is a powerful and easy way to interact with R programming. It is an Integrated Development Environment (IDE) for R\footnote{Originally, RStudio was only developed for R. More recently, it extended its use for other programming languages (e.g.~Python), and to accentuate its reach to other programming languages, RStudio changed its name to Posit to avoid the misinterpretation that it is only dedicated to R.} that comes with a multi-panel window setup that provides access to all primary things on a single screen. Such approach facilitates writing code since all information is available in a single window that includes a console, a script editor that supports direct code execution, as well as tools for plotting, history, debugging and workplace management (see \url{https://www.rstudio.com/}\footnote{As we are writing this book, the name Posit is not yet in use, and the website is still defined as \emph{rstudio}.}).

Besides the convenience of having all panels on a single screen, we strongly recommend the use of Rstudio as it offers many important features that facilitates scripting. For instance, the script editor provides many features including auto-completion of functions/R elements, hover menus that provides information regarding the arguments of the functions, handy shortcuts (see \ref{scripts}) etc. Additionally, the Environment section provides easy access to all objects available in the console. Last but not least, RStudio works with a powerful system of \emph{projects} (see \ref{rprojects})

\hypertarget{installing-r-and-rstudio}{%
\subsection{Installing R and RStudio}\label{installing-r-and-rstudio}}

The first step in this journey is to install R. For this, visit \href{https://www.r-project.org/}{The R Project for Statistical Computing} website. From there, follow the download instructions to install R on your operating system. We suggest you download the latest version of R and install it with default options. Note that if you are running R 4.0 or higher, you will need to install Rtools:
\url{https://cran.r-project.org/bin/windows/Rtools/}

Next, you need to install RStudio/Posit. To do so, visit the \href{https://rstudio.com/products/rstudio/download/}{RStudio desktop download page} and follow the installation instructions. Download and install the latest version of RStudio with default options.

We then advise you to apply the following adjustments:

\begin{itemize}
\tightlist
\item
  Uncheck \emph{Restore .RData} into the workspace at the startup (\emph{Tools} \textgreater{} \emph{Global Options\ldots{}}\textgreater{} \emph{General})
\item
  Select \emph{Never} for \emph{Save workspace to.RData on exit} (\emph{Tools} \textgreater{} \emph{Global Options\ldots{}}\textgreater{} \emph{General})
\item
  Change the color scheme to dark (e.g.~``Idle Fingers'') (\emph{Tools} \textgreater{} \emph{Global Options\ldots{}}\textgreater{} \emph{Appearance})
\item
  Put the console on the right (\emph{View} \textgreater{} \emph{Panes} \textgreater{} \emph{Console on Right})
\end{itemize}

Many other options are available, and we let you explore them yourself to customize Rstudio to your own liking.

\hypertarget{getting-started-in-r}{%
\section{Getting Started in R}\label{getting-started-in-r}}

\hypertarget{packages}{%
\subsection{Conventions}\label{packages}}

Before starting with R, it is important to talk about a few writing conventions that will be used in this book. These conventions are the one that are adopted in most book about R.

Throughout this book, since the goal is to teach you to read and write your own code in R, we need to refer to some R functions and R packages. In most cases, the raw R-code that we will be writing and that we advise you to reproduce is introduced in some \emph{special sections} such as:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

This section shows the code to type on top, and the results (as shown by the R console) in the bottom. To save some space, we may not always show the outputs of the code. Hence it is important for you to run the code to learn it, and to understand it.

Since in most situations, providing code alone is not sufficient, we will also provide explanation in writing. When doing so, we need to refer to R functions and packages throughout the text. In that case, we will clearly make the distinctions between R objects, R functions, and R packages by applying the following rules:

\begin{itemize}
\tightlist
\item
  An R object will be written simply as such: \texttt{name\_object}
\item
  An R function will always be written by ending with \emph{()}: \texttt{name\_function()}
\item
  An R package will always be written between \emph{\{\}}: \texttt{\{name\_package\}}
\end{itemize}

In some cases, we may want to specify from which package a function belongs to. Rather than calling \texttt{name\_function()} from the \texttt{\{name\_package\}} package, we adopt the R terminology \texttt{name\_package::name\_function()}. This terminology is very important to know and (sometimes) to use in your script to avoid surprises and error.

For illustration, multiple packages have a function called \texttt{select()}. Since we are often interested in using the \texttt{select()} function from the \texttt{\{dplyr\}} package, we can use \texttt{dplyr::select()} in our code to call it. The reason for this particular writing is to avoid errors by calling the \emph{wrong} \texttt{select()} function. By simply calling \texttt{select()}, we call the \texttt{select()} function from the last package loaded that contains a function with that name. However, by specifying the package it belongs to (here \texttt{\{dplyr\}}) we ensure that the right \texttt{select()} function (here from \texttt{\{dplyr\}}) is always called.

\hypertarget{install-and-load-packages}{%
\subsection{Install and Load Packages}\label{install-and-load-packages}}

The base installation of R comes with many useful packages that contain many of the functions you will use on a daily basis. However, once you want some more specific analyses, you will quickly feel the urge to extend R's capabilities. This is possible by using R packages.

An R package is a collection of functions, data sets, help files, and documentation, developed by the community that extends the capabilities of base R by improving existing base R functions or by adding new ones.

As of early 2022, there were more than 16000 different packages available on the CRAN alone (excluding packages that are available through other sources such as GitHub). Here is a short list of packages that we will be consistently using throughout this book.

\begin{itemize}
\tightlist
\item
  Essential packages (or collections): \texttt{\{tidyverse\}}, \texttt{\{readxl\}}, \texttt{\{writexl\}}
\item
  Custom Microsoft office document creation: \texttt{\{officer\}}, \texttt{\{flextable\}}, \texttt{\{rvg\}}, \texttt{\{openxlsx\}}
\item
  Sensory specific packages: \texttt{\{SensoMineR\}}, \texttt{\{FactoMineR\}}, \texttt{\{factoextra\}}
\end{itemize}

There are many more packages available for statistical tests of all varieties, to multivariate analysis, to machine learning, to text analysis, etc., some being mentioned later in this book.

\begin{quote}
Due to this extensive number of packages, it is not always easy to remember which package does what, nor what are the functions that they propose. Of course, the help file can provide such information. More interestingly, some packages provide \emph{Cheat Sheets} which aim in describing the most relevant functions and their use. Within RStudio, some Cheat Sheets can be found under \emph{Help} \textgreater{} \emph{Cheat Sheets}, but many more can be found online.
\end{quote}

To install a package, you can type \texttt{install.packages("package\_name")} in your console. R will download (an internet connection is required) the packages from the CRAN and install it into your computer. Each package only needs to be installed once per R version.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
If a script loads a package that is not yet installed, RStudio will prompt a message on top so that you can install them directly. Also, note that if you do not have write access on your computer, you might need IT help to install your packages.
\end{quote}

Once you have installed a package onto your computer, its content is only available for use once it's loaded. To load a package, use \texttt{library(package\_name)}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

A package should only be installed once, however it should be loaded for each new session of R. To simplify your scripting, we recommend to start your scripts with all the packages that you would need. So as soon as you open your script, you can run the first lines of code and ensure that all your functions are made available to you.

If you forget to load a package of interest, and yet run your code, you will get an error of the sort: \texttt{Error\ in\ ...:\ could\ not\ find\ function\ "..."}

Note that certain packages may no longer be maintained, and the procedure presented above hence no longer works for those packages. This is for instance the case for \texttt{\{sensR\}}, an excellent package dedicated to the analysis of discrimination tests.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"sensR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As you can see, running this code provide the following message: \texttt{Warning\ in\ install.packages\ :\ package\ ‘sensR’\ is\ not\ available\ for\ this\ version\ of\ R}

No worries, there is an alternative way to get it installed by using the \texttt{install\_version()} function from \texttt{\{remotes\}}. In this case, we need to provide the version of the package to install. Since the latest version of \texttt{\{sensR\}} is 1.5.2, we can install it as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_version}\NormalTok{(}\StringTok{"sensR"}\NormalTok{, }\AttributeTok{version =} \StringTok{"1.5.2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Last but not least, packages are often improved over time (e.g.~through bug fixes, addition of new functions etc.). To update some existing packages, you can use the function \texttt{update.packages()} or simply re-install it using \texttt{install.packages(package\_name)}.

\begin{quote}
RStudio also proposes a section called \emph{Packages} (bottom right of your screen if you applied the changes proposed in \ref{rstudio}) where you can see which packages are installed, install new packages, or update already existing packages in a few clicks.
\end{quote}

\hypertarget{first-analysis-in-r}{%
\subsection{First Analysis in R}\label{first-analysis-in-r}}

Like any language, R is best learned through examples. Let's start with a simple example where we analyze a tetrad test to illustrate the basic principles.

Suppose you have 15 out of 44 correct answers in a tetrad test. Using the package \texttt{\{sensR\}}\footnote{In the previous section, we show you how to install it!}, it's very easy to analyze these data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sensR)}

\NormalTok{num\_correct }\OtherTok{\textless{}{-}} \DecValTok{15}  
\NormalTok{num\_total }\OtherTok{\textless{}{-}} \DecValTok{44}  
  
\NormalTok{discrim\_res }\OtherTok{\textless{}{-}} \FunctionTok{discrim}\NormalTok{(}\AttributeTok{correct =}\NormalTok{ num\_correct, }\AttributeTok{total =}\NormalTok{ num\_total, }
                       \AttributeTok{method =} \StringTok{"tetrad"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(discrim\_res)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Estimates for the tetrad discrimination protocol with 15 correct
## answers in 44 trials. One-sided p-value and 95 % two-sided confidence
## intervals are based on the 'exact' binomial test. 
## 
##         Estimate Std. Error Lower Upper
## pc        0.3409     0.0715 0.333 0.499
## pd        0.0114     0.1072 0.000 0.249
## d-prime   0.2036     0.9659 0.000 1.019
## 
## Result of difference test:
## 'exact' binomial test:  p-value = 0.5141 
## Alternative hypothesis: d-prime is greater than 0
\end{verbatim}

In a few lines of code, you've just analysed your tetrad test data.

\hypertarget{scripts}{%
\subsection{R Scripts}\label{scripts}}

You may have entered the code to analyze your tetrad test data directly into the R Console. Although this is possible, and there are many situations where it makes sense (e.g.~opening a help menu, taking a quick look at your data, debugging a function, or maybe a simple calculation or testing), it is not the most efficient way of working and we would recommend \textbf{NOT} to do so. Indeed, the code directly written in the console cannot easily be modified, retrieved, or saved. Hence, once you close or restart your R session, you will lose it all. Also, if you make an error in your code (even just a typo), or simply want to make a small change, you will have to re-enter the entire set of commands, typing it all over again. For all those reasons (and many more), you should write any important code into a \textbf{script}.

An R script is simply a text file (with the extension \emph{.R}) containing R code, set of commands (that you would enter on the command line in R) and comments that can easily be edited, executed, and saved later for (re)use.

You can create a new script in RStudio by clicking the \emph{New File} icon in the upper left of the main toolbar and then selecting \emph{RScript}, by clicking \emph{File} in the main menu and then selecting \emph{New File} \textgreater{} \emph{R Script}, or by simply using \emph{CTRL + SHIFT + N} (Windows)\footnote{The shortcuts are given for Windows users. For Mac users, replace \emph{CTRL} by \emph{Cmd} and it should also work.}. The script will open in the Script Editor panel and is ready for text entry. Once you are done you can save your script by clicking the \emph{Save} icon at the top of the Script Editor and can open it later to re-run your code and/or continue your work where you left it.

Unlike typing code in the console, writing code in an R script is not being executed. Instead, you need to send/run it to the console. There are a few ways to do this. If you want to run a line of code, place the cursor anywhere on the line of the code and use the shortcut \emph{Ctrl + Enter} . If you want a portion of the code, select by highlighting the code of interest and run it using the same shortcut. To run the entire script (all lines of the code) you can click `Run' in the upper right of the main toolbar or use the shortcut \emph{Ctrl + Shift + Enter}.

A few other relevant shortcuts are:

\begin{itemize}
\tightlist
\item
  Interrupt current command - \emph{Esc}
\item
  Navigate command history - \emph{up and lower arrows}
\item
  Attempt completion - \emph{Tab}
\item
  Call help for a function - \emph{F1}
\item
  Restart R Session: \emph{Ctrl + Shift + F10}
\item
  Search in File - \emph{CTRL + F}
\item
  Search in All Files (within a project or folder) - \emph{CTRL + SHIFT + F}
\item
  Commenting a line of code - \emph{CTRL + SHIFT + C}
\item
  Insertion of a section in the code - \emph{CTRL + SHIFT + R}
\item
  Insertion of a pipe (\texttt{\%\textgreater{}\%}) - \emph{CTRL + SHIFT + M}
\end{itemize}

There are many more shortcut options. A complete list is available within R Studio under \emph{Tools} \textgreater{} \emph{Keyboard Shortcut Help} (or directly using \emph{ALT + SHIFT + K}). So have a look at them, and don't hesitate to learn by heart the one that you use regularly as it will simplify your scripting procedure.

\hypertarget{rprojects}{%
\subsection{Create a Local Project}\label{rprojects}}

Next to scripts, working with RStudio projects will facilitate your life even further. RStudio projects make it straightforward to divide your work into multiple contexts, each with its own working directory, workspace, history, and source documents. It keeps all of your files (R scripts, R markdown documents, R functions, data etc.) in one place. RStudio projects allow independence between projects, which means that you can open more than one project at the time, and switch at ease between them without fear of interference (they all use their own R session). Moreover, those projects are not linked to any computer, meaning that the file path are linked to the project itself: While sharing a RStudio project with colleagues, they do not need to update any file path to make it work.

To create a new project locally in RStudio, select \emph{File} \textgreater{} \emph{New Project\ldots{}} from the main menu. Typically, a new project is created in a new directory, also you can also transform an already existing folder on your computer into an RStudio Project. You can also create a new project by clicking on the \emph{Project} button in the top right of RStudio and selecting \emph{New Project\ldots{}}. Once your new project has been created you will now have a new folder on your computer that contains the basic file structure. You probably want to add folders to better organize all the files and documents, such as a folder for input, output and scripts.

For consistency, we suggest you keep the same folder structure across projects. For example, you may create a folder that contains your scripts, one for the data, one for exporting results from R (excel files, figures, report, etc.). If you adopt this strategy, you may see an interest in the code below, which automatically creates all your folder. To run this code, the \texttt{\{fs\}} package is required. Here, 5 folders are being created:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fs)}

\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path=}\FunctionTok{c}\NormalTok{(}\StringTok{"code"}\NormalTok{, }\StringTok{"data"}\NormalTok{, }\StringTok{"docs"}\NormalTok{, }\StringTok{"output"}\NormalTok{, }\StringTok{"template"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{further-tips-on-how-to-read-this-book}{%
\section{\texorpdfstring{Further tips on \emph{how to read this book?}}{Further tips on how to read this book?}}\label{further-tips-on-how-to-read-this-book}}

In this book, we assume that the readers have already some basic knowledge in R.
If you are completely new to R, we recommend you reading ``R for Data Science'' by \href{https://r4ds.had.co.nz/}{Wickham and Grolemund} \citeyearpar{Wickham2016} or looking at some documentation online to get you started with the basics.

Just like with any spoken language, the same message can be said in various ways. The same applies with writing scripts in R, each of us having our own styles, or our own preferences towards certain procedures, packages, functions, etc. In other words, writing scripts is personal. Through this book, we are not trying to impose our way of thinking/proceeding/building scripts, instead we aim in sharing our knowledge built through past experiences to help you find your own.

But to fully decode our message, you'll need some reading keys. These keys will be described in the next sections.

Note that the lines of code presented in this section do not run and are simply presented for illustration.

\hypertarget{pipes}{%
\subsection{\texorpdfstring{Introduction to the \texttt{\{magrittr\}} and the notion of \emph{pipes}}{Introduction to the \{magrittr\} and the notion of pipes}}\label{pipes}}

R is an \emph{evolving} programming language that expends very rapidly.

If most additions/improvements have a fairly limited reach, the introduction of the \texttt{\{tidyverse\}} in 2016 by H. Wickham revolutionized the way of scripting in R for many users. At least for us, it had a large impact as we fully embraced its philosophy, as we see its advantage for Data Science and for analyzing our sensory and consumer data. It is hence no surprise that you'll read and learn a lot about it in this book.

As you may know, the \texttt{\{tidyverse\}} is a grouping of packages dedicated to Data Science, which includes (amongst others) \texttt{\{readr\}} for data importation, \texttt{\{tibble\}} for the data structure, \texttt{\{stringr\}} and \texttt{\{forcats\}} for handling strings and factors, \texttt{\{dplyr\}} and \texttt{\{tidyr\}} for manipulating and tidying data, \texttt{\{ggplot2\}} for data visualization, and \texttt{\{purrr\}} for functional programming. But more importantly, it also includes \texttt{\{magrittr\}}, the package that arguably impacted the most our way of scripting by introducing the notion of \emph{pipes} (defined as \texttt{\%\textgreater{}\%}) as it provides code that is much easier to read and understand.

To illustrate the advantage of coding with pipes, let's use the example provided by H. Wickham in his book \emph{R for Data Science}.
It is some code that tells a story about a little bunny names Foo Foo:
\textgreater{} Little bunny Foo Foo
\textgreater{} Went hopping through the forest
\textgreater{} Scooping up the field mice
\textgreater{} and bopping them on the head

If we were meant to tell this story though code, we would start by creating an object name \texttt{FooFoo} which is a little bunny:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo\_foo }\OtherTok{\textless{}{-}} \FunctionTok{little\_bunny}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To this object, we then apply different functions (we save each step as a different object):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo\_foo\_1 }\OtherTok{\textless{}{-}} \FunctionTok{hop}\NormalTok{(foo\_foo, }\AttributeTok{through=}\NormalTok{forest)}
\NormalTok{foo\_foo\_2 }\OtherTok{\textless{}{-}} \FunctionTok{scoop}\NormalTok{(foo\_foo\_1, }\AttributeTok{up=}\NormalTok{field\_mice)}
\NormalTok{foo\_foo\_3 }\OtherTok{\textless{}{-}} \FunctionTok{bop}\NormalTok{(foo\_foo\_2, }\AttributeTok{on=}\NormalTok{head)}
\end{Highlighting}
\end{Shaded}

One of the main downsides of this approach is that you'll need to create intermediate names for each step. If natural names can be used, this will not be a problem, otherwise it can quickly become a source of error (using the wrong object for instance)! Additionally, such approach may affect your disk memory since you're creating a new object in each step. This can be problematic when the original data set is large.

As an alternative, we could consider running the same code by over-writing the original object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo\_foo }\OtherTok{\textless{}{-}} \FunctionTok{hop}\NormalTok{(foo\_foo, }\AttributeTok{through=}\NormalTok{forest)}
\NormalTok{foo\_foo }\OtherTok{\textless{}{-}} \FunctionTok{scoop}\NormalTok{(foo\_foo, }\AttributeTok{up=}\NormalTok{field\_mice)}
\NormalTok{foo\_foo }\OtherTok{\textless{}{-}} \FunctionTok{bop}\NormalTok{(foo\_foo, }\AttributeTok{on=}\NormalTok{head)}
\end{Highlighting}
\end{Shaded}

If this solution looks neater and more efficient (less thinking, less typing, less memory use), it is more difficult to debug, as the entire code should be re-run from the beginning (when \texttt{foo\_foo} was originally created). Moreover, calling the same object in each step obscures the changes performed in each line.

To these two approaches, we prefer a third one that strings all the functions together without intermediate steps of saving the results. This procedure uses the so-called pipes (defined by \texttt{\%\textgreater{}\%}), which takes automatically as input the output generated by the previous line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo\_foo }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hop}\NormalTok{(}\AttributeTok{through =}\NormalTok{ forest) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{scoop}\NormalTok{(}\AttributeTok{up =}\NormalTok{ field\_mice) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bop}\NormalTok{(}\AttributeTok{on =}\NormalTok{ head)}
\end{Highlighting}
\end{Shaded}

This code is easier to read and understand as it focuses more on the verbs (here \texttt{hop()}, \texttt{scoop()}, and \texttt{bop()}) rather than the names (\texttt{foo\_foo\_1}, or \texttt{foo\_foo}). It can be surprising at first, but no worries, by the time you've read this book, you'll be fully familiar with this concept.

When lines are piped, R runs the entire block at once. So how can we understand the intermediate steps that were done, or how can we fix the code if an error occur? The answer to these questions is simple: run back the code bits by bits.

For instance, in this previous example, we could start by printing \texttt{foo\_foo} (in practice, only select \texttt{foo\_foo} and run this code only) only to ensure that it is the object that we were supposed to have. If it is the case, we can then extend the selection to the next line by selecting all the code until (but excluding\footnote{If your code ends up on a pipe, R is expecting additional code and will not show results: this usually creates errors since the next piece of code is probably not matching the current pipe's expected code.}!) the pipe. Repeat this until you found your error, or you've ensured that all the steps have been performed correctly.

While reading this book, we advise you to apply this trick to each long pipes for you to get a hand on it, and to visualize the intermediate steps.

\begin{quote}
Within a pipe, it is sometime needed to call the temporary data or output generated in the previous step. Since the current object does not exist yet, nor have a name (it is still \emph{under-construction}), we need to find another way to call it. In practice, this is very simple and can be done by using \texttt{.} as we will see it extensively in Chapter \ref{data-manip}.
\end{quote}

Note however that although pipes are very powerful, they are not always the best option:

\begin{itemize}
\tightlist
\item
  A rule of thumb suggests that if you are piping more than 10 lines of code, you're probably better of splitting it into 2 or more blocks (saving results in intermediate step) as this simplifies debugging.
\item
  If some steps require multiple inputs, or provides multiple outputs, pipes should not be used as they usually require a primary object to transform.
\item
  The system of pipes works linearly: if your code requires a complex dependency structure, the pipes should be avoided.
\end{itemize}

\hypertarget{tibbles}{%
\subsection{Tibbles}\label{tibbles}}

Within the \texttt{\{tidyverse\}}, another valuable (yet often forgotten) package is the \texttt{\{tibble\}} package. This package aims in providing a new format to store data.

In appearance, a \emph{tibble} looks like just any other table, whether it is a matrix or a data frame. But in the background, it is defined as an optimized version of a data frame that kept (and extended) all the relevant parts, and removed all the unnecessary part.

To show you the properties of a tibble, let's load the data set called \texttt{sensochoc} from the \texttt{SensoMineR} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(SensoMineR)}
\FunctionTok{data}\NormalTok{(chocolates)}
\end{Highlighting}
\end{Shaded}

If you type \texttt{sensochoc} in your R session, the entire data set is being printed, which makes it difficult to read. Here, we opt for simpler solution by only showing the first lines using \texttt{head()}

\begin{quote}
Here and throughout the book, some of the outputs are being reduced and only the first results are being printed (to avoid printing too much things that may not be so relevant.). So do not panic if only a part of the outputs is shown in the book compared to your screen.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(sensochoc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Panelist Session Rank Product CocoaA MilkA
## I001        1       1    1   choc6      7     6
## I002        1       1    6   choc3      6     7
## I003        1       1    3   choc2      8     6
## I004        1       1    5   choc1      7     8
## I005        1       1    2   choc4      8     5
## I006        1       1    4   choc5      7     5
\end{verbatim}

This is the typical output from a \texttt{matrix()} or \texttt{data.frame()} in R. In particular, one can note that the first column does not have header, as it represents the row names.

Let's convert this table into a tibble (using \texttt{as\_tibble()}) and look at the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{()}
\FunctionTok{print}\NormalTok{(sensochoc\_tb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 348 x 18
##   Panelist Session Rank  Product CocoaA MilkA CocoaF
##   <fct>    <fct>   <fct> <fct>    <int> <int>  <int>
## 1 1        1       1     choc6        7     6      6
## 2 1        1       6     choc3        6     7      2
## 3 1        1       3     choc2        8     6      5
## 4 1        1       5     choc1        7     8      8
## # ... with 344 more rows, and 11 more variables:
## #   MilkF <int>, Caramel <int>, Vanilla <int>,
## #   Sweetness <int>, Acidity <int>, Bitterness <int>,
## #   Astringency <int>, Crunchy <int>, Melting <int>,
## #   Sticky <int>, Granular <int>
\end{verbatim}

The appearance of the table looks quite different, and there are some interesting things that can be noticed:

\begin{itemize}
\tightlist
\item
  The dimensions of the table is shown straight at the top.
\item
  By default, the printing function for tibbles only print a pre-defined number of rows (by default 10), and as many columns as the screen allows. All other columns are informed in some text under the table.
\item
  {[}Not applicable here{]} Although data frame converts some names (e.g.~replacing spaces with .), tibbles keep the same as in the original file.
\item
  {[}Not applicable here{]} Numbers are also automatically formatted to 3 values after the decimal (by default), and negative values are printed in red.
\item
  Under the header, the type of each variable is informed. Although very valuable, this information is not provided with matrix or data frame.
\item
  The row names are lost, as tibbles do not have row names. In this example, there is no information in the row names, so we can ignore them, although we could have easily recovered them by adding \texttt{rownames\ =\ "name\ variable"} in \texttt{as\_tibble()} which would have added them as a new column called \texttt{name\ variable} to the data.
\end{itemize}

\begin{quote}
Certain packages including \texttt{\{SensoMineR\}} do not accept tibbles. Instead, they require matrices or data frame as inputs, and use their row names for certain analysis (see Section \ref{sensory-analysis} for an example of PCA). Fortunately, there is a very easy way to convert tibbles to (say) data frame through \texttt{as.data.frame()} combined with \texttt{column\_to\_rownames()} to automatically pass the information present in a column as row names (the complementary function \texttt{rownames\_to\_column()} also exists).
\end{quote}

To go further, let's extract from \texttt{sensochoc} the 4th column (\texttt{Product}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc[,}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] choc6 choc3 choc2 choc1 choc4
## Levels: choc1 choc2 choc3 choc4 choc5 choc6
\end{verbatim}

By extracting a column of a data frame, the resulting output is converted into a vector.
Let's reproduce the same extraction to \texttt{sensochoctb}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb[,}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 348 x 1
##   Product
##   <fct>  
## 1 choc6  
## 2 choc3  
## 3 choc2  
## 4 choc1  
## # ... with 344 more rows
\end{verbatim}

Subsetting from a tibble with \texttt{{[}{]}} always returns a tibble, which is very convenient with programming as we then know what to expect (unlike data frame which can return a data frame or a vector depending on the situation).

Last but not least, tibbles can take as entries single elements (e.g.~numbers, characters, dates, etc.), but also lists of elements. This particular property is very interesting since it allows combining different outputs in one table although they may have different structures.

Let's start with converting the table in a long format:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"Session"}\NormalTok{,}\StringTok{"Rank"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"Panelist"}\NormalTok{,}\StringTok{"Product"}\NormalTok{), }\AttributeTok{names\_to=}\StringTok{"Attributes"}\NormalTok{, }
               \AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This new tibble has 4872 rows and 4 columns. Since we have 14 attributes, that means that each attribute has 348 data points.

Let's nest the results by \texttt{Attributes}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest\_by}\NormalTok{(Attributes)}
\end{Highlighting}
\end{Shaded}

This new tibble has only 14 rows (one per attribute), yet the new column called \texttt{data} contains a list of dimensions {[}348 rows and 3 columns{]}. This correspond to the original data\footnote{We could easily retrieve the original data by simply using \texttt{sensochoc\_tbl\ \%\textgreater{}\%\ unnest(data)}}.

Let's now run an ANOVA for each attribute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ANOVA =} \FunctionTok{list}\NormalTok{(}\FunctionTok{aov}\NormalTok{(Scores }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Panelist, }\AttributeTok{data=}\NormalTok{data)))}
\end{Highlighting}
\end{Shaded}

This code adds another column (called \texttt{ANOVA}) which contains the results of the ANOVA model.
Let's imagine we're interested in \texttt{Acidity}, we could extract the results of the anova as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Attributes }\SpecialCharTok{==} \StringTok{"Acidity"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(ANOVA))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Attributes'. You
## can override using the `.groups` argument.
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 7
## # Groups:   Attributes [1]
##   Attributes term     df sumsq meansq stati~1   p.value
##   <chr>      <chr> <dbl> <dbl>  <dbl>   <dbl>     <dbl>
## 1 Acidity    Prod~     5  325.  65.0    17.1   5.67e-15
## 2 Acidity    Pane~    28  917.  32.7     8.62  2.88e-25
## 3 Acidity    Resi~   314 1193.   3.80   NA    NA       
## # ... with abbreviated variable name 1: statistic
\end{verbatim}

Since extracting the information regarding the models is also of interest, let's add that to our tibble as well. This can easily be done using the \texttt{glance()} function from \texttt{\{broom\}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Results =} \FunctionTok{list}\NormalTok{(broom}\SpecialCharTok{::}\FunctionTok{glance}\NormalTok{(ANOVA)))}
\end{Highlighting}
\end{Shaded}

Here again, if we would want to see the results for \texttt{Acidity} only, we could extract that information as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensochoc\_tb }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Attributes }\SpecialCharTok{==} \StringTok{"Acidity"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(Results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Attributes'. You
## can override using the `.groups` argument.
\end{verbatim}

\begin{verbatim}
## # A tibble: 1 x 7
## # Groups:   Attributes [1]
##   Attributes logLik   AIC   BIC deviance  nobs r.squa~1
##   <chr>       <dbl> <dbl> <dbl>    <dbl> <int>    <dbl>
## 1 Acidity     -708. 1486. 1621.    1193.   348    0.510
## # ... with abbreviated variable name 1: r.squared
\end{verbatim}

So as we can see, the same tibble of 14 row and 4 columns contain, per attribute, the raw \texttt{data}, the results of the \texttt{ANOVA}, as well as the overall \texttt{Results} of each of the model. Although all these tables have completely different structures (\texttt{data} has 348 rows and 3 columns, whereas \texttt{Results} has 1 row and 6 columns), they are still related to the same \emph{objects} (here attributes). Hence keeping them in the same \emph{tidy} place facilitates follow-up analysis by avoiding creating multiple objects, and by reducing the risks of error. An example of such use of tibbles is provided in Section \ref{tibble-use}, in which we also show how to use the information stored in the different elements.

\hypertarget{calling-variables}{%
\subsection{Calling Variables}\label{calling-variables}}

In R, variables can be called in different ways when programming. If the names of variables should be read from the data (e.g.~``Product'', ``products'', ``samples'', etc.), you will often use strings, meaning that the name used will be defined between quotes (e.g.~\texttt{"Product"}).

Within the \texttt{\{tidyverse\}}, the names of variables that are included within a data set are usually called as it is, without quote:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny)}
\end{Highlighting}
\end{Shaded}

This is true for simple names that do not contain any special characters (e.g.~space, \texttt{-,\ etc.}). For names that contain special characters, the use of \emph{backticks} are required (note that \emph{backticks} can also be used with simple names):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Judge}\StringTok{\textasciigrave{}}\NormalTok{, Product, }\StringTok{\textasciigrave{}}\AttributeTok{Color evenness}\StringTok{\textasciigrave{}}\NormalTok{).}
\end{Highlighting}
\end{Shaded}

While going through this book, you'll notice that many functions from the \texttt{\{tidyverse\}} sometimes require quotes, and sometimes don't. The simple way to know whether quotes are required or not is based on its existence in the data set or not: If the column exists and should be used, no quotes should be used. On the contrary, if the variable doesn't exist and should be created, then quotes should be used.

Let's illustrate this through a simple example involving \texttt{pivot\_longer()} and \texttt{pivot\_wider()} successively (see \ref{pivots} for more information). For \texttt{pivot\_longer()}, we create two new variables, one that contains the column names (informed by \texttt{names\_to}) and one that contains the values (informed by \texttt{values\_to}). Since these variables are being created, quotes are required for the new names. For \texttt{pivot\_wider()}, quotes are not needed since the names of the variables to use (\texttt{names\_from} and \texttt{values\_from}) are present in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }\AttributeTok{names\_to=}\StringTok{"Variables"}\NormalTok{, }
               \AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Variables, }\AttributeTok{values\_from=}\NormalTok{Scores)}
\end{Highlighting}
\end{Shaded}

Unfortunately this rule of thumb is not always true (e.g.~\texttt{separate()}, \texttt{unite()}, \texttt{column\_to\_rownames()}) but you'll quickly get familiar with these exceptions.

\hypertarget{print-save}{%
\subsection{Printing vs.~Saving results}\label{print-save}}

In many examples through this book, we apply changes to certain elements without actually saving them in an R object. This is quite convenient for us as many changes we do are only done for pedagogic reasons, and are not necessarily relevant for our analyses.

Here is an example of such case (see \ref{renaming}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product)}
\end{Highlighting}
\end{Shaded}

When you run this code, you can notice that we rename \texttt{Judge} to \texttt{Panellist}, and \texttt{Product} to \texttt{Sample}\ldots at least this is what you see on screen. However, if you look at \texttt{sensory}, the data set still contains the column \texttt{Judge} and \texttt{Product} (\texttt{Panellist} and \texttt{Sample} do not exist!). This is simply because we did not save the changes.

If we would want to save the element in a new object, we should save the outcome in an element using \texttt{\textless{}-}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newsensory }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{newsensory} corresponds to \texttt{sensory}, but with the new names. Of course, if you would want to overwrite the previous file with the new names, you simply need to ensure that the name of the output is the same as the name of the input (like we did with \texttt{foo\_foo} in \ref{pipes}). Concretely, we replace here \texttt{newsensory} by \texttt{sensory}, meaning that the new names are saved in \texttt{sensory} (so the old names \texttt{Judge} and \texttt{Product} are definitely lost). This procedure saves computer memory and does not require you coming up with new names all the time. However, it also means that some changes that you applied may be lost, and if you have a mistake in your code, it is more complicated to find and ultimately solve it (you may need to re-run your entire script).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product)}
\end{Highlighting}
\end{Shaded}

To visualize the changes, you would need to type \texttt{newsensory} or \texttt{sensory} in R.
Another (faster) way to visualize it is to put the entire block of code between brackets: Putting code between brackets is equivalent to asking to print the output after being run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(sensory }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product))}
\end{Highlighting}
\end{Shaded}

Note that if you run all these lines of codes in R, you will get an error stating \texttt{Column\ \textquotesingle{}Judge\textquotesingle{}\ doesn\textquotesingle{}t\ exist.} This is a good illustration of a potential error mentioned above: We overwrote the original \texttt{sensory} (containing \texttt{Judge} and \texttt{Product}) with another version in which these columns were already renamed as \texttt{Panellist} and \texttt{Sample}. So when you re-run this code, you are trying to apply again the same changes to columns that no longer exist, hence the error.

This is something that you need to take into consideration when overwriting elements (in this case, you should initialize \texttt{sensory} to its original version before trying).

\begin{quote}
It is also worth reminding the readers that we deliberately \textbf{do not print} each and every output generated. Instead, we advise the reader to re-write the code and run it in parallel as they read the book. The outputs will then be printed on their screen. Also, in some cases, we took the liberty to reduce the outputs (e.g.~only showing the first rows and/or columns) which can lead to some inconsistencies between what is printed in the book, and what is shown on your screen. Please be aware that this can (and will) happen.
\end{quote}

\hypertarget{running-code-and-handling-errors}{%
\subsection{Running code and handling errors}\label{running-code-and-handling-errors}}

For you to get the most out of this book, you need to understand (and eventually adhere to) our philosophy of scripting, and our way of working. This is why we are providing you with some tips to use, if you're comfortable with them:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a folder for this book on your computer, and create a script for each chapter in which you re-type yourself each line of code. If you work with the online version, you could copy/paste the code to go faster, but you may miss some subtleties.
\item
  Do not be discourage when you get some errors: we all get some. At first, this can be very frustrating, especially when you are not able to fix them quickly. If you get stuck on an error and cannot fix it immediately, take a break and come back later with fresh eyes, you may solve it then. And with time and experience, you'll notice that you can reduce the amount of errors, and will also solve them faster (you will also learn to understand the error messages provided by R).
\item
  The more code, the more difficult it is to find errors. This is true whether you use \emph{regular} R-code or pipes. The best way to solve errors in such circumstances is to run the code line by line until you find the error, and understand why the input/output does not match expectations.
\item
  In the particular case of pipes, debugging errors means that you shouldn't run the entire block of code, but select parts of it and run it by adding in each run a new line. This can either be done by stopping your selection just before the adequate \texttt{\%\textgreater{}\%} sign (as mentioned earlier), or by adding after the last \texttt{\%\textgreater{}\%} sign the function \texttt{identity()}\footnote{\texttt{identity()} is a function that returns as output the input as it is. This function is particularly useful in pipes as you can finish your pipes with it, meaning that you can put any line in comments (starting with `\#') without worrying about finishing your pipe with a \texttt{\%\textgreater{}\%}}.
\end{enumerate}

\hypertarget{git-and-github}{%
\section{Version Control / Git and GitHub}\label{git-and-github}}

Version control is a tool that tracks changes to files, especially source code files. Using version control means that you can not only track the changes, but manage them by for instance describing the changes, or reverting to previous versions. This is particularly important when collaborating with other developers.

Version control systems are simply software that helps users manage changes to source code over time. The reasons why everyone should use version control include backing up work, restoring prior versions, documenting reasons for changes, quickly determining differences in versions, easily sharing code, and developing in parallel with others.

There are many tools for Version Control out there, but Git/GitHub are by far the most common one. We highly recommend that you integrate both Git and GitHub into your data science workflow. For a full review of Git and GitHub from an R programming perspective, we recommend \href{https://happygitwithr.com/}{Happy Git with R} by Jenny Bryant. In what follows, we simply provide the minimum information needed to get you up and running with Git and GitHub. Also, for an insightful discussion of the need for version control, please see \citet{Bryan2018}.

\hypertarget{git}{%
\subsection{Git}\label{git}}

Git is a version control system that runs locally and automatically organizes and saves versions of code on your computer, but does not connect to the internet. Git allows you to revert to earlier versions of your code, if necessary. To set up Git, follow the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Download and install the latest version of Git.
  Download and install Git with standard options (allow 3rd party software) for \href{https://git-scm.com/download/win}{Windows} or \href{https://git-scm.com/download/mac}{Mac}
\item
  Enable Git Bash in RStudio
  Go to `Tool' on the top toolbar and select `Global Options\ldots{}' \textgreater{} `Terminal'. In the drop-down box for `New terminals open', select `Git Bash'.
\item
  Configure Git from Rstudio
  The easiest way is to use the package \texttt{\{usethis\}}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}
\FunctionTok{use\_git\_conf}\NormalTok{ (}\AttributeTok{user.name =} \StringTok{"your username"}\NormalTok{, }\AttributeTok{user.email =} \StringTok{"your email address"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{github}{%
\subsection{GitHub}\label{github}}

GitHub is a cloud-based service that supports Git usage. It allows online backups of your code and facilitates collaboration between team members. While Git creates local repositories on your computer, GitHub allows users to create remote online repositories for their code.

To set up GitHub, follow the steps below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Register for a GitHub Account
  To get started you can sign up for a free GitHub account: \href{https://github.com/}{GitHub}
\end{enumerate}

We recommend you not to tie your account to your work email and to use all lowercase to avoid confusion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Create a Test Repository in GitHub
\end{enumerate}

Once you log into your account, create a new repository by clicking the green button `New'. You have to then name your repository and make some selections. We recommend you select the option `Private' and click on the option ``Initialize this repository with a README''. The last step is to click on `Create Repository'.

Once the repository has been created you need to copy the repository URL to create a project in RStudio (next step). If you select the repository you just created, click on the green button `Code' and copy the URL link.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Create an RStudio Project from GitHub
\end{enumerate}

As we have seen, to create a new project, select `File' \textgreater{} `New Project\ldots{}' from the top bar menu or by clicking on the `Project' button in the top right of RStudio and by selecting `New Project\ldots{}'. Select then `Version Control' \textgreater{} `Git'. Paste the repository URL link, select where you want to save this project locally, and click ``Open in new session''. Finally, click `Create Project'.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Register GitHub from Studio
\end{enumerate}

At his point, you will be asked to log into GitHub from RStudio. You should only have to do this once.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Push and Commit Changes
\end{enumerate}

Once you are done with your coding, or have finished updating a series of scripts, you can simply push, or send them to GitHub, so others can see your changes. You have to first commit and then push it to GitHub. To do so, you can click the `Git' icon on the top menu of RStudio and select the option `Commit'. You can select what you want to commit and describe the changes you did. After committing your code/files, you have to push it by clicking the option `Push'.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  Pull Changes
\end{enumerate}

In case you are working with other colleagues, a good practice is to always pull (which means download) the latest code available (i.e.~the code that your collaborators have recently pushed) before you get started and before pushing any changes. To do so, you can click the `Git' icon on the top menu and select the option `Pull'.

If you've read this through (no worries if everything is not completely clear yet, it will come!), and followed the different steps here, you should be ready to learn data science for sensory and consumer scientists. Let's get started?

\hypertarget{data_science}{%
\chapter{Why Data Science?}\label{data_science}}

\begin{quote}
In this chapter we explain what is data science and discuss why data science is valuable to sensory and consumer scientists. While this book focuses on the aspects of data science that are most important to sensory and consumer scientists, we recommend the excellent book from \citet{Wickham2016} for a more general introduction to data science.
\end{quote}

\hypertarget{history-and-definition}{%
\section{History and Definition}\label{history-and-definition}}

You may have heard that data science was called the ``sexiest job of the 21st century'' by Harvard Business Review (\citet{Davenport2012}). But what is data science? Before we give our definition, we provide some brief history for context. For a comprehensive survey of this topic, we recommend \citet{Cao2017}.

To begin, there was a movement in early computer science to call their field ``data science.'' Chief among the advocates for this viewpoint was Peter Naur, winner of the 2005 Turing award \footnote{A prize roughly equivalent in prestige to a Nobel prize, but for computer science.}. This viewpoint is detailed in the preface to his 1974 book, ``Concise Survey of Computer Methods,'' where he states that data science is ``the science of dealing with data, once they have been established'' (\citet{Naur1974}). According to Naur, this is the purpose of computer science. This viewpoint is echoed in the statement, often attributed to Edsger Dijkstr, that ``Computer science is no more about computers than astronomy is about telescopes.''

Interestingly, a similar viewpoint arose in statistics, as reflected in John Tukey's statements that ``Data analysis, and the parts of statistics which adhere to it, must \ldots{} take on the characteristics of science rather than those of mathematics'' and that ``data analysis is intrinsically an empirical science'' (\citet{Tukey1962}). This movement culminated in 1997 when Jeff Wu proposed during his inaugural lecture upon becoming the chair of the University of Michigan's statistics department, entitled ``Statistics = Data Science?,'' that statistics should be called data science (\citet{Wu1997}).

These two movements\footnote{It is worth noting that these two movements were connected by substantial work in the areas of statistical computing, knowledge discovery, and data mining, with important work contributed by Gregory Piatetsky-Shapiro, Usama Fayyad, and Padhraic Smyth among many others. See \citet{Fayyad1996}, for example.} came together in 2001 in William S. Cleveland's paper ``Data Science: An Action Plan for Expanding the Technical Areas in the Field of Statistics'' (\citet{Cleveland2001}). In this highly influential monograph, Cleveland makes the key assertion that ``The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly.''

A more recent development in the history of data science has been the realization that the standard outputs of data science - such as tables, charts, reports, dashboards, and even statistical models - can be viewed as tools that must be used in the real world in order to be valuable. This realization stems from the influence of the technology sector, where the field of design has focused on improving the ease of use of websites, apps, and devices. To quote Steve Jobs, perhaps the most influential champion of design within the technology space:

\begin{quote}
``Design is not just what it looks and feels like. Design is how it works.''
\end{quote}

Based on this history, we provide our definition of \textbf{data science}:

\begin{quote}
Data science is the intersection of statistics, computer science, and industrial design.
\end{quote}

Accordingly, we use the following three definitions of these fields:

\begin{itemize}
\tightlist
\item
  \textbf{Statistics}: The branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data.
\item
  \textbf{Computer Science}: Computer science is the study of processes that interact with data and that can be represented as data in the form of programs.
\item
  \textbf{Industrial Design}: The professional service of creating and developing concepts and specifications that optimize the function, value, and appearance of products and systems for the mutual benefit of both user and manufacturer.
\end{itemize}

Hence data science is the delivery of value through the collection, processing, analysis, and interpretation of data.

\hypertarget{benefits-of-data-science}{%
\section{Benefits of Data Science}\label{benefits-of-data-science}}

Now that we have a working definition of data science, we consider some reasons for sensory and consumer scientists to embrace it. Many of these reasons apply to any modern scientific discipline, yet the fact that sensory and consumer scientists often occupy a central location in their organizations (such as sitting between product development and marketing, for example) means that sensory and consumer scientists must routinely create useful outputs for consumption by a wide variety of stakeholders. Moreover, sensory and consumer data are often diverse, so facility in data manipulation and flexibility in data analysis are especially important skills for sensory scientists.

\hypertarget{reproducible-research}{%
\subsection{Reproducible Research}\label{reproducible-research}}

One of the most important ideas in data science is that of reproducible research (cf. \citet{Peng2011}). Importantly, reproducibility in the context of data science does not refer to the repeatability of the experimental results themselves if the experiment were to be conducted again. What is instead meant by reproducible research is the ability to proceed from the input data to the final results in reproducible steps. Ideally, these steps should be well-documented so that any future researcher, including the researcher who originally conducted the work, should be able to determine all choices made in data cleaning, manipulation, and analysis that led to the final results. Since sensory and consumer scientists often work in teams, this clarity ensures that anyone on the team can understand the steps that led to prior results, and can apply those steps to their own research going forward.

\hypertarget{standardized-reporting}{%
\subsection{Standardized Reporting}\label{standardized-reporting}}

Related to the idea of reproducible research is that of standardized reporting. By following a data-scientific workflow, including automated reporting (see Chapter \ref{auto-report}), we can standardize our reporting across multiple projects. This standardization has many benefits:

\begin{itemize}
\tightlist
\item
  \textbf{Consistent Formatting} When standardized reporting is used, outputs created by a team are formatted consistently regardless of who creates them. This consistency helps consumers of the reports - whether those consumers are executives, clients, or other team members - quickly interpret results.
\item
  \textbf{Upstream Data Consistency} Once a standardized workflow is put in place, consistency of data formatting gains a new importance as producers of the report can save significant time by not having to reformat new data. This fact puts pressure on the data collection procedure to become more consistent, which ultimately supports knowledge management (ADD DATABASE REFERENCES).
\item
  \textbf{Shared Learning} Once a team combines standardized reporting with tools for online collaboration such as GitHub (see Appendix \ref{git-and-github}), any improvement to reporting (for example, to a table, chart, text output, or even to the reporting format itself) can be leveraged by all members of the team. Thus improvements compound over time, to the benefit of all team members.
\end{itemize}

\hypertarget{data-scientific-workflow}{%
\section{Data Scientific Workflow}\label{data-scientific-workflow}}

A schematic of a data scientific workflow is shown in Figure \ref{fig:ds-workflow}. Each section is described in greater detail below.

\begin{figure}
\includegraphics[width=0.9\linewidth]{images/data_science_workflow2} \caption{Data scientific workflow. (Diagram inspired by hadley/r4ds)}\label{fig:ds-workflow}
\end{figure}

\hypertarget{data-collection2}{%
\subsection{Data Collection}\label{data-collection2}}

\hypertarget{design}{%
\subsubsection{Design}\label{design}}

From the standpoint of classical statistics, experiments are conducted to test specific hypotheses and proper experimental design ensures that the data collected will allow hypotheses of interest to be tested (c.f. \citet{Fisher1935}). Sir Ronald Fisher, the father of modern statistics, felt so strongly on this topic that he said:

\begin{quote}
``To call in the statistician after the experiment is done may be no more than asking him to perform a postmortem examination: he may be able to say what the experiment died of.''
\end{quote}

This topic of designed experiments, which are necessary to fully explore causal or mechanistic explanations, is covered extensively in \citet{Lawson2014}.

Since Fisher's time, ideas around experimental design have relaxed somewhat, with \citet{Tukey1977} arguing that exploratory and confirmatory data analysis can and should proceed in tandem.

\begin{quote}
``Unless exploratory data analysis uncovers indications, usually quantitative ones, there is likely to be nothing for confirmatory data analysis to consider.

Experiments and certain planned inquires provide some exceptions and partial exceptions to this rule. They do this because one line of data analysis was planned as a part of the experiment or inquiry. \emph{Even here, however, restricting one's self to the planned analysis -- failing to accompany it with exploration -- loses sight of the most interesting results too frequently to be comfortable.} (Emphasis original)''
\end{quote}

In this book, we take no strong opinions on this topic, as they belong more properly to the study of statistics than to data science. However, we agree that results from an experiment explicitly designed to test a specific hypothesis should be viewed as more trustworthy than results incidentally obtained. Moreover, as we describe in Chapter \ref{machine-learning}, well-selected sample sets support more generalizable predictions from machine learning models.

\hypertarget{execute}{%
\subsubsection{Execute}\label{execute}}

Execution of the actual experiment is a crucial step in the data science workflow, although not one in which data scientists themselves are necessarily involved. Even so, it is imperative that data scientists communicate directly and frequently with the experimenters so that nuances of the data are properly understood for modeling and interpretation.

\hypertarget{import}{%
\subsubsection{Import}\label{import}}

Once the data are collected, they need to find their way into a computer's working memory to be analyzed. This importation process should be fully scripted in code, as we detail in Chapter \ref{data-collection}, and raw data files should never be directly edited. This discipline ensures that all steps taken to import the data will be understood later and that the reasoning behind all choices will be documented. Moreover, writing code to import raw data allows for new data to be analyzed quickly in the future as long as the data formatting is consistent. For sensory scientists, who regularly run similar tests, a streamlined workflow for data import and analysis both saves much time and protects against errors.

\hypertarget{data-preparation}{%
\subsection{Data Preparation}\label{data-preparation}}

Preparing data for analysis typically involves two steps: data inspection and data cleaning.

\hypertarget{inspect_2}{%
\subsubsection{Inspect}\label{inspect_2}}

In this step, the main goal is to gain familiarity with the data. Under ideal circumstances, this step includes reviewing the study documentation, including the study background, sampling, design, analysis plan, screener (if any), and questionnaire. As part of this step, the data should be inspected to ensure they have been imported properly and relevant data quality checks, such as checks for consistency and validity, should be performed. Preliminary summary tables and charts should also be preformed at this step to help the data scientist gain familiarity with the data. These steps are discussed in further detail in Section \ref{inspect} of Chapter \ref{data-prep}.

\hypertarget{clean_2}{%
\subsubsection{Clean}\label{clean_2}}

Data cleaning is the process of preparing data for analysis. In this step we must identify and correct any errors, and ensure the data are formatted consistently and appropriately for analysis. As part of this step, we will typically tidy our data, a concept that we cover in more detail in Section \ref{tidy-data}. It is extremely important than any changes to the data are made in code with the reasons for the changes clearly documented. This way of working ensures that, a year from now, we don't revisit our analysis to find multiple versions of the input data and not know which version was the one used for the final analysis\footnote{Anyone working in the field for more than five years has almost certainly experienced this problem, perhaps even with their own data and reports}. We discuss data cleaning in further detail in Section \ref{clean}.

\hypertarget{data-analysis2}{%
\subsection{Data Analysis}\label{data-analysis2}}

Data analysis is one of the areas of data science that most clearly overlaps with traditional statistics. In fact, any traditional or computational statistical technique can be applied within the context of data science.

In practice, the dominant cultural difference between the two fields can be summarized as:

\begin{itemize}
\tightlist
\item
  Statistics often focuses on advancing explicit theoretical understanding of an area through parameter estimation within first-principle models.
\item
  Data science often focuses on predictive ability using computational models that are validated empirically using held-out subsets of the data.
\end{itemize}

Another cultural difference between the two fields is that data science, evolving more directly out of computer science, has been more historically interested in documenting the code used for analysis with the ultimate goal of reproducible research. See \citet{Peng2011} for more information on this topic, for example. This difference is gradually disappearing, however, as statistics more fully embraces a data scientific way of scripting analyses.

Data analysis is covered in greater detail in Chapter \ref{data-analysis}. The typical steps of data analysis are data transformation, exploration, and modeling, which we review below.

\hypertarget{transform}{%
\subsubsection{Transform}\label{transform}}

Data transformation is slightly different from data preparation. In data preparation, we prepare the raw data for processing in a non-creative way, such as reshaping existing data or storing character strings representing dates as date formatted variables. With data transformation, we create new data for analysis by applying functions to the raw data. These functions can be simple transformations such as inversions or logarithms, or can be summary operations such as computing means and variances, or could be complex operations such as principle components analysis or missing value imputation. In a machine learning context (see Chapter \ref{machine-learning}), this step is often referred to as ``feature engineering.'' In any case, these functions provide the analyst an opportunity to improve the value of the analysis through skillful choices. Data transformation is covered in more detail in Chapter \ref{data-analysis}.

\hypertarget{explore}{%
\subsubsection{Explore}\label{explore}}

Just as data transformation differs slightly from data preparation, data exploration differs slightly from data inspection. When we inspect the data, our goal is to familiarize ourselves with the data and potentially spot errors as we do so. With data exploration, our goal is to begin to understand the results of the experiment and to allow the data to suggest hypotheses for follow-up analyses or future research. The key steps of data exploration are graphical visualizations (covered in Chapter \ref{data-viz}) and exploratory analyses (covered in Chapter \ref{data-analysis}). As we will discuss later in this book, employing automated tools for analysis requires caution; the ease with which we can conduct a wide range of analyses increases the risk that chance results will be regarded as meaningful. In Chapter \ref{machine-learning} we will discuss techniques, such as cross-validation, that can help mitigate this risk.

\hypertarget{model}{%
\subsubsection{Model}\label{model}}

At last we reach the modeling step of our workflow, which is the step in which we conduct formal statistical modeling. This step may also include predicitve modeling, which we cover in Chapter \ref{machine-learning}, as mentioned above. One difference between data science and classical statistics is that this step may feed back into the transform and explore steps, as data scientists are typically more willing to allow the data to suggest new hypotheses for testing (recall Tukey's quotation above). This step is described in further detail in Chapter \ref{data-analysis}.

\hypertarget{value-delivery2}{%
\subsection{Value Delivery}\label{value-delivery2}}

We now arrive at the final stage of the data science workflow, value delivery, which is the stage most influenced by industrial design. Recall the definition we provided above:

\begin{itemize}
\tightlist
\item
  \textbf{Industrial Design}: The professional service of creating and developing concepts and specifications that optimize the function, value, and appearance of products and systems for the mutual benefit of both user and manufacturer.
\end{itemize}

From this perspective, our product consists of the final results as provided to the intended audience. Consequently, we may need to adjust both the results themselves and the way they are presented according to whether the audience consists of product developers, marketing partners, upper management, or even the general public. Hence, in this stage, we communicate our results and potentially reformulate our outputs so that they will provide maximum value to the intended audience. Although we describe value delivery in more detail in Chapter \ref{value-delivery}, we briefly review the two steps of value delivery, communicate and reformulate, below.

\hypertarget{communicate}{%
\subsubsection{Communicate}\label{communicate}}

The goal of the communication step is to exchange information stemming from our data scientific work. Importantly, communication is a two-way street, so it is just as important to listen in this step as it is to share results. Without feedback from our audience, we won't be able to maximize the impact of our work. We discuss this topic in more detail in Section \ref{communicate2}, and note that automated reporting, which we cover in Chapter \ref{auto-report} also plays a large role in this step by saving time in building slides that can later be spent in thinking about the storytelling aspects of our communications.

\hypertarget{reformulate}{%
\subsubsection{Reformulate}\label{reformulate}}

In the final step of our data scientific workflow, we incorporate feedback received during the communication step back into the workflow. This step may involve investigating new questions and revising the way we present results. Since we seek to work in a reproducible manner, the improvements we make to our communication can be committed to code and the lessons these improvements reflect can be leveraged again in the future. It is also important to note that, as we reformulate, we may need to return all the way to the data cleaning step, if we learn during the communication step that some aspect of the data import or initial interpretation needs to be revised. Reformulation is discussed in greater detail in Section \ref{reformulate2}.

\hypertarget{how-to-learn-data-science}{%
\section{How to Learn Data Science}\label{how-to-learn-data-science}}

Learning data science is much like learning a language or learning to play an instrument - you have to practice. Our advice based on mentoring many students and clients is to get started sooner rather than later, and to accept that the code you'll write in the future will always be better than the code you'll write today. Also, many of the small details that separate an proficient data scientist from a novice can only be learned through practice as there are too many small details to learn them all in advance. So, starting today, do your best to write at least some code for all your projects. If a time deadline prevents you from completing the analysis in R, that's fine, but at least gain the experience of making an RStudio project and loading the data in R\footnote{We recommend following the instructions in Appendix \ref{start-R} to get started.}. Then, as time allows, try to duplicate your analyses in R, being quick to search for solutions when you run into errors. Often simply copying and pasting your error into a search engine will be enough to find the solution to your problem. Moreover, searching for solutions is its own skill that also requires practice. Finally, if you are really stuck, reach out to a colleague (or even the authors of this book) for help.

\hypertarget{cautions-dont-that-everybody-does}{%
\section{Cautions: Don't that Everybody Does}\label{cautions-dont-that-everybody-does}}

We have all been in situations in which, for a given study, we edited the raw data files (e.g.~removed respondents who were not present for the full study) and saved them using a different name. Some time later, as we need to get back to this study, or share the data with colleagues, finding the \emph{correct} file quickly becomes a challenge that may end up being time consuming.

It seems clear that such way of working is not viable, and as mentioned earlier, raw data should never be edited. Instead, we prefer to run every data manipulation steps (e.g.~removing respondents) in R by also commenting why certain decisions are being made. This simplifies deeply the workflow, and the future you will be grateful when you will re-open this file later on and can easily find out what was done and why.

The same also applies for the analysis part. Documenting which analyses and which parameters were used ensure reproducible research. If at first, documenting you code may seem \emph{a loss of time}, it will pay back later when you will access your code again later in time, as decisions taken while you are writing your code may not be so clear anymore afterwards.

Another important aspect is time: do not always go for the fastest or (what seems to be) easiest solution when coding. Instead, try to find the best possible balance between easy/fast coding and smart/efficient coding. For example, it may seem simpler to hard-code the names of the variables as they are in your data set then to read them from the file. However, this approach means that your code is restricted to that particular study, or to any study that exactly fits that format. But as soon as there is a small change (e.g.~a small difference in the naming of one of the variables), it will quickly cost you a lot of time to adapt the code to your new study.

Talking about efficiency, it is also advised to never use (through copy/paste for instance) the same lines of code more than twice. If you apply the same code multiple times in your analysis, or eventually across scrip files, consider better alternatives such as loops and/or functions. This point is of utmost importance as any small change in that piece of code (e.g.~changing a parameter, fixing a bug, etc.) only needs to be done once to be applied everywhere. On the other hand, if you reproduce the same code multiple times, you need to ensure that you correctly modified each and every parts that contain that code (and it is easy to skip some!)

Last but least, remember that coding is an endless process, as it can always be improved. So do not hesitate to go back to your own code, and update it to make it more efficient, more flexible, more concise etc. as you learn new things, or as new tools are being made available.

With these preliminaries completed, and with you (hopefully) sufficiently motivated, let's begin learning data science!

\hypertarget{data-manip}{%
\chapter{Data Manipulation}\label{data-manip}}

\begin{quote}
This chapter aims in introducing the \texttt{\{tidyverse\}} as you'll learn how to manipulate and transform your data by exploring simple (e.g.~renaming columns, sorting tables, relocating variables, etc.) to complex transformations (e.g.~transposing parts of the table, combining/merging tables, etc.)\footnote{Note that some of the examples presented in this chapter emphasize the \emph{how to?}, not the \emph{why?}, and are not necessarily chosen to convey a scientific meaning.}. Such transformations are done through simple examples that are relevant to Sensory and Consumer Science.
\end{quote}

\hypertarget{why-manipulating-data}{%
\section{Why Manipulating Data?}\label{why-manipulating-data}}

Before starting, for this chapter, most functions used are from the \texttt{\{tidyverse\}}. Also many transformations proposed are not being saved. If you want to apply these changes to your data set, please visit \ref{print-save}. Let's start with loading this package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

In sensory science, different data collection tools (e.g.~different devices, software, methodologies, etc.) may provide the same data in different ways. Also, different statistical analyses may require having the data structured differently.

A simple example to illustrate this latter point is the analysis of liking data.

Let \emph{C} consumers provide their hedonic assessments on \emph{P} samples. To evaluate if samples have received different mean liking scores at the population level, an ANOVA is performed on a long thin table with 3 columns (consumer, sample, and the liking scores), where the combination of \(C\times{P}\) is spread in rows.

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.80in}|p{0.69in}|p{0.58in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Consumer}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Liking}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{c}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{p}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(c,p)}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(C,P)}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

However, to assess whether consumers have the same preference patterns at the individual level, internal preference mapping or cluster analysis is performed. Both these analyses require as input a short and large table with the \emph{P} products in rows and the \emph{C} consumers in columns.

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.42in}|p{0.42in}|p{0.41in}|p{0.56in}|p{0.41in}|p{0.58in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{\ ...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{c}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...\ }}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{p}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{3}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(p,c)}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(p,C)}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(P,c)}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.41in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{.}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{L(P,C)}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Another example of data manipulation consists in summarizing data, by for instance computing the mean by product of the liking scores, or by generating frequency tables (e.g.~distribution of the liking scores by product etc.). In this case, the transformation alters the data as the individual differences are lost. Ultimately, the output table is smaller as it would contain here only \emph{P} rows and 2 columns:

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.58in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Liking}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6.4}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8.2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{p}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7.1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{...}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6.1}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

For these reasons, it is essential to learn to manipulate data and transition from one structure to another (when possible).

\hypertarget{tidy-data}{%
\section{Tidying Data}\label{tidy-data}}

Hadley Wickham (\citet{Wickham2014}) defined \emph{tidy data} as ``data sets that are arranged such that each variable is a column and each observation (or case) is a row.'' Depending on the statistical unit to consider and the analyses to perform, data may need to be manipulated to be presented in a tidy form.

\hypertarget{simple_manips}{%
\subsection{Simple Manipulations}\label{simple_manips}}

We define here as \emph{simple manipulations} any data transformations that can easily be performed in other software such as Excel (using copy-paste, sorting and filtering, creating a pivot table, etc.). However, we strongly recommend performing any sorts of transformation in R as this will reduce the risk of errors, typically be faster, more reliable, and will be reusable if you need to perform the same operations on similar data in the future (including updated versions of the current data set). Moreover, these operations will become easier and more natural for you to use as you get familiar with them. Most importantly, performing these transformations in R do not alter the original data, meaning that changes can be reverted (which is not possible when you alter the raw data directly).

\hypertarget{handling-columns}{%
\subsubsection{Handling Columns}\label{handling-columns}}

\hypertarget{renaming}{%
\paragraph*{Renaming Variables}\label{renaming}}
\addcontentsline{toc}{paragraph}{Renaming Variables}

Before starting transforming any data, we need data. So let's start with importing the \emph{biscuits\_sensory\_profile.xlsx} file\footnote{For more details about the data set and/or data importation, please see Section \ref{data-import}.}. For importing the data, the packages \texttt{\{here\}} and \texttt{\{readxl\}} are being used. Here, the data are being saved in the object called \texttt{sensory}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{)}
\NormalTok{sensory }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first simple transformation we consider consists of renaming one or multiple variables. This procedure can easily be done using the \texttt{rename()} function from \texttt{\{dplyr\}}. In each of the examples below, we use the \texttt{names()} function to show just the names of the resulting data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Judge"                   
## [2] "Product"                 
## [3] "Shiny"                   
## [4] "External color intensity"
## [5] "Color evenness"
\end{verbatim}

In \texttt{sensory}, let's rename \texttt{Judge} into \texttt{Panellist}, and \texttt{Product} into \texttt{Sample} (here we apply transformations without saving the results, so the original data set remains unchanged).

To do so, we indicate in \texttt{rename()} that \emph{new\_name} is replacing \emph{old\_name} as following \texttt{rename(newname\ =\ oldname)}. Additionally, we can apply multiple changes by simply separating them with a \texttt{,}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Panellist"               
## [2] "Sample"                  
## [3] "Shiny"                   
## [4] "External color intensity"
## [5] "Color evenness"
\end{verbatim}

Alternatively, it is also possible to rename a column using its position:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Consumer =} \DecValTok{1}\NormalTok{, }\AttributeTok{Biscuit =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Consumer"                
## [2] "Biscuit"                 
## [3] "Shiny"                   
## [4] "External color intensity"
## [5] "Color evenness"
\end{verbatim}

If this procedure of renaming variables should be applied on many variables following a structured form (e.g.~transforming names into snake\_case, CamelCase, \ldots, see \url{https://en.wikipedia.org/wiki/Letter_case\#Use_within_programming_languages} for more information), the use of the \texttt{\{janitor\}} package comes handy thanks to its \texttt{clean\_names()} function and the \texttt{case} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}

\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{(}\AttributeTok{case =} \StringTok{"snake"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "judge"                   
## [2] "product"                 
## [3] "shiny"                   
## [4] "external_color_intensity"
## [5] "color_evenness"
\end{verbatim}

Note that the \texttt{\{janitor\}} package offers many options, and although the transformation is performed here on all the variables, it is possible to apply it on certain variables only.

\hypertarget{re-organizing-columns}{%
\paragraph*{Re-Organizing Columns}\label{re-organizing-columns}}
\addcontentsline{toc}{paragraph}{Re-Organizing Columns}

Another simple transformation consists in re-organizing the data set, either by re-ordering, adding, and/or removing columns.

For re-ordering columns, \texttt{relocate()} is being used. This function allows re-positioning a (set of) variable(s) before or after another variable. By re-using the \texttt{sensory} data set, let's position all the variables starting with `Qty' between \texttt{Product} and \texttt{Shiny}. This can be specified in two different ways, either by positioning them after \texttt{Product} or before \texttt{Shiny}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"Qty"}\NormalTok{), }\AttributeTok{.after =}\NormalTok{ Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}

\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"Qty"}\NormalTok{), }\AttributeTok{.before =}\NormalTok{ Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Judge"                     
## [2] "Product"                   
## [3] "Qty of inclusions"         
## [4] "Qty of inclusions in mouth"
## [5] "Shiny"
\end{verbatim}

\begin{verbatim}
## [1] "Judge"                     
## [2] "Product"                   
## [3] "Qty of inclusions"         
## [4] "Qty of inclusions in mouth"
## [5] "Shiny"
\end{verbatim}

\hypertarget{removingselecting-columns}{%
\paragraph*{Removing/Selecting Columns}\label{removingselecting-columns}}
\addcontentsline{toc}{paragraph}{Removing/Selecting Columns}

Another very important function regarding columns transformation is the \texttt{select()} function from \texttt{\{dplyr\}} (see Section \ref{packages} for a justification of the particular writing \texttt{dplyr::select()}) which allows selecting a set of variables, by simply informing the variables that should be kept in the data. Let's limit ourselves in selecting \texttt{Judge}, \texttt{Product}, and \texttt{Shiny}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 3
##   Judge Product Shiny
##   <chr> <chr>   <dbl>
## 1 J01   P01      52.8
## 2 J01   P02      48.6
## 3 J01   P03      48  
## 4 J01   P04      46.2
## # ... with 95 more rows
\end{verbatim}

When a long series of variables should be kept in the same order, the use of the \texttt{:} is used.

Let's keep all the variables going from \texttt{Cereal\ flavor} to \texttt{Dairy\ flavor}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\StringTok{\textasciigrave{}}\AttributeTok{Cereal flavor}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Dairy flavor}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 6
##   Judge Product `Cereal flavor` RawDo~1 Fatty~2 Dairy~3
##   <chr> <chr>             <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01                24.6    28.2    13.8       0
## 2 J01   P02                25.8    28.8     7.2       0
## 3 J01   P03                30      26.4     0         0
## 4 J01   P04                16.2    28.2     0         0
## # ... with 95 more rows, and abbreviated variable
## #   names 1: `RawDough flavor`, 2: `Fatty flavor`,
## #   3: `Dairy flavor`
\end{verbatim}

However, when only one (or few) variable needs to be removed, it is easier to specify which one to remove rather than informing all the ones to keep. Such solution is then done using the \texttt{-} sign. The previous example can then be obtained using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Shiny, Melting))}
\end{Highlighting}
\end{Shaded}

The selection process of variables can be further informed through functions such as \texttt{starts\_with()}, \texttt{ends\_with()}, and \texttt{contains()}, which all select variables that either starts, ends, or contains a certain character or sequence of character. To illustrate this, let's only keep the variables that starts with `Qty':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"Qty"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 2
##   `Qty of inclusions` `Qty of inclusions in mouth`
##                 <dbl>                        <dbl>
## 1                 9.6                         27.6
## 2                10.8                         22.2
## 3                 7.8                         10.2
## 4                 0                           13.2
## # ... with 95 more rows
\end{verbatim}

Rather than selecting variables based on their names, we can also select them based on their position (e.g.~\texttt{dplyr::select(2:5)} to keep the variables that are at position 2 to 5).

Selection of variables can also be done using some \emph{rules} thanks to the \texttt{where()} function. Let's consider the situation in which we only want to keep the variables that are nominal (or \emph{character} in R), which automatically keeps \texttt{Judge} and \texttt{Product}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 2
##   Judge Product
##   <chr> <chr>  
## 1 J01   P01    
## 2 J01   P02    
## 3 J01   P03    
## 4 J01   P04    
## # ... with 95 more rows
\end{verbatim}

\texttt{dplyr::select()} is a very powerful function that facilitates the selection of complex variables through very intuitive functions. Ultimately, it can also be used to \texttt{relocate()} and even \texttt{rename()} variables, as shown in the example below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{Panellist =}\NormalTok{ Judge, }\AttributeTok{Sample =}\NormalTok{ Product, }
\NormalTok{                Shiny}\SpecialCharTok{:}\NormalTok{Thickness, }\SpecialCharTok{{-}}\FunctionTok{contains}\NormalTok{(}\StringTok{"olor"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

More examples illustrating the use of \texttt{dplyr::select()} are provided throughout the book. In particular, in the next session, another important function called \texttt{across()} will be introduced. This function can also be associated with \texttt{dplyr::select()} to give even more freedom in our selection. An example of such application is given in \ldots{}

\hypertarget{creating-columns}{%
\paragraph*{Creating Columns}\label{creating-columns}}
\addcontentsline{toc}{paragraph}{Creating Columns}

In some cases, new variables need to be created from existing ones. Examples of such situations include taking the quadratic term of a sensory attribute to test for curvature, or simply considering a new variables as the sum or the subtraction between two (or more) others. Such creation of a variable is processed through the \texttt{mutate()} function from the \texttt{\{dplyr\}} package. This function takes as inputs the name of the variable to create, and the \emph{formula} that defines that variable.

Let's create two new variables, one called \texttt{Shiny2} which corresponds to \texttt{Shiny} squared up, and one called \texttt{StiMelt} which corresponds to \texttt{Sticky} + \texttt{Melting.} Since we only use these three variables, let's first reduce the data set to these three variables with \texttt{select()} to improve readability:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Shiny, Sticky, Melting) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Shiny2 =}\NormalTok{ Shiny}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}
         \AttributeTok{StiMelt =}\NormalTok{ Sticky }\SpecialCharTok{+}\NormalTok{ Melting)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 5
##   Shiny Sticky Melting Shiny2 StiMelt
##   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>
## 1  52.8   37.2    33.6  2788.    70.8
## 2  48.6   35.4    36    2362.    71.4
## 3  48     37.2     8.4  2304     45.6
## 4  46.2   21.6    34.2  2134.    55.8
## # ... with 95 more rows
\end{verbatim}

\begin{quote}
If you want to transform a variable, say by changing its type, or re-writing its content, you can use \texttt{mutate()} and assign to the new variable the same name as the original one. This will overwrite the existing column with the new one. To illustrate this, let's transform \texttt{Product} from upper case to lower case only. This can be done by mutating \texttt{Product} into the lowercase version of \texttt{Product} (\texttt{tolower(Product)}):
\texttt{sensory\ \%\textgreater{}\%\ mutate(Product\ =\ tolower(Product))}
\texttt{mutate()} being one of the most important function from the \texttt{\{dplyr\}} package, more examples of its use are presented throughout this book.
\end{quote}

Since performing mathematical computations on non-numerical columns is not possible, conditions can easily be added through \texttt{mutate()} combined with \texttt{across()}. Let's imagine we want to round all variables to 0 decimal, which can only be applied to numerical variables.
To do so, we \texttt{mutate()} \texttt{across()} all variables that are considered \texttt{as.numeric()} (using \texttt{where()}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), round, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01        53        30      23      10      23
## 2 J01   P02        49        30      13      11      13
## 3 J01   P03        48        46      17       8      14
## 4 J01   P04        46        46      38       0      49
## # ... with 95 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

In case only a selection of numerical variables should be rounded, we could also replace \texttt{where(is.numeric)} by a vector (using \texttt{c()}) with the names of the variables to round.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Shiny, Sticky, Melting) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Shiny"}\NormalTok{, }\StringTok{"Sticky"}\NormalTok{), round, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{merging-and-separating-columns}{%
\paragraph*{Merging and Separating columns}\label{merging-and-separating-columns}}
\addcontentsline{toc}{paragraph}{Merging and Separating columns}

It can happen that some columns of a data set contain information (strings) that cover different types of information. For instance, we could imagine coding the name of our panelists as \emph{FirstName\_LastName} or \emph{Gender\_Name}, and we would want to separate them into two columns to make the distinction between the different information (i.e.~\emph{FirstName} and \emph{LastName}, or \emph{Gender} and \emph{Name} respectively). In other situations, we may want to merge information present in multiple columns into one.

For illustration, let's consider the information stored in the \emph{Product Info} sheet from \emph{biscuits\_sensory\_profile.xlsx}. This table includes information regarding the biscuits, and more precisely their Protein and Fiber content (Low or High).

After importing the data, let's merge these two columns so that both information is stored in one column called \texttt{ProtFib}.
To do so, \texttt{unite()} (from \texttt{\{tidyr\}}) is used. This function takes as first element the name of the new variables, followed by all the columns to \emph{unite}, and by providing the separation key to use between these elements (here \emph{-}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prod\_info }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Product Info"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(ProtFib, Protein, Fiber, }\AttributeTok{sep =} \StringTok{"{-}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 3
##   Product ProtFib   Type 
##   <chr>   <chr>     <chr>
## 1 P01     Low-Low   Trial
## 2 P02     Low-High  Trial
## 3 P03     High-High Trial
## 4 P04     High-High Trial
## # ... with 7 more rows
\end{verbatim}

By default, \texttt{unite()} removes from the data set the individual variables that have been merged. To keep these original variables, the parameter \texttt{remove\ =\ FALSE} can be used.

\begin{quote}
Although it is not relevant for combining columns, it is interesting to mention an additional package that can be used to combine elements together. This package is called \texttt{\{glue\}} and provides interesting alternatives to the usual \texttt{paste()} and \texttt{paste0()} functions.
\end{quote}

To reverse the changes (saved here in \texttt{prod\_info}) and to separate a column into different variables, \texttt{separate()} (from \texttt{\{tidyr\}}) is used. Similarly to \texttt{unite()}, \texttt{separate()} takes as first parameter the name of the variable to split, followed by the names for the different segments generated, and of course the separator defined by \texttt{sep}.

In our example, this would be done as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prod\_info }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(ProtFib, }\FunctionTok{c}\NormalTok{(}\StringTok{"Protein"}\NormalTok{, }\StringTok{"Fiber"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"{-}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 4
##   Product Protein Fiber Type 
##   <chr>   <chr>   <chr> <chr>
## 1 P01     Low     Low   Trial
## 2 P02     Low     High  Trial
## 3 P03     High    High  Trial
## 4 P04     High    High  Trial
## # ... with 7 more rows
\end{verbatim}

\hypertarget{conditions}{%
\paragraph*{Conditions}\label{conditions}}
\addcontentsline{toc}{paragraph}{Conditions}

In some cases, the new column to create depend directly on the value(s) of one or more columns present in the data.
An examples of such situations consists in categorizing a continuous variable into groups by converting the age (in year) of the participants into age groups. For such simple examples, some pre-existing functions (e.g.~\texttt{cut()} in this situation) can be used.
However, in other situations, pre-defined functions do not exist and the transformation should be done \emph{manually} using conditions.

Let's illustrate this by converting the \texttt{Shiny} variable (from \texttt{sensory}) from numeric to classes.
Since the scale used is a 60pt scale, let's start by creating a class called \emph{Low} if the score is lower than 30, and \emph{High} otherwise.

Here, pre-defined functions (e.g.~\texttt{cut()}) are not being used intentionally as a manual transformation is preferred. Instead, \texttt{mutate()} is associated to \texttt{ifelse()}, which works as following: \texttt{ifelse(condition,\ results\ if\ condition\ is\ TRUE,\ results\ if\ condition\ is\ not\ TRUE)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ShinyGroup =} \FunctionTok{ifelse}\NormalTok{(Shiny }\SpecialCharTok{\textless{}} \DecValTok{30}\NormalTok{, }\StringTok{"Low"}\NormalTok{, }\StringTok{"High"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 2
##   Shiny ShinyGroup
##   <dbl> <chr>     
## 1  52.8 High      
## 2  48.6 High      
## 3  48   High      
## 4  46.2 High      
## # ... with 95 more rows
\end{verbatim}

Let's imagine the same variable should now be split into three levels: \emph{Low}, \emph{Medium}, and \emph{High}. Such additional group could be obtained by adding an \texttt{ifelse()} condition within the existing \texttt{ifelse()} condition (we use 48 instead of 40 for the upper limit to \texttt{Medium} so that results are displayed on screen):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ShinyGroup =} \FunctionTok{ifelse}\NormalTok{(Shiny }\SpecialCharTok{\textless{}} \DecValTok{20}\NormalTok{, }\StringTok{"Low"}\NormalTok{,}
                             \FunctionTok{ifelse}\NormalTok{(Shiny }\SpecialCharTok{\textless{}} \DecValTok{48}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 2
##   Shiny ShinyGroup
##   <dbl> <chr>     
## 1  52.8 High      
## 2  48.6 High      
## 3  48   High      
## 4  46.2 Medium    
## # ... with 95 more rows
\end{verbatim}

Since there are only 3 conditions in total here, only two entangled \texttt{ifelse()} are required. This makes the code still manageable. However, in more complex situations (say 10 different conditions are required), such solution quickly becomes tedious to read, to track, and to debug if errors are being made. Instead, the use of an alternative function called \texttt{case\_when()} is preferred. In the previous case, the same conditions would be written as follow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ShinyGroup =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    Shiny }\SpecialCharTok{\textless{}} \DecValTok{20} \SpecialCharTok{\textasciitilde{}} \StringTok{"Low"}\NormalTok{,}
    \FunctionTok{between}\NormalTok{(Shiny, }\DecValTok{20}\NormalTok{, }\DecValTok{48}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Medium"}\NormalTok{,}
\NormalTok{    Shiny }\SpecialCharTok{\textgreater{}} \DecValTok{40} \SpecialCharTok{\textasciitilde{}} \StringTok{"High"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 2
##   Shiny ShinyGroup
##   <dbl> <chr>     
## 1  52.8 High      
## 2  48.6 High      
## 3  48   Medium    
## 4  46.2 Medium    
## # ... with 95 more rows
\end{verbatim}

This provides the same results as previously, except for the exact value 48 which was assigned as \emph{High} in the \texttt{ifelse()} example and to \emph{Medium} in the \texttt{case\_when()} example. This is due to the way \texttt{between()}\footnote{By default, \texttt{between(x,\ value1,\ value2)} considers value1 \textless= x \textless= value2.} considers its borders.

\hypertarget{handling-rows}{%
\subsubsection{Handling Rows}\label{handling-rows}}

After manipulating columns, the next logical step is to manipulate rows. Such operations include three aspects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Re-arranging the rows in a logical way;
\item
  Selecting certain rows;
\item
  Filtering entries based on given variables;
\item
  Splitting the data in sub-groups based on the entries of a given variable.
\end{enumerate}

\hypertarget{re-arranging-rows}{%
\paragraph*{Re-arranging Rows}\label{re-arranging-rows}}
\addcontentsline{toc}{paragraph}{Re-arranging Rows}

The first step of re-arranging rows is done through the \texttt{arrange()} function from the \texttt{\{dplyr\}} package. This function allows sorting the data in the ascending order\footnote{For numerical order, this is simply re-arranging the values from the lowest to the highest. For strings, the entries are then sorted alphabetically unless the variable is a factor in which case the order of the levels for that factor is being used.}. To arrange them in a descending order, the function \texttt{desc()} is then required.

Let's re-arrange the data by \texttt{Judge} and \texttt{Product}, \texttt{Judge} being sorted in an ascending order whereas \texttt{Product} is being sorted in a descending order:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Judge, }\FunctionTok{desc}\NormalTok{(Product))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   POpt      4.8      33.6    15.6    32.4    13.8
## 2 J01   P10      53.4      36.6    11.4    18      10.8
## 3 J01   P09       0        42.6    18      21      36  
## 4 J01   P08       0        51.6    48.6    23.4    18  
## # ... with 95 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

\hypertarget{selecting-rows}{%
\paragraph*{Selecting Rows}\label{selecting-rows}}
\addcontentsline{toc}{paragraph}{Selecting Rows}

The next step is to select a subset of the data by keeping certain rows only. If the position of the rows to keep is know, this information can be used directly using the \texttt{slice()} function. Let's select from \texttt{sensory} all the data that is related to \texttt{P01}. A quick look at the data informs us that it corresponds to rows 1 to 89, with a step of 11:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{11}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01      52.8      30      22.8     9.6    22.8
## 2 J02   P01      44.4      34.2    14.4    18.6    43.2
## 3 J03   P01      40.2      23.4     9       7.8    49.8
## 4 J04   P01      37.8      26.4    15      23.4    15.6
## # ... with 5 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

This is a manual way to select data. However, this procedure may generate an erroneous subset in the case that the row order in the data changes. To avoid mistakes, a more \emph{stable} procedure of filtering data is proposed in the next section.

\hypertarget{filtering-data}{%
\paragraph*{Filtering Data}\label{filtering-data}}
\addcontentsline{toc}{paragraph}{Filtering Data}

To define sub-set of data, the \texttt{filter()} function is being used. This function requires providing an argument that is expressed as a \emph{test}, meaning that the outcome should either be TRUE (keep the value) or FALSE (discard the value) when the condition is verified or not respectively. In R, this is expressed by the double `=' sign \texttt{==}.

Let's filter the data to only keep the data related to sample \texttt{P02}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Product }\SpecialCharTok{==} \StringTok{"P02"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P02      48.6      30      13.2    10.8    13.2
## 2 J02   P02      39.6      32.4    18      19.8    25.2
## 3 J03   P02      39        18.6    13.2     9      28.8
## 4 J04   P02      39.6      41.4    33      25.2    10.2
## # ... with 5 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

Other relevant test characters are the following:

\begin{itemize}
\tightlist
\item
  \texttt{!Product\ ==\ "P02"} or \texttt{Product\ !=\ "P02"} means different from, and will keep all samples except \texttt{P02};
\item
  \texttt{\%in\%\ my\_vector} keeps any value included within the vector \texttt{my\_vector} (e.g.~\texttt{Product\ \%in\%\ c("P01","P02","P03")} keeps all data from \texttt{P01}, \texttt{P02}, and \texttt{P03})
\end{itemize}

In some cases, the tests to perform are more complex as they require multiple conditions.
There are two forms of conditions:

\begin{itemize}
\tightlist
\item
  \texttt{\&} (read \emph{and}) is multiplicative, meaning that all the conditions need to be true (\texttt{Product\ ==\ "P02"\ \&\ Shiny\ \textgreater{}\ 40});
\item
  \texttt{\textbar{}} (read \emph{or}) is additive, meaning that only one of the conditions needs to be true (\texttt{Product\ ==\ "P03"\ \textbar{}\ Shiny\ \textgreater{}\ 40})
\end{itemize}

This system of condition is particularly useful when you have missing values as you could remove all the rows that contain missing values for a given variable. Since we do not have missing values here, let's create some by replacing all the evaluations for \texttt{Shiny} that are larger than 40 by missing values. This is done here using \texttt{ifelse()}, which takes three arguments (in this order): the test to perform (here \texttt{Shiny\ \textgreater{}\ 40}), the instruction if the test passes (here replace with \texttt{NA}), and the instruction if the test doesn't pass (here keep the value stored in \texttt{Shiny}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_na }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Shiny =} \FunctionTok{ifelse}\NormalTok{(Shiny }\SpecialCharTok{\textgreater{}} \DecValTok{40}\NormalTok{, }\ConstantTok{NA}\NormalTok{, Shiny))}
\end{Highlighting}
\end{Shaded}

In a second step, we filter out all missing values from \texttt{Shiny}. In practice, this is done by keeping all the values that are not missing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_na }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Shiny))}
\end{Highlighting}
\end{Shaded}

This procedure removed 20 rows since the original table had 99 rows and 3 columns, whereas the filtered table only has 79 rows and 3 columns.

\hypertarget{splitting-data}{%
\paragraph*{Splitting Data}\label{splitting-data}}
\addcontentsline{toc}{paragraph}{Splitting Data}

After filtering data, the next logical step is to split data into subsets based on a given variable (e.g.~by gender). For such purpose, one could consider using \texttt{filter()} by applying it to each subgroup. In a previous example, this is what we have done when we filtered data for sample \texttt{P02} only. Of course, the same procedure can be performed until all the other sub-groups are created. However, this solution becomes tedious as the number of samples increases. Instead, we prefer to use \texttt{split()} which takes as arguments the data and the column to split from:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Product)}
\end{Highlighting}
\end{Shaded}

This function creates a list of \emph{n} elements (\emph{n} being the number of samples), each element corresponding to the data related to one sample. Such list can then be used in automated analyses by performing on each sub-data through the \texttt{map()} function, as it will be illustrated in section \ref{data-analysis}.

\hypertarget{pivots}{%
\subsection{Reshaping Data}\label{pivots}}

Reshaping the data itself is done through pivoting which allows transitioning from a long and thin table to a short and wide table and vice-versa.

To illustrate this, let's start with a fictive example in which we have 3 consumers providing their liking scores for 2 products, This table corresponds to a long and thin format:

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.80in}|p{0.69in}|p{0.58in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Consumer}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Liking}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{9}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C3}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.8in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C3}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Let's imagine we need to re-structure the data where products are displayed in rows, and consumers in columns. This corresponds to the short and wide format:

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.42in}|p{0.42in}|p{0.42in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C3}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{9}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.42in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

As we will see in the following section, it is very easy to transition from one version to another thanks to \texttt{pivot\_longer()} and \texttt{pivot\_wider()}, both being functions from \texttt{\{tidyr\}}.

\hypertarget{pivot_longer}{%
\subsubsection{Pivoting Longer}\label{pivot_longer}}

Currently, our \texttt{sensory} data table is a table in which we have as many rows as Judge x Product, the different attributes being spread across multiple columns. However, in certain situations, it is relevant to have all the attributes stacked vertically, meaning that the table will have Judge x Product x Attributes rows. Such simple transformation can be done with \texttt{pivot\_longer()} which takes as inputs the attributes to pivot, the name of the variables that will contain these names (\texttt{names\_to}), and the name of the column that will contain their entries (\texttt{values\_to})

\begin{quote}
With \texttt{pivot\_longer()} and any other function that requires selecting variables, it is often easier to deselect variables that we do not want to include rather than selecting all the variables of interest. Throughout the book, both solutions are being considered.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3,168 x 4
##   Judge Product Attribute                Score
##   <chr> <chr>   <chr>                    <dbl>
## 1 J01   P01     Shiny                     52.8
## 2 J01   P01     External color intensity  30  
## 3 J01   P01     Color evenness            22.8
## 4 J01   P01     Qty of inclusions          9.6
## # ... with 3,164 more rows
\end{verbatim}

This transformation converts a table of 99 rows and 34 columns into a table containing 3168 (99\emph{32) rows and 4 columns.
In the pivoted table, the names of the variables (stored here in }Attribute*) are in the same order as presented in the original table.

In case the attribute names are following a standard structure, say ``attribute\_name modality'' as is the case in \texttt{sensory} for some attributes, an additional parameter of \texttt{pivot\_longer()} becomes handy as it can split the \texttt{Attribute} variable just created into say \texttt{Attribute} and \texttt{Modality}.

To illustrate this, let's reduce \texttt{sensory} to \texttt{Judge}, \texttt{Product}, and all the variables that end with odor or flavor (all other variables being discarded). After pivoting the subset of columns, we automatically split the attribute names into \texttt{Attribute} and \texttt{Modality} by informing the separator between names (here, a space):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }
                \FunctionTok{ends\_with}\NormalTok{(}\StringTok{"odor"}\NormalTok{), }\FunctionTok{ends\_with}\NormalTok{(}\StringTok{"flavor"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Judge, Product), }
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{"Attribute"}\NormalTok{, }\StringTok{"Modality"}\NormalTok{), }
               \AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{, }\AttributeTok{names\_sep =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 693 x 5
##   Judge Product Attribute Modality Score
##   <chr> <chr>   <chr>     <chr>    <dbl>
## 1 J01   P01     Fatty     odor       6.6
## 2 J01   P01     Roasted   odor      15.6
## 3 J01   P01     Cereal    flavor    24.6
## 4 J01   P01     RawDough  flavor    28.2
## # ... with 689 more rows
\end{verbatim}

This parameter combines both the power of \texttt{pivot\_longer()} and \texttt{separate()} in one unique process.

Note that more complex transformations through the use of regular expressions (and the \texttt{names\_pattern} option) can be considered. More information on regular expression is provided in \ref{regex}

\begin{quote}
It can happen that with \texttt{pivot\_longer()}, further transformation performed on the long and thin table may not maintain their original order (usually, the names are re-ordered alphabetically). If you don't want such re-ordering to happen because it would impact the desired outcome, there is a simple workaround that ensures that the order is kept. The solution simply consists in transforming the newly created variable as a factor which takes as levels the order of the elements as they were in the original data (use \texttt{fct\_inorder()} to maintain the order as shown in the data). An example is shown in the following code:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny, Salty, Bitter, Light) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"Judge"}\NormalTok{, }\StringTok{"Product"}\NormalTok{), }
               \AttributeTok{names\_to =} \StringTok{"Variable"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Variable =} \FunctionTok{fct\_inorder}\NormalTok{(Variable))}
\end{Highlighting}
\end{Shaded}

Other examples using this trick will be used throughout this book (e.g.~see Section \ref{tipsntricks})

As an alternative to \texttt{pivot\_longer()}, the package called \texttt{\{reshape2\}} provides a function called \texttt{melt()} which pivots automatically the entire set of numerical variables, the qualitative variables being considered as \emph{id variables}. If performed on a matrix with row names, the new table will have two columns containing the row and column names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reshape2)}
\FunctionTok{melt}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

\hypertarget{pivoting-wider}{%
\subsubsection{Pivoting Wider}\label{pivoting-wider}}

The complementary/opposite function to \texttt{pivot\_longer()} is \texttt{pivot\_wider()}. This function pivots data horizontally, hence reducing the number of rows and increasing the number of columns. In this case, the two main parameters to provide is which column will provide the new columns to create (\texttt{name\_from}), and what are the values to use to fill this table (\texttt{values\_from}).

From the previous example, we could set \texttt{names\_from\ =\ Attribute} and \texttt{values\_from\ =\ Score} to return to the original format of \texttt{sensory}. However, let's reduce the data set to \texttt{Product}, \texttt{Judge}, and \texttt{Shiny} only, and let's pivot the \texttt{Judge} and \texttt{Shiny} columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Judge, }\AttributeTok{values\_from =}\NormalTok{ Shiny)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 10
##   Product   J01   J02   J03   J04   J05   J06   J07
##   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1 P01      52.8  44.4  40.2  37.8  43.8  43.2  44.4
## 2 P02      48.6  39.6  39    39.6  33.6  38.4  25.8
## 3 P03      48    36    35.4  15    37.2  33    16.2
## 4 P04      46.2  36    48    38.4  47.4  37.8  27  
## # ... with 7 more rows, and 2 more variables:
## #   J08 <dbl>, J09 <dbl>
\end{verbatim}

This procedure creates a table with as many rows as there are products, and as many columns as there are panelists (+1 since the product information is also in a column).

These procedures are particularly useful in consumer studies, since \texttt{pivot\_longer()} and \texttt{pivot\_wider()} allows restructuring the data for analysis such as ANOVA (\texttt{pivot\_longer()} output) and preference mapping or clustering (\texttt{pivot\_wider()} structure).

It is important to notice that the \texttt{pivot\_wider()} format potentially contains more data. Let's imagine the sensory test was performed following an incomplete design, meaning that each panelist did not evaluate all the samples. Although the long and thin structure would not show missing values (the entire rows without data being removed), the shorter and larger version would contain missing values for the products that panelists did not evaluate. If the user wants to automatically replace these missing values with a fixed value, say, it is possible through the parameter \texttt{values\_fill} (e.g.~\texttt{values\_fill=0} would replace each missing value with a 0). Additionally, after pivoting the data, if multiple entries exist for a combination row-column, \texttt{pivot\_wider()} will return a list of elements. In the next Section, an example illustrating such situation and how to present it will be presented.

\hypertarget{transformation-that-alters-the-data}{%
\subsection{Transformation that Alters the Data}\label{transformation-that-alters-the-data}}

In some cases, the final table to generate requires altering the data, by (say) computing the mean across multiple values, or counting the number of occurrences of factor levels for instance. In other words, we summarize the information, which also tend to reduce the size of the table. It is hence no surprise that the function used for such data reduction is called \texttt{summarise()} (or \texttt{summarize()}, both notation work) and belongs to the \texttt{\{dplyr\}} package.

\hypertarget{introduction-to-summary-statistics}{%
\subsubsection{Introduction to Summary Statistics}\label{introduction-to-summary-statistics}}

In practice, \texttt{summarise()} applies a function (whether it is the \texttt{mean()}, or a simple count using \texttt{n()} for instance) on a set of values. Let's compute the mean on all numerical variables of \texttt{sensory}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 32
##   Shiny Exter~1 Color~2 Qty o~3 Surfa~4 Print~5 Thick~6
##   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
## 1  23.9    33.7    28.2    20.6    23.3    40.7    25.5
## # ... with 25 more variables: `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>,
## #   `Roasted flavor` <dbl>,
## #   `Overall flavor persistence` <dbl>, ...
\end{verbatim}

As can be seen, the grand mean is computed for each attribute. It can also be noticed that all the other variables that were not involved have been removed (e.g.~\texttt{Judge} and \texttt{Product} as they are not numerical variables).

If multiple functions should be applied, we could perform all the transformation simultaneously as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), }\FunctionTok{list}\NormalTok{(}\AttributeTok{min =}\NormalTok{ min, }\AttributeTok{max =}\NormalTok{ max)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 64
##   Shiny_min Shiny_max Externa~1 Exter~2 Color~3 Color~4
##       <dbl>     <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1         0        54       6.6    55.2     6.6    53.4
## # ... with 58 more variables:
## #   `Qty of inclusions_min` <dbl>,
## #   `Qty of inclusions_max` <dbl>,
## #   `Surface defects_min` <dbl>,
## #   `Surface defects_max` <dbl>,
## #   `Print quality_min` <dbl>,
## #   `Print quality_max` <dbl>, ...
\end{verbatim}

In this example, each attribute is duplicated with \texttt{\_min} and \texttt{\_max} to provide the minimum and maximum value for each attribute.

\begin{quote}
It would be a good exercise to restructure this table using \texttt{pivot\_longer()} with \texttt{names\_sep} followed by \texttt{pivot\_wider()} to build a new table that shows for each attribute (in rows) the minimum and the maximum in two different columns.
\end{quote}

By following the same principles, many other functions can be performed, whether they are built-in R or created by the user.

Here is a recommendation of interesting descriptive functions to consider with \texttt{summarise()}:

\begin{itemize}
\tightlist
\item
  \texttt{mean()}, \texttt{median()} (or more generally \texttt{quantile()}) for the mean and median (or any other quantile);
\item
  \texttt{sd()} and \texttt{var()} for the standard deviation and the variance;
\item
  \texttt{min()}, \texttt{max()}, \texttt{range()} (provides both the min and max) or \texttt{diff(range())} (for the difference between min and max);
\item
  \texttt{n()} and \texttt{sum()} for the number of counts and the sum respectively.
\end{itemize}

\hypertarget{introduction-to-grouping}{%
\subsubsection{\texorpdfstring{Introduction to \emph{grouping}}{Introduction to grouping}}\label{introduction-to-grouping}}

It can appear that the interest is not in the grand mean, but in the mean per product (say), or per product and panelist (for test with duplicates). In such cases, \texttt{summarize()} should aggregate set of values per product, or per product and panelist respectively. Such information can be passed on through \texttt{group\_by()}\footnote{We strongly recommend you to \texttt{ungroup()} blocks of code that includes \texttt{group\_by()} once the computations are done to avoid any unexpected results. Otherwise, further computations may be done on the groups when it should be performed on the full data.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), mean)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 33
##   Product Shiny Exter~1 Color~2 Qty o~3 Surfa~4 Print~5
##   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
## 1 P01      41.9    26.2    15.8    15.5    27.7    37.7
## 2 P02      39.1    29.5    20.3    14      20.5    39.5
## 3 P03      30.5    43.6    30.7    17.6    18.6    43.3
## 4 P04      42.6    43.3    37.7    15.1    32.8    30.3
## # ... with 7 more rows, 26 more variables:
## #   Thickness <dbl>, `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>,
## #   `Roasted flavor` <dbl>, ...
\end{verbatim}

This procedure creates a tibble with 11 rows (product) and 33 columns (32 sensory attributes + 1 column including the product information) which contains the mean per attribute for each sample, also known as the sensory profiles of the products.

\begin{quote}
In some cases, the data should not be aggregated across rows, but by rows. In such cases, it is important to specify that each computation should be done per row by using \texttt{rowwise()} prior to performing the transformation. For instance, if we would want to extract the minimum between \texttt{Shiny}, \texttt{Salty}, and \texttt{Bitter}, we could write the following code:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Shiny, Salty, Bitter) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Min =} \FunctionTok{min}\NormalTok{(Shiny, Salty, Bitter))}
\end{Highlighting}
\end{Shaded}

\hypertarget{means}{%
\subsubsection{Illustrations of Data Manipulation}\label{means}}

Let's review the different transformations presented here by generating the sensory profiles of the samples through different approaches\footnote{It is important to realize that any data manipulation challenge can be tackled in many different ways, so don't be afraid to think out of the box when solving them.}.

In the previous example, we've seen how to obtain the sensory profile using \texttt{summarise()} \texttt{across()} all numerical variables. In case a selection of the attributes should have been done, we could use the same process by simply informing which attributes to transform:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, mean)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The list of attributes to include can also be stored in an external vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_attr }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(sensory)[}\DecValTok{5}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(sensory)]}
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{all\_of}\NormalTok{(sensory\_attr), mean)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

A different approach consists in combining \texttt{summarise()} to \texttt{pivot\_longer()} and \texttt{pivot\_wider()}. This process requires summarizing only one column by Product and Attribute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Attribute =} \FunctionTok{fct\_inorder}\NormalTok{(Attribute)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product, Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Scores =} \FunctionTok{mean}\NormalTok{(Scores)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Attribute, }\AttributeTok{values\_from =}\NormalTok{ Scores) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Here, we transformed \texttt{Attribute} into a factor using \texttt{fct\_inorder()} to ensure that the double pivoting procedure maintains the original order. Without this line of code, the final table would have the columns reordered alphabetically.

\begin{quote}
As you can see, R provides the following message: \emph{\texttt{summarise()} has grouped output by `Product'. You can override using the \texttt{.groups} argument.} This message is just informative, and can be hidden by adding the following code at the start of your script: \texttt{options(dplyr.summarise.inform\ =\ FALSE)}.
\end{quote}

What would happen if we would omit to \texttt{summarise()} the data in between the two pivoting functions? In that case, we also remove the column \texttt{Judge} since the means should be computed across panelists\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Attribute, }\AttributeTok{values\_from =}\NormalTok{ Scores)}
\end{Highlighting}
\end{Shaded}

As can be seen, each variable is of type list in which each cell contains \texttt{dbl\ {[}9{]}}: This corresponds to the scores provided by the 9 panelists to that product and that attribute. Since we would ultimately want the mean of these 9 values to generate the sensory profiles, a solution comes directly within \texttt{pivot\_wider()} through the parameter \texttt{values\_fn} which applies the function provided here on each set of values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Attribute, }\AttributeTok{values\_from =}\NormalTok{ Scores, }
              \AttributeTok{values\_fn =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

Through this simple example, we've seen that the same results can be obtained through different ways. It is important to keep this in mind as you may find the solution to your own challenges by simply considering different paths.

\hypertarget{combining-data-from-different-sources}{%
\subsection{Combining Data from Different Sources}\label{combining-data-from-different-sources}}

It often happens that the data to analyze is stored in different files, and need to be combined or merged. Depending on the situations, different solutions are required.

\hypertarget{binding-vertically}{%
\subsubsection{Binding Vertically}\label{binding-vertically}}

Let's start with a simple example where the tables match in terms of variables, and should be combined vertically. To illustrate this situation, the data stored in the file \emph{excel-scrap.xlsx} is used. This file contains a fictive example in which 12 assessors evaluated 2 samples on 3 attributes in triplicate, each replication being stored in a different sheet.

The goal here is to read the data stored in the different sheets, and to combine them vertically in one unique file for further analysis. Let's start with importing the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path }\OtherTok{\textless{}{-}} \FunctionTok{file.path}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"excel\_scrap.xlsx"}\NormalTok{)}
\NormalTok{session1 }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(path, }\AttributeTok{sheet =} \DecValTok{1}\NormalTok{)}
\NormalTok{session2 }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(path, }\AttributeTok{sheet =} \DecValTok{2}\NormalTok{)}
\NormalTok{session3 }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(path, }\AttributeTok{sheet =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To combine the tables vertically, we could use the basic R function \texttt{rbind()}. However, we prefer to use \texttt{bind\_rows()} from \texttt{\{dplyr\}} since it better controls for the columns by ensuring that the order is respected. Moreover, if one of the tables contains a variable that the other don't, this variable will be kept and filled in with missing values when the information is missing. Additionally, we\texttt{bind\_rows()} allows keeping track of the \emph{origin} of the data through the parameter \texttt{.id}. This is of particular interest in this example since a new \texttt{Session} column can be created (and used) to distinguish between tables. This process is used to avoid losing such useful information especially since it is not directly available within the data: If it were, the parameter \texttt{.id} could have been ignored.

This solution works fine, especially since there were only 3 files to combine. Ultimately, we would prefer to automate the reading process so that all the files are directly imported and combined. This more efficient solution is presented in \ref{import-mult-sheet}.

\hypertarget{binding-horizontally}{%
\subsubsection{Binding Horizontally}\label{binding-horizontally}}

In other cases, the tables to combine contain different information (variables) on the same entities (rows), and the tables should be merged horizontally. To do so, a first solution consists in using the functions \texttt{cbind()} (\texttt{\{base\}}) and/or \texttt{bind\_cols()} (\texttt{\{dplyr\}}) can be used. However, some of these functions require that the tables to combine must already have the rows in the exact same order (no check is being done) and must be of the same size.

If that is not the case, merging tables should be done using \texttt{merge()} (\texttt{\{base\}}), or preferably through the different \texttt{*\_join()} functions from (\texttt{\{dplyr\}}). For illustration, let's consider these two tables to merge:

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{3}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{D}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Depending on the \emph{merging degree} to consider between tables X and Y, there are four different \texttt{*\_join()} versions to consider:

\begin{itemize}
\tightlist
\item
  \texttt{full\_join()} keeps all the cases from X and Y regardless whether they are present in the other table or not (in case they are not present, missing values are introduced) {[}it corresponds to \texttt{merge(all=TRUE)}{]};
\end{itemize}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{3}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{D}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{inner\_join()} only keeps the common cases, i.e.~cases that are present in both X and Y {[}corresponds to \texttt{merge(all=FALSE)}{]};
\end{itemize}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{left\_join()} keeps all the cases from X {[}corresponds to \texttt{merge(all.x=TRUE,\ all.y=FALSE)}{]};
\end{itemize}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{3}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{right\_join()} keeps all the cases from Y {[}corresponds to \texttt{merge(all.x=FALSE,\ all.y=TRUE)}{]};
\end{itemize}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var2}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{1}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{2}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{D}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{4}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{anti\_join()} only keeps the elements from X that are not present in Y (this is particularly useful if you have a tibble Y of elements that you would like to remove from X).
\end{itemize}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.52in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Var1}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.52in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{3}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

The merging procedure requires the users to provide a \emph{key}, i.e.~a (set of) variable(s) used to combine the tables. For each unique element defined by the key, a line is being created. When needed, rows of a table are being duplicated. Within the different \texttt{*\_join()} functions, the key is informed by the \texttt{by} parameter, which may contain one or more variables with the same or different names.

For illustration, let's use the data set called \emph{biscuits\_consumer\_test.xlsx}, which contains three tabs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"biscuits\_consumer\_test.xlsx"}\NormalTok{)}
\FunctionTok{excel\_sheets}\NormalTok{(file\_path)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Biscuits"         "Time Consumption"
## [3] "Weight"
\end{verbatim}

The three sheets contain the following information, which need to be combined:

\begin{itemize}
\tightlist
\item
  \emph{Biscuits}: The consumers' evaluation of the 10 products and their assessment on liking, hunger, etc. at different moments of the test.
\item
  \emph{Time Consumption}: The amount of cookies and the time required to evaluate them in each sitting.
\item
  \emph{Weight}: The weight associated to each cookie.
\end{itemize}

Let's start by combining \emph{Time Consumption} and \emph{Weight} so that we can compute the total weight of biscuits eaten by each consumer in each sitting. In this case, the joining procedure is done by \texttt{Product} since the weight is only provided for each product. The total weight eaten (\texttt{Amount}) is then computed by multiplying the number of cookies eaten (\texttt{Nb\ biscuits}) by \texttt{Weight}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Time Consumption"}\NormalTok{)}
\NormalTok{weight }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Weight"}\NormalTok{)}

\NormalTok{consumption }\OtherTok{\textless{}{-}}\NormalTok{ time }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{full\_join}\NormalTok{(weight, }\AttributeTok{by =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Amount =} \StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}} \SpecialCharTok{*}\NormalTok{ Weight)}
\end{Highlighting}
\end{Shaded}

As can be seen, the \texttt{Weight} information stored in the \emph{Weight} sheet has been replicated every time each sample has been evaluated by another respondent.

The next step is then to merge this table to \texttt{Biscuits}. In this case, since both data set contain the full evaluation of the cookies (each consumer evaluating each product), the joining procedure needs to be done by \texttt{Judge} and \texttt{Product} simultaneously. A quick look at the data shows two important things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \emph{Biscuits}, the consumer names only contain the numbers whereas in \texttt{consumption}, they also contain a \texttt{J} in front of the name: This needs to be fixed as the names need to be identical to be merged, else they will be considered separately and missing values will be introduced. In practice, this will be done by mutating \texttt{Consumer} and by pasting a \texttt{J} in front of the number using the function \texttt{paste0()}.
\item
  The names that contain the product (\texttt{Samples} and \texttt{Product}) and consumers (\texttt{Consumer} and \texttt{Judge}) information are different in both data set. We could rename these columns in one data set to match the other, but instead we will keep the two names and inform it within \texttt{full\_join()}. This is done through the \texttt{by} parameter as following: \texttt{"name\ in\ dataset\ 1"\ =\ "name\ in\ dataset\ 2"}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{biscuits }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Biscuits"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Consumer =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"J"}\NormalTok{, Consumer)) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{full\_join}\NormalTok{(consumption, }
             \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"Consumer"} \OtherTok{=} \StringTok{"Judge"}\NormalTok{, }\StringTok{"Samples"} \OtherTok{=} \StringTok{"Product"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The three data sets are now flawlessly joined into one that can be further manipulated and/or analysed.

\hypertarget{data-viz}{%
\chapter{Data Visualization}\label{data-viz}}

\begin{quote}
One of the main goal of data analysis is to produce results. Although informative, such results are only impactful if they can be well-communicated. It is hence of utmost importance to present them in a neat way, often through visuals.
In this chapter, two forms of visuals (namely tables and graphs) are being generated using R. Although some design principles are being tackled, the aim of this chapter is to mainly focus on the \emph{how to?} rather than on the design itself.
Although R already comes with tools for building and printing tables and graphs, we opt for using additional packages (\texttt{\{flextable\}} and \texttt{\{gt\}} for tables, \texttt{\{ggplot2\}} for graphs) as they provide more flexibility and possibilities.
\end{quote}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Tables and graphs are the two fundamental vehicles to communicate information clearly and effectively. They are useful visual elements to summarize and organize information to show patterns and relationships. Tables and graphs allow the audience/reader to easily and quickly get a clear idea of the data findings, make comparisons, get insights from it and ultimately, draft conclusions without much effort.

The best medium of communication, whether a table, a bar chart, a line chart, or a radar plot, will highly depend on the type of data, the amount of data to be displayed (e.g.~number of attributes or samples), and the purpose of the analysis.

Usually, tables are meant to be read, so they are ideal when you have data that cannot easily be presented by other communication elements, or when the data requires more specific attention. However, if you encounter a situation where you have a very long and/or wide table, (which is common in sensory and consumer studies), other vehicle of communication should be considered. The same remark also applies to a graphical visualization, and if you have very little data to display, tables might be best suited.

Sometimes (if not often) you have to play with your data, and test displaying it as a table or different types of graphs, before deciding which one suit best. As practical advice, do not hesitate to ask colleagues for feedback as having an external point of view often helps. Remember, to select the best way to communicate your data, you must understand the needs of your audience, the purpose for which various forms of display can be effectively used, but also the strengths and weaknesses of each type of data representation considered.

\hypertarget{design-principles}{%
\section{Design Principles}\label{design-principles}}

Regardless of the way you decide to display your data, you must understand visual perception and its application to graphical communication. It is important to spend some time with the design and aesthetic aspects of your visualization. You should be able to recognize smart design by becoming familiar with some aspects and examples of great design. Inattention to the visual design such as tables with improper alignment of numbers, excessive use of lines and fill colors, can greatly diminish their effectiveness. In other words, when used adequately, design should help you communicate your results by \emph{clarifying} it, and not distract your audience from it.

Some important pre-attentive aspects that you should be aware of will be presented in this section, but to read more about visual perception and graphical communication, as well as some examples of great design, we strongly recommend \emph{Storytelling with Data} by Cole Nussbaumer Knaflic and \emph{Show me the Number: Designing Table and Graphs to Enlighten} by Stephen Few. (REF)

Since a \emph{picture is worth a thousand words}, let's demonstrate the difference between pre-attentive and attentive processing using an example provided by Stephen Few in his book \emph{Show me the Number: Designing Table and Graphs to Enlighten}. First, take a look at the numbers below and determine, as quickly as you can, how many times the number 5 appears:

98734979027513649782316497802316487415113697412369846321
12346879231648791300023665698774646821397486248797964312
12369874474578962341680021364789613469174312679812439612
12332146987412361789461230502135467980213648126498731203

This appears to be a tedious task and it most likely took you a few minutes because it involved \textbf{attentive processing}. The list of numbers did not have any hint (also called pre-attentive attributes) that could help you to easily distinguish the number five from the other numbers. Hence, you are forced to perform a sequential search throughout the whole list.

Let's do it again, but now using the list of numbers below:

98734979027\textbf{5}1364978231649780231648741\textbf{5}113697412369846321
1234687923164879130002366\textbf{5}698774646821397486248797964312
12369874474\textbf{5}78962341680021364789613469174312679812439612
12332146987412361789461230\textbf{5}0213\textbf{5}467980213648126498731203

This time the task is much easier and you can count the number of times the number 5 appears much faster. This is because we used the \textbf{pre-attentive attribute} of color intensity to distinguish the number five, standing it out in contrast to the rest. This example shows in an easy way the power of pre-attentive attributes for effective visual communication. As stated by Cole Nussbaumer Knaflic in her book \emph{Storytelling with Data}, when we use pre-attentive attributes strategically, we enable our audience to see what we want them to see before they even know they are seeing it!

The various pre-attentive attributes that can be used to draw your audience's attention quickly and create a visual hierarchy of information include \textbf{attributes of form} such as line length, line width, orientation, shape, size, added marks, and enclosure, \textbf{attributes of color} which would be hue and intensity, also spatial position and motion. Some of the strategies for a smart design for graphical communication described by Cole Nussbaumer include:

\begin{itemize}
\tightlist
\item
  \textbf{Highlight the important stuff} - use tools such as bold, italics, underlining, uppercase text, color, and different sizes to draw your audience's attention to what you want them to focus on.
\item
  \textbf{Eliminate Distractions} - while some elements should be highlighted, unnecessary or irrelevant items or information should be identified to be cut or de-emphasized to minimize your audience's distraction. Get rid of noncritical data or components, things that wouldn't change the main message, and summarize when details are not needed. When a piece of information is necessary to come along with your visualization but is not really a message-impacting, you should de-emphasize it - light gray usually works well for that purpose.
\end{itemize}

Now the main stage is set, let's focus on how to build nice tables and graphs in R.

\hypertarget{table_making}{%
\section{Table Making}\label{table_making}}

By default, R allows printing matrices or data frames as tables. However, these tables cannot be customized and are only informative. An example of table can be shown here:

\begin{verbatim}
##            Name col 1 Name col 2 Name col 3
## Name row 1          1          3          5
## Name row 2          2          4          6
\end{verbatim}

To extend table customization, dedicated packages are required. For illustration, the sensory data (stored in \emph{biscuits\_sensory\_profile.xlsx}) is used.
Before starting, let's first load the usual libraries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readxl)}
\FunctionTok{library}\NormalTok{(here)}
\end{Highlighting}
\end{Shaded}

Let's imagine we want to communicate the sensory profiles of the 11 biscuits on a few attributes including \texttt{Shiny}, \texttt{Sweet}, \texttt{Sour}, \texttt{Bitter}, and \texttt{Salty}. A first step consists in transforming the data to create such results (see \ref{means} for more details).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{) }
\NormalTok{mean\_sensory }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Product, Shiny, Sweet, Sour, Bitter, Salty) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{where}\NormalTok{(is.numeric),}\AttributeTok{.fns =}\NormalTok{ mean))}
\end{Highlighting}
\end{Shaded}

\hypertarget{flextable}{%
\subsection{\texorpdfstring{Introduction to \texttt{\{flextable\}}}{Introduction to \{flextable\}}}\label{flextable}}

Now the correct table has been created, let's represent it in a neater way using our first dedicated package called \texttt{\{flextable\}}. Before going too deep in designing the table, let's simply apply the \texttt{flextable()} function to \texttt{mean\_sensory} (after loading \texttt{\{flextable\}} first):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(flextable)  }
\NormalTok{flex\_table }\OtherTok{\textless{}{-}}\NormalTok{ mean\_sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{flextable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.64in}|p{0.59in}|p{0.56in}|p{0.59in}|p{0.59in}|p{0.54in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Shiny}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Sweet}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Sour}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Bitter}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{Salty}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{41.933}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{22.20}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{8.000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{5.100}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{39.133}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{15.80}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{4.933}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{2.933}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{30.533}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{10.40}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{7.800}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{4.667}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{42.600}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{16.60}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{4.267}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{3.600}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{13.933}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{21.00}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{3.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{6.733}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{5.867}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{6.867}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{14.73}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{4.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{6.533}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{8.200}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{32.467}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{19.47}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.8667}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{5.000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{2.800}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{8.933}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{14.27}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.9333}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{11.067}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{2.467}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{3.533}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{13.33}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{0.0000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{9.267}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{4.000}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{26.133}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{30.47}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{5.0667}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{9.667}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{9.000}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.64in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{16.867}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.56in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{17.73}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{2.2000}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.59in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{15.867}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{6.667}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

As you can see, the table is being printed in the \emph{Viewer} section of RStudio. This table is already better designed, although it is still quite overcrowded that could benefit from some additional design work to look nicer.

A first simple improvement consists in reducing the number of decimals (\texttt{colformat\_double()}), changing the font size (\texttt{fontsize()}, not visible in our output) and type (\texttt{bold()} or \texttt{italic()}), and aligning the text (\texttt{align()}) for instance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flex\_table\_design }\OtherTok{\textless{}{-}}\NormalTok{ flex\_table }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{colformat\_double}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fontsize}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{part =} \StringTok{"all"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bold}\NormalTok{(}\AttributeTok{bold =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{italic}\NormalTok{(}\AttributeTok{j =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{italic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{part =} \StringTok{"body"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{align}\NormalTok{(}\AttributeTok{align =} \StringTok{"center"}\NormalTok{, }\AttributeTok{part =} \StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.65in}|p{0.54in}|p{0.57in}|p{0.50in}|p{0.55in}|p{0.51in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Product}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Shiny}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sweet}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sour}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Bitter}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Salty}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{41.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{22.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.10}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{39.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.93}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{10.40}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{7.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.67}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{42.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.60}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{21.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.87}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.20}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{32.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{19.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.80}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{11.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.47}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.33}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{26.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.67}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{17.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.67}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

As can be seen, the function names are very intuitive, and so are the options. In particular, it is interesting to see that most functions allows applying changes to the entire table (\texttt{part\ =\ "all"}), the header only (\texttt{part\ =\ "header"}) or the body only (\texttt{part\ =\ "body"}). And even within a part, it is possible to make a sub-selection by selecting the rows (\texttt{i}) or columns (\texttt{j}) to include or exclude: Here, all the text in the body is set in italic except for the product names, hence the option \texttt{j=-1} (read exclude the first column).

After presenting some of the basic aesthetic options, let's go once step further and play around with coloring. For instance, let's imagine we would change the header background and text color, and would want to call the audience's attention by highlighting the optimized formulation. The following code could be used to do this (results are not being saved):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flex\_table\_design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bg}\NormalTok{(}\AttributeTok{bg =} \StringTok{"black"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{color}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fontsize}\NormalTok{(}\AttributeTok{size =} \DecValTok{13}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{, }\AttributeTok{i =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{color}\NormalTok{(}\AttributeTok{i =} \DecValTok{11}\NormalTok{, }\AttributeTok{color =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{part =} \StringTok{"body"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{color}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey70"}\NormalTok{, }\AttributeTok{part =} \StringTok{"body"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_header\_lines}\NormalTok{(}\AttributeTok{values =} \StringTok{"Sensory Profile of 11 biscuits"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.65in}|p{0.54in}|p{0.57in}|p{0.50in}|p{0.55in}|p{0.51in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{6}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 3.31in+10\tabcolsep+5\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sensory\ Profile\ of\ 11\ biscuits}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Product}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Shiny}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sweet}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sour}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Bitter}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\cellcolor[HTML]{000000}\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{FFFFFF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Salty}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{41.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{22.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.10}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{39.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.93}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{10.40}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{7.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.67}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{42.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.60}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{21.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.87}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.20}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{32.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{19.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.80}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{11.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.47}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.33}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{26.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.67}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{B3B3B3}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{17.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{FFA500}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.67}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Alternatively, we could decide to be more sober by applying other pre-attentive attributes. For instance, the size of the table can be adjusted, and an horizontal line can be added to delimit the optimal sample from the other. For the latter part, the customization of the line can be made using the function \texttt{fp\_border()} from the \texttt{\{officer\}} package\footnote{More information regarding the \texttt{\{officer\}} package are provided in Section \ref{powerpoint}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(officer)}
\NormalTok{flex\_table\_design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{hline}\NormalTok{(}\AttributeTok{i=}\DecValTok{10}\NormalTok{, }\AttributeTok{border=}\FunctionTok{fp\_border}\NormalTok{(}\AttributeTok{color=}\StringTok{"grey70"}\NormalTok{, }\AttributeTok{style=}\StringTok{"dashed"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autofit}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.65in}|p{0.54in}|p{0.57in}|p{0.50in}|p{0.55in}|p{0.51in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Product}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Shiny}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sweet}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sour}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Bitter}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Salty}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{41.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{22.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.10}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{39.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.93}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{10.40}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{7.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.67}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{42.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.60}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{21.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.87}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.20}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{32.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{19.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.80}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{11.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.47}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.33}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{26.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.67}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.00}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}->{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}->{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}->{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}->{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}->{\arrayrulecolor[HTML]{B3B3B3}\global\arrayrulewidth=1pt}-}



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{17.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.67}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

In some situations, applying some design options might mess up the appearance of your table, in particular its border lines. If that should happen to you, just apply the function \texttt{fix\_border\_issues()} at the end of your code to solve it.

Lastly, you can use conditional formatting if you want to highlight some specific values (e.g.~values for \texttt{Sweet} that are below 20 should be colored in blue, and above 30 in red), as in the example below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{color\_code }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(mean\_sensory}\SpecialCharTok{$}\NormalTok{Sweet }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }
                     \FunctionTok{ifelse}\NormalTok{(mean\_sensory}\SpecialCharTok{$}\NormalTok{Sweet }\SpecialCharTok{\textgreater{}=} \DecValTok{30}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{))}
\NormalTok{flex\_table\_design }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{color}\NormalTok{(}\AttributeTok{j=}\SpecialCharTok{\textasciitilde{}}\NormalTok{Sweet, }\AttributeTok{color=}\NormalTok{color\_code)}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.65in}|p{0.54in}|p{0.57in}|p{0.50in}|p{0.55in}|p{0.51in}}



\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Product}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Shiny}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sweet}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Sour}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Bitter}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textbf{Salty}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{41.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{22.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.10}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{39.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.93}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{10.40}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{7.80}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.67}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{42.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.60}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.60}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{21.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.87}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.20}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{32.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{19.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.80}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{8.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{14.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.93}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{11.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.47}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{3.53}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{13.33}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{0.00}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.27}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{4.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{26.13}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{FF0000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{30.47}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{5.07}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.67}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{9.00}}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.65in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{16.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{0000FF}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{17.73}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.5in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{2.20}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.55in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{15.87}}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.51in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{8}{8}\selectfont{\global\setmainfont{Calibri}{\textit{6.67}}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Other illustrations of the use of \texttt{\{flextable\}} are provided in section \ref{powerpoint}.

For curious readers who want to get a deeper look into all the possibilities provided by this package, we refer them to the book \href{https://ardata-fr.github.io/flextable-book/index.html}{\emph{Using the flextable R package}} by David Gohel and to its \href{https://ardata.fr/en/flextable-gallery/}{gallery of tables} for inspiration.

\hypertarget{introdution-to-gt}{%
\subsection{\texorpdfstring{Introdution to \texttt{\{gt\}}}{Introdution to \{gt\}}}\label{introdution-to-gt}}

As an alternative to \texttt{\{flextable\}}, the \texttt{\{gt\}} package can also be considered as it also produces nice-looking tables for reports or presentations.
Let's first install (if needed) and load the \texttt{\{gt\}} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gt)}
\end{Highlighting}
\end{Shaded}

Focusing now on the consumer study (\emph{biscuits\_consumer\_test.xlsx}), let's display a table with the average number of biscuits (for each variant) consumers ate, and their corresponding eating time. To do so, we first need to transform the time columns (expressed as min and s) to a double format and express them in minutes. Then, we can group them by product to get the average for the time spent and the number of biscuits eaten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_consumer\_test.xlsx"}\NormalTok{) }

\NormalTok{mean\_consumer }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }
                                   \AttributeTok{sheet=}\StringTok{"Time Consumption"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Product, }\StringTok{\textasciigrave{}}\AttributeTok{Time (min)}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Time (min)}\StringTok{\textasciigrave{}}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Min"}\NormalTok{, }\StringTok{"Sec"}\NormalTok{), }\AttributeTok{sep=}\StringTok{"min"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Min"}\NormalTok{,}\StringTok{"Sec"}\NormalTok{), as.numeric)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Time =}\NormalTok{ Min}\SpecialCharTok{+}\NormalTok{Sec}\SpecialCharTok{/}\DecValTok{60}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Time"}\NormalTok{, }\StringTok{"Nb biscuits"}\NormalTok{), mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now that the data is ready, we can display it with some basics adjustments to make it look nicer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_consumer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gt}\NormalTok{ () }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cols\_align}\NormalTok{(}\AttributeTok{align =} \StringTok{"center"}\NormalTok{, }\AttributeTok{columns =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fmt\_number}\NormalTok{(}\AttributeTok{columns =} \FunctionTok{c}\NormalTok{(}\StringTok{"Time"}\NormalTok{, }\StringTok{"Nb biscuits"}\NormalTok{) , }\AttributeTok{decimals =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tab\_header}\NormalTok{(}\AttributeTok{title =} \FunctionTok{md}\NormalTok{(}\StringTok{"**Cons. time and nb. of biscuits eaten**"}\NormalTok{), }
             \AttributeTok{subtitle =} \FunctionTok{md}\NormalTok{ (}\StringTok{"*Average taken from 99 consumers*"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{ccc}
\caption*{
{\large \textbf{Cons. time and nb. of biscuits eaten}} \\ 
{\small \emph{Average taken from 99 consumers}}
} \\ 
\toprule
Product & Time & Nb biscuits \\ 
\midrule
1 & $6.47$ & $3.94$ \\ 
2 & $6.74$ & $4.00$ \\ 
3 & $6.46$ & $2.91$ \\ 
4 & $6.48$ & $2.61$ \\ 
5 & $6.57$ & $4.06$ \\ 
6 & $6.45$ & $3.42$ \\ 
7 & $6.55$ & $3.58$ \\ 
8 & $6.23$ & $2.77$ \\ 
9 & $6.24$ & $3.01$ \\ 
10 & $6.95$ & $4.41$ \\ 
\bottomrule
\end{longtable}

Note that we used \emph{Markdown} to style the title and subtitle by wrapping the values passed to the title or subtitle with the \texttt{md()} function. In Markdown, \texttt{**text**} writes the text in bold, and \texttt{*text*} in italic.

The \texttt{\{gt\}} package offers several resources to make beautiful tables. Let's illustrate this by focusing on the average number of biscuits eaten only since the average consumption time is very similar across products. The idea is to use pre-attentive attributes for the audience to clearly see which samples were the most popular (i.e.~the most eaten) and which one were not. Let's first prepare the data and calculate the overall time consumption considering all products.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_consumer\_2 }\OtherTok{\textless{}{-}}\NormalTok{ mean\_consumer }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}} \StringTok{"Time"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{ (}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now that the data is ready, we can display using a similar style as before in which we add some color-code to accentuate products' consumption. We will also add a \texttt{note} to the table that expresses the average time used to consumer the biscuits. So let's start with creating the table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{note }\OtherTok{\textless{}{-}} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"Avg. consumption time: "}\NormalTok{, }
              \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mean\_consumer}\SpecialCharTok{$}\NormalTok{Time),}\DecValTok{2}\NormalTok{), }\StringTok{" min"}\NormalTok{)}

\NormalTok{consumption }\OtherTok{\textless{}{-}}\NormalTok{ mean\_consumer\_2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gt}\NormalTok{ () }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cols\_align}\NormalTok{(}\AttributeTok{align =} \StringTok{"center"}\NormalTok{, }\AttributeTok{columns =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fmt\_number}\NormalTok{(}\AttributeTok{columns =} \StringTok{"Nb biscuits"}\NormalTok{ , }\AttributeTok{decimals =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tab\_header}\NormalTok{(}\AttributeTok{title =} \FunctionTok{md}\NormalTok{ (}\StringTok{"**Number of biscuits eaten**"}\NormalTok{), }
             \AttributeTok{subtitle =} \FunctionTok{md}\NormalTok{ (}\StringTok{"*Average taken from 99 consumers*"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tab\_source\_note}\NormalTok{(}\AttributeTok{source\_note =}\NormalTok{ note)}
\end{Highlighting}
\end{Shaded}

Now, let's color code the cells based on the average number of biscuits eaten. To color code, the range of average number of biscuits eaten is required. Then, we can use the \texttt{col\_numeric()} function from the \texttt{\{scales\}} package to generate the colors of interest (in practice, we provide the colour for the minimum, maximum, and the function generates automatically all the colors in between to create the gradient).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scales)}
\NormalTok{nb\_range }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(mean\_consumer\_2}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{)}
\NormalTok{consumption }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{data\_color}\NormalTok{(}\AttributeTok{columns=}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{, }
               \AttributeTok{colors=}\FunctionTok{col\_numeric}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"\#FEF0D9"}\NormalTok{,}\StringTok{"\#990000"}\NormalTok{), }
                                  \AttributeTok{domain=}\NormalTok{nb\_range, }\AttributeTok{alpha=}\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\setlength{\LTpost}{0mm}
\begin{longtable}{cc}
\caption*{
{\large \textbf{Number of biscuits eaten}} \\ 
{\small \emph{Average taken from 99 consumers}}
} \\ 
\toprule
Product & Nb biscuits \\ 
\midrule
10 & $4.41$ \\ 
5 & $4.06$ \\ 
2 & $4.00$ \\ 
1 & $3.94$ \\ 
7 & $3.58$ \\ 
6 & $3.42$ \\ 
9 & $3.01$ \\ 
3 & $2.91$ \\ 
8 & $2.77$ \\ 
4 & $2.61$ \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Avg. consumption time: 6.52 min\\
\end{minipage}

Applying this strategy of coloring the number of biscuits eaten according to their range makes the table nicer and easier to get insights from. In our case, we can quickly see that groups of products based on their average consumption: Product 10 is the most eaten, followed by a group that include products 5, 2, and 1. At last, samples 8 and 4 are the least consumed samples.

Although the package \texttt{\{gt\}} proposed some nice feature, additional options are provided by its extension package called \texttt{\{gtExtras\}} which provides additional themes, formatting capabilities, opinionated diverging color palette, extra tools for highlighting values, possibility of embed bar plots in the table etc. For more information, please check \url{https://jthomasmock.github.io/gtExtras/}.

To illustrate one of the possible use of \texttt{\{gtExtras\}} let's twist the previous table as following: Since each consumer was provided with a maximum of 10 biscuits, let's transform the average consumption into percentages. We can then re-create the previous table in which we also add a bar-chart based on the percentages:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gtExtras)}

\NormalTok{mean\_consumer\_2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits (\%)}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{100}\SpecialCharTok{*}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\SpecialCharTok{/}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gt}\NormalTok{ () }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cols\_align}\NormalTok{(}\AttributeTok{align =} \StringTok{"center"}\NormalTok{, }\AttributeTok{columns =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fmt\_number}\NormalTok{(}\AttributeTok{columns =} \StringTok{"Nb biscuits"}\NormalTok{ , }\AttributeTok{decimals=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tab\_header}\NormalTok{(}\AttributeTok{title =} \FunctionTok{md}\NormalTok{ (}\StringTok{"**Number of biscuits eaten**"}\NormalTok{), }
             \AttributeTok{subtitle =} \FunctionTok{md}\NormalTok{ (}\StringTok{"*Average taken from 99 consumers*"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tab\_source\_note}\NormalTok{(}\AttributeTok{source\_note =}\NormalTok{ note) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gt\_plt\_bar\_pct}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits (\%)}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{scaled=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this section, due to their simplicity and flexibility, we emphasized the use of \texttt{\{flextable\}} and \texttt{\{gt\}} to build beautiful tables for your reports. However, there are other alternatives including (amongst others) \texttt{\{kable\}} and \texttt{\{kableExtra\}}, or \texttt{\{huxtable\}} for readers that are not yet fully satisfied.

\hypertarget{chart-making}{%
\section{Chart Making}\label{chart-making}}

``A picture is worth 1000 words''. This saying definitely applies to Statistics as well, since visual representation of data often appears clearer than the values themselves stored in a table. It is hence no surprise that R is also a powerful tool for building graphics.

In practice, there are various ways to build graphics in R. In fact, R itself comes with an engine for building graphs through the \texttt{plot()} function. An extensive description can be found in (\emph{R Graphics 2nd edition Paul Murrell CRC Press}). Due to its philosophy, its simplicity, and the point of view adopted in this book, we will limit ourselves to graphics built using the \texttt{\{ggplot2\}} package.

\hypertarget{philosophy-of-ggplot2}{%
\subsection{\texorpdfstring{Philosophy of \texttt{\{ggplot2\}}}{Philosophy of \{ggplot2\}}}\label{philosophy-of-ggplot2}}

\texttt{\{ggplot2\}} belongs to the \texttt{\{tidyverse\}}, and was developed by H. Wickham and colleagues at RStudio. It is hence no surprise that a lot of the procedures that we are learning throughout this book also applies to \texttt{\{ggplot2\}}. More generally, building graphics with \texttt{\{ggplot2\}} fits very well within the pipes (\texttt{\%\textgreater{}\%}) system from \texttt{\{magrittr\}}. In fact, \texttt{\{ggplot2\}} works with its own piping system that uses the \texttt{+} symbol instead of \texttt{\%\textgreater{}\%}.

In practice, \texttt{\{ggplot2\}} is a multi-layer graphical tools, and graphics are built by adding layers to existing graphs. The advantage of such procedure is that \texttt{ggplot} objects are not fixed: They can be printed at any time, and can still be improved by adding other layers if needed. To read more about \texttt{\{gglot2\}} and its philosophy, please refer to \url{http://vita.had.co.nz/papers/layered-grammar.pdf}\href{http://vita.had.co.nz/papers/layered-grammar.pdf}{link}.

Note that since building graphics is limited to one's imagination, it is not possible to tackle each and every possibilities offered by \texttt{\{ggplot2\}} (and its extensions). For that reason, we prefer to focus on how \texttt{\{ggplot2\}}works, and by using as illustration throughout the book examples of graphics that are useful in Sensory and Consumer Science. This should be more than sufficient to get you started, and should cover 90\% of your daily needs. Still, if that should not be sufficient, we invite you to look into the online documentation or to references such as \url{https://r-graph-gallery.com/}\href{https://r-graph-gallery.com/}{link}.

\hypertarget{getting-started-with-ggplot2}{%
\subsection{\texorpdfstring{Getting started with \texttt{\{ggplot2\}}}{Getting started with \{ggplot2\}}}\label{getting-started-with-ggplot2}}

To use \texttt{\{ggplot2\}}, it needs to be loaded. This can either be done directly using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

However, as said before, this step is not always needed since it is part of \texttt{\{tidyverse\}}: At the start of this chapter, when we loaded the \texttt{\{tidyverse\}} package, we also loaded \texttt{\{ggplot2\}}.

To illustrate the use of \texttt{\{ggplot2\}}, both the sensory data (\emph{biscuits\_sensory\_profile.xlsx}) and the number of biscuit eaten by each respondents (\emph{biscuits\_consumer\_test.xlsx}) are used. Although these files have already been loaded in Section @ref(table\_making), let's re-load them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{) }
\NormalTok{p\_info }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Product Info"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Type)}

\NormalTok{sensory }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(p\_info, }\AttributeTok{by=}\StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{relocate}\NormalTok{(Protein}\SpecialCharTok{:}\NormalTok{Fiber, }\AttributeTok{.after=}\NormalTok{Product)}

\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{,}\StringTok{"biscuits\_consumer\_test.xlsx"}\NormalTok{)}

\NormalTok{Nbiscuits }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Time Consumption"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Product =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"P"}\NormalTok{, Product)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{N =} \StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To initiate a graph, the function \texttt{ggplot()} is called.
Since the data to be used are stored in \texttt{sensory}, \texttt{ggplot()} is applied on \texttt{sensory}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

Running this line of code generates an empty graphic stored in \texttt{p}. This is logical since no layers have been added yet.
So let's imagine we want to look at the overall relationship between \texttt{Sticky} and \texttt{Melting}. To evaluate this relationship, a scatter plot with \texttt{Sticky} in the X-axis and \texttt{Melting} in the Y-axis is created. To do so, two types of information are required:

\begin{itemize}
\tightlist
\item
  the type of visual (here a scatter point);
\item
  the information regarding the data to plot (what should be represented).
\end{itemize}

Such information can be provided as such:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting))}
\end{Highlighting}
\end{Shaded}

This code adds a layer that consists of points (defined by \texttt{geom\_point()}) in which the X-axis coordinates are defined by \texttt{Sticky} and the Y-axis coordinates by \texttt{Melting}, as defined through aesthetics (or \texttt{aes()}). This layer is added to the already existing graph \texttt{p}.

\hypertarget{scatter}{%
\subsubsection{Introduction to Aesthetics}\label{scatter}}

In the previous example, one can notice that many points are being printed. This surprising result is logical since \texttt{sensory} contains the raw sensory data, meaning that there are as many points as there are assessors evaluating products.

Let's color the points per products to see if we can see any patterns. Since the color code is specific to the data (more precisely to \texttt{Product}), it should be informed within the aesthetics by adding \texttt{colour=Product} within \texttt{aes()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting, }\AttributeTok{colour=}\NormalTok{Product))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-117-1.pdf}

As you can see, any parameters provided within \texttt{aes()} may depend on a variable (e.g.~\texttt{colour} in the previous example).
If for any reasons, a specific setting should uniformly be applied to all the elements of the graph, then it should be defined outside \texttt{aes()}.

Let's illustrate this by providing a simple example in which we change the type of the dots from circle to square using \texttt{pch}, and by increasing their size using \texttt{cex}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting, }\AttributeTok{colour=}\NormalTok{Product), }\AttributeTok{pch=}\DecValTok{15}\NormalTok{, }\AttributeTok{cex=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-118-1.pdf}

Regardless of the products, all the points are now shown as large squares.

Depending on the \texttt{geom\_*()} function considered, different parameters should be informed within \texttt{aes()}. Here is a list of the most common \texttt{aes()} you would use:

\begin{itemize}
\tightlist
\item
  \texttt{x}, \texttt{y}, \texttt{z}, provides the coordinates on the X, Y, Z dimensions respectively;
\item
  \texttt{colour}/\texttt{color}, \texttt{fill} controls for the color code\footnote{You can also use \texttt{alpha} to control for the transparency of the elements by defining values between 0 (completely transparent) to 1 (no transparency).} that is being applied to the different elements of a graph;
\item
  \texttt{group} makes the distinction between points that belong to different groups\footnote{Note that \texttt{colour} and \texttt{fill} are specific cases of groups as they additionally provide a visual cue on the groups through the color code};
\item
  \texttt{text}, \texttt{label} prints text/labels on the graph;
\item
  \texttt{size} controls the size of the element (this should preferably be used with numerical variables).
\end{itemize}

It may not be clear yet on how those aesthetics work, but don't worry, many examples illustrating the use of these various types of aesthetics are provided throughout the book.

\hypertarget{linechart}{%
\subsubsection{\texorpdfstring{Introduction to \texttt{geom\_*()} functions}{Introduction to geom\_*() functions}}\label{linechart}}

Since \texttt{\{ggplot2\}} is a multi-layer graph, let's add another layer. For example, the name/code of the panelists associated to each point can be printed.

Such procedure is done through the use of another \texttt{geom\_*()} function in \texttt{geom\_text()}\footnote{Try using \texttt{geom\_label()} instead of \texttt{geom\_text()} to see the difference between these two.} which requires in \texttt{aes()} the position of the labels (\texttt{x} and \texttt{y}) as well as the \texttt{label} itself.

To avoid having the label overlapping with the point, the text is slightly shifted vertically using \texttt{nudge\_y}. For simplicity, let's rebuild the graph from the start:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(sensory)}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting, }\AttributeTok{colour=}\NormalTok{Product))}\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting, }\AttributeTok{label=}\NormalTok{Judge), }\AttributeTok{nudge\_y=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-119-1.pdf}

One interesting remark is that some information required in \texttt{aes()} is being repeated across the different \texttt{geom\_*()} used. Such writing can be simplified by providing the \texttt{aes()} information that applies to all \texttt{geom\_*()} to the original \texttt{ggplot()} call\footnote{Intrinsically, this is what is done with \texttt{sensory} which is only mentioned within \texttt{ggplot()} and is not repeated across the different \texttt{geom\_*()} functions.}. The previous code hence can be simplified to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sensory, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sticky, }\AttributeTok{y=}\NormalTok{Melting, }\AttributeTok{label=}\NormalTok{Judge))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{colour=}\NormalTok{Product))}\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{nudge\_y=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-121-1.pdf}

With this new code, you'll notice that:

\begin{itemize}
\tightlist
\item
  \texttt{x} and \texttt{y} are automatically applied to both \texttt{geom\_point()} and \texttt{geom\_text()};
\item
  although \texttt{label} is only relevant for \texttt{geom\_text()}, it can still be provided at the beginning as it will be ignored by \texttt{geom\_point()};
\item
  \texttt{colour} should only be provided within \texttt{geom\_point()} else the text would also be colored according to \texttt{Product} (which we do not want here);
\item
  \texttt{nudge\_y} is defined outside \texttt{aes()} as it applies to all the text.
\end{itemize}

Since the graphics look at the relationship between two quantitative variables, let's add another layer to the previous graph that shows the regression line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{line\_p }\OtherTok{\textless{}{-}}\NormalTok{ p}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\NormalTok{lm, }\AttributeTok{formula=}\StringTok{"y\textasciitilde{}x"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-123-1.pdf}

This code adds a regression line to the graphic. It is built using the \texttt{lm()} engine in which the simple linear regression model \texttt{y\textasciitilde{}x} is fitted. This result is somewhat surprising since we have not run any regression yet, meaning that \texttt{geom\_smooth()} is performing this analysis in the background by itself.

In practice, most \texttt{geom\_*()} function comes with a statistical procedure attached to it. This means that on the raw data, the \texttt{geom\_*()} function calls its \texttt{stat\_*()} function that runs the corresponding analysis. In the previous example, \texttt{geom\_smooth()} calls \texttt{stat\_smooth()}.

Let's illustrate this concept again using another example: bar-charts that is applied on the data stored in \texttt{Nbiscuits}.
Here, we want to see the distribution (through bar-charts) of the number of biscuits eaten per consumer. A quick look at the data shows that some respondents ate portions of the cookies. To simplify the analysis, let's consider the total number of entire cookies eaten: if a respondent has eaten say 3.5 biscuits, it will be rounded down to 3 full cookies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Nbiscuits }\OtherTok{\textless{}{-}}\NormalTok{ Nbiscuits }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{N =} \FunctionTok{floor}\NormalTok{(N))}
\end{Highlighting}
\end{Shaded}

To create such distribution, a first solution consists in counting for each product how many respondents ate 0 biscuit, 1 biscuit, 2 biscuits, etc. This is automatically done using \texttt{geom\_bar} and \texttt{stat="count"}. The parameter \texttt{position="dodge"} is used to get the results per biscuit side by side rather than stacked up vertically (value by default):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(Nbiscuits, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{N, }\AttributeTok{fill=}\NormalTok{Product))}\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"count"}\NormalTok{, }\AttributeTok{position=}\StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-126-1.pdf}

In the background, this corresponds to grouping the data by \texttt{Product}, summarizing the results by counting \texttt{N}, and then performing \texttt{geom\_bar()} in which no transformation is required (we set \texttt{stat="identity"})\footnote{This code could even be simplified by using \texttt{geom\_col()} which corresponds to \texttt{geom\_bar()} with \texttt{stat="identity"} as default.}:

As can be seen, these two graphics are identical.

\hypertarget{making-graphs-pretty}{%
\subsubsection{Making graphs pretty}\label{making-graphs-pretty}}

In the two previous graphs generated (stored in \texttt{line\_p} and \texttt{bar\_p}), some features can be changed to produce clearer visualizations. Currently, the background is grey with vertical and horizontal white lines, the legend is positioned on the right side, the axis is defined based on the data itself (and so are the axis titles), there is no title, etc. All these points (and many more) can be modified, as it will be shown in this section.

Let's start with a quick win by completely changing the overall appearance of the graphic. To do so, predefined \emph{themes} with pre-set background (with or without lines, axis lines, etc.) can be applied. The two themes we use the most are \texttt{theme\_minimal()} and \texttt{theme\_bw()} (see \url{https://ggplot2.tidyverse.org/reference/ggtheme.html}\href{https://ggplot2.tidyverse.org/reference/ggtheme.html}{link} for a complete list of pre-defined themes.)

Let's start with improving \texttt{bar\_p} using \texttt{theme\_minimal()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\OtherTok{\textless{}{-}}\NormalTok{ bar\_p}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-129-1.pdf}

Rather than using pre-defined themes (or to complement pre-defined themes), the different parameters of the graph can be controlled through \texttt{theme()}.

Let's modify the axes by changing their names and by applying more logical breaks. For instance, the limits of the x-axis can be extended to -1 and 11 to ensure that all the histograms are visible, else R removes some and returns a warning: \texttt{Removed\ 10\ rows\ containing\ missing\ values}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\OtherTok{\textless{}{-}}\NormalTok{ bar\_p }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{"Number of Biscuits eaten"}\NormalTok{, }
                     \AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{), }
                     \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{, }\StringTok{"All of them"}\NormalTok{), }
                     \AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{11}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Number of Respondents"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-131-1.pdf}

Last but not least, a title is being added to the graph using \texttt{ggtitle()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\OtherTok{\textless{}{-}}\NormalTok{ bar\_p }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Distribution of the number of biscuits eaten"}\NormalTok{,}
          \StringTok{"(Results are split per biscuit type)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-133-1.pdf}

Let's apply a similar transformation to \texttt{line\_p}. Here, we are aiming in having a more \emph{realistic} plot using cartesian coordinates, a nice theme, no legend, and a title to the graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{line\_p }\OtherTok{\textless{}{-}}\NormalTok{ line\_p }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{60}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{coord\_fixed}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Relationship between Melting and Sticky"}\NormalTok{,}
          \StringTok{"Stickier biscuits tend to be less melting."}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{colour=}\StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-135-1.pdf}

\hypertarget{common-charts}{%
\subsection{Common Charts}\label{common-charts}}

You have now an overview of the basics of \texttt{\{ggplot2\}} and its philosophy. You'll find plenty of other examples throughout this book to help you develop your skills in building graphics in R.

Since making an exhaustive list of plots that are relevant in sensory and consumer science is out of the scope of this book, it is not going to be further developed here. Yet, here is a summary of the \texttt{geom\_*()} that are commonly used:

\begin{itemize}
\tightlist
\item
  Scatter points:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_point()}: create a scatter point (see example Section \ref{scatter})
  \end{itemize}
\item
  Line charts:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_line()}: create a line that connects points (see example Section \ref{barchart});
  \item
    \texttt{geom\_smooth()}: add a regression line (see \ldots);
  \item
    \texttt{geom\_hline()} (resp. \texttt{geom\_vline()}): add a horizontal (resp. vertical) line using \texttt{yintercept} (resp. \texttt{xintercept});
  \item
    \texttt{geom\_segment()}: draw a segment going from (\texttt{x};\texttt{y}) to (\texttt{xend};\texttt{yend})\footnote{This function can also be used to draw arrows through the parameter \texttt{arrow} and the function of that same name \texttt{arrow()}.}.
  \end{itemize}
\item
  Bar charts:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_col()} and \texttt{geom\_bar()}: produce bar-charts by either using the raw values, or by computing the frequencies first (see example Section \ref{barchart});
  \item
    \texttt{geom\_histogram()} and \texttt{geom\_freqpoly()}: work in a similar way as \texttt{geom\_bar()} except that it divides the x axis into bins before counting the number of observation in each bin and either represent it as bars (\texttt{geom\_histogram}) or lines (\texttt{geom\_freqpoly()}).
  \end{itemize}
\item
  Distribution:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_density()}: build the density plot;
  \item
    \texttt{geom\_boxplot()}: build the well-known boxplot;
  \item
    \texttt{geom\_violin()}: application of \texttt{geom\_density()} displayed in \texttt{geom\_boxplot()} fashion.
  \end{itemize}
\item
  Text and Labels:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_text} and \texttt{geom\_label}: add text to the graph (see example Section \ref{linechart});
  \item
    the package \texttt{\{ggrepel\}} provides alternative functions (\texttt{geom\_text\_repel()} and \texttt{geom\_label\_repel()}) that re-position labels to avoid overlapping (\emph{repel} stands for \emph{repulsive}).
  \end{itemize}
\item
  Rectangles\footnote{In Sensory and Consumer science, this will often be used for building surface plot responses (e.g.~external preference map), and hence is associated to \texttt{geom\_contour()} to show the different lines.}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_tile()}, \texttt{geom\_rect}: create area either using its center point (\texttt{geom\_tile()}) or its four corner (\texttt{geom\_rect()}) defined by \texttt{xmin}, \texttt{xmax}, \texttt{ymin}, and \texttt{ymax} (see example\ldots);
  \item
    \texttt{geom\_raster()}: high performance alternative to \texttt{geom\_tile()}/\texttt{geom\_rect} where all the tiles have the same size.
  \end{itemize}
\end{itemize}

Besides \texttt{geom*()}, a lot of graphical parameters can further be controlled. This includes of course the \texttt{theme()} and the \texttt{aes()}:

\begin{itemize}
\tightlist
\item
  For pre-defined themes, see example;
\item
  \texttt{axis} parameters including its title (\texttt{axis.title}), text (\texttt{axis.text}), ticks (\texttt{axis.ticks}), line (\texttt{axis.line}), and all their sub-levels.
\item
  \texttt{legend} parameters including its position (\texttt{legend.position}), direction (\texttt{legend.direction}), its text (\texttt{legend.text}, \texttt{legend.title}), the design of the box (\texttt{legend.box}, \texttt{legend.background}) etc.
\item
  \texttt{panel} parameters including its background (\texttt{panel.background}), the grid lines (\texttt{panel.grid}), the border (\texttt{panel.border}), etc.
\item
  \texttt{plot} parameters including the different titles (\texttt{plot.title}, \texttt{plot.subtitle}, \texttt{plot.caption}), the background (\texttt{plot.backgorund}), etc.
\end{itemize}

Most of these parameters can be controlled at different levels of granularity:

\begin{itemize}
\tightlist
\item
  overall, e.g.~\texttt{panel.grid};
\item
  more detailed, e.g.~\texttt{panel.grid.major} and \texttt{panel.grid.minor};
\item
  most detailed, e.g.~\texttt{panel.grid.major.x}, \texttt{panel.grid.major.y}, etc.
\end{itemize}

Depending whether the option to modify is some text, a line, or a rectangle, \texttt{element\_text()}, \texttt{element\_line()}, or \texttt{element\_rect()} would be respectively used to control them. These functions provide general (e.g.~\texttt{color}) as well as specific options (e.g.~\texttt{family} and \texttt{face} for text, \texttt{linetype} for lines etc.) to each type.

Note that if some elements should be left blank, \texttt{element\_blank()} can be used regardless of the nature of the element.

Let's illustrate these concepts using our previous graph stored in \texttt{line\_p}. Here, the goal is to remove the grid line, to replace the x and y axis lines by arrows, and to re-position the axis titles to the far end of the axis so that it is next to the arrow head.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{line\_p }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{panel.grid=}\FunctionTok{element\_blank}\NormalTok{(), }
        \AttributeTok{panel.border=}\FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.line=}\FunctionTok{element\_line}\NormalTok{(}\AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{ends =} \StringTok{"last"}\NormalTok{, }
                                             \AttributeTok{type =} \StringTok{"closed"}\NormalTok{)),}
        \AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: The following aesthetics were dropped during statistical transformation: label
## i This can happen when ggplot fails to infer the correct grouping structure in the data.
## i Did you forget to specify a `group` aesthetic or to convert a numerical variable into a factor?
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-136-1.pdf}

Similarly to the theme, aesthetics can also be adjusted. In previous examples, the x-axis in \texttt{bar\_p} was adjusted by setting limits, providing breaks and replacing the values by certain labels using \texttt{scale\_x\_continuous()}.
Most aesthetics parameters can be controlled by equivalent functions for which the name is using the following structure \texttt{scale\_*nameaes*\_*typescale*}, and where:

\begin{itemize}
\tightlist
\item
  \emph{nameaes} corresponds to any aesthetics including \texttt{x}, \texttt{y}, \texttt{colour} or \texttt{fill}, \texttt{alpha}, etc.
\item
  \emph{typescale} corresponds to the type of scale, where it is \texttt{continuous}, \texttt{discrete}, or \texttt{manual} (amongst others).
\end{itemize}

Such functions fully control how the corresponding aesthetic should behave, by providing the correspondence between a variable level and (say) its color.
In the graph saved in \texttt{bar\_p}, remember that we filled in the bar chart using the product information. Let's imagine that we are particularly interested in biscuit \texttt{P3}, and want to compare it to the rest of the biscuits. We propose to make \texttt{P3} stand out by filling it in orange, and by setting all the other biscuits in the same gray tone.

Such procedure can be done using \texttt{scale\_fill\_manual()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"P1"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }\StringTok{"P2"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }
                             \StringTok{"P3"}\OtherTok{=}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"P4"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }
                             \StringTok{"P5"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }\StringTok{"P6"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{,}
                             \StringTok{"P7"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }\StringTok{"P8"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{,}
                             \StringTok{"P9"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{, }\StringTok{"P10"}\OtherTok{=}\StringTok{"gray50"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-137-1.pdf}

When multiple aesthetics are being used, the legend might become overwhelming or redundant. It is possible to turn off some of these visuals within the \texttt{scale\_*()} functions, or by using \texttt{guides()} and by setting \texttt{nameaes=\textquotesingle{}none\textquotesingle{}} as shown in the \texttt{line\_p} example.

\hypertarget{miscealleneous}{%
\subsection{Miscealleneous}\label{miscealleneous}}

\hypertarget{structuring-the-axis}{%
\subsubsection{Structuring the axis}\label{structuring-the-axis}}

By default, \texttt{ggplot()} generates plot that fits the data and that fits within the output screen. This means that some graphics might not be perfectly representing the data due to some distortion. In a previous example (\texttt{line\_p}), the dimensions were made comparable through \texttt{coord\_fixed()}.

Other transformation can be performed. For instance, the graphic can be transposed using \texttt{coord\_flip()} as in the following example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_p }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-138-1.pdf}

\hypertarget{summary-through-an-example-spider-plots}{%
\subsubsection{Summary through an example: Spider Plots}\label{summary-through-an-example-spider-plots}}

To conclude this section, and summarize most concepts presented in this chapter, let's introduce the well-known spider plots. The use of such plots are quite polarizing amongst analysts and the reason of this choice here is purely educational, as 1. there are no pre-defined options in \texttt{\{ggplot2\}} that provides such charts, and 2. they present some interesting challenges.

Let's start with deconstructing a spider-plot: a spider plot is a line chart presented in a circular way. So let's start with building a line chart of our sensory profiles (the means are considered here). For more clarity, only two of the samples (\texttt{P03} and \texttt{POpt}) are represented.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_mean }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to=}\StringTok{"Variables"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Variables =} \FunctionTok{fct\_inorder}\NormalTok{(Variables)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product, Variables) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Mean =} \FunctionTok{mean}\NormalTok{(Scores)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Product }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"P03"}\NormalTok{, }\StringTok{"POpt"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Product'. You can
## override using the `.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider\_line }\OtherTok{\textless{}{-}}\NormalTok{ sensory\_mean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Variables, }\AttributeTok{y=}\NormalTok{Mean, }\AttributeTok{colour=}\NormalTok{Product, }\AttributeTok{linetype=}\NormalTok{Product))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch=}\DecValTok{20}\NormalTok{, }\AttributeTok{cex=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group=}\NormalTok{Product), }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{""}\NormalTok{, }\AttributeTok{labels=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"P03"}\OtherTok{=}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"POpt"}\OtherTok{=}\StringTok{"grey50"}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_linetype\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"P03"}\OtherTok{=}\StringTok{"solid"}\NormalTok{, }\StringTok{"POpt"}\OtherTok{=}\StringTok{"dashed"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## i Please use `linewidth` instead.
\end{verbatim}

Next step is to represent this line chart in a circular way. This can be done using \texttt{coord\_polar()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider\_line }\SpecialCharTok{+} \FunctionTok{coord\_polar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-140-1.pdf}

This already looks like a spider plot! However, a closer look at it highlights a point that needs improvement: There is no connection between the last attribute (\texttt{Melting}) and the first one (\texttt{Shiny}).

To counter this, the following twofold solution is proposed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Associate each attribute to its position (e.g.~\texttt{Shiny} is 1, \texttt{External\ color\ intensity} is 2, until \texttt{Melting} which would be 32);
\item
  Duplicate the last attribute (\texttt{Melting}) and associate it to position 0.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var }\OtherTok{\textless{}{-}} \FunctionTok{levels}\NormalTok{(sensory\_mean}\SpecialCharTok{$}\NormalTok{Variables)}
\NormalTok{sensory\_mean\_pos }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Variables =} \FunctionTok{c}\NormalTok{(var[}\FunctionTok{length}\NormalTok{(var)], var),}
                  \AttributeTok{Position =} \DecValTok{0}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(var)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{full\_join}\NormalTok{(sensory\_mean, var\_pos, }\AttributeTok{by=}\StringTok{"Variables"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The previous graph is then rebuilt by forcing the position of the attributes on the x-axis using \texttt{Position} (\texttt{Variables} is used for the labels). Here position 0 is voluntarily omitted (\texttt{breaks\ =\ 1:length(var)} and \texttt{labels\ =\ var}), meaning that only the labels going from 1 to the last variable are being showed. However, the x-axis is forced to go from 0 to the number of attributes (\texttt{limits\ =\ c(0,\ length(var))}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider\_plot }\OtherTok{\textless{}{-}}\NormalTok{ sensory\_mean\_pos }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Position, }\AttributeTok{y=}\NormalTok{Mean, }\AttributeTok{colour=}\NormalTok{Product, }\AttributeTok{linetype=}\NormalTok{Product))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch=}\DecValTok{20}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group=}\NormalTok{Product), }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{""}\NormalTok{, }\AttributeTok{breaks=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(var), }
                     \AttributeTok{labels=}\NormalTok{var, }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{length}\NormalTok{(var)))}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name=}\StringTok{""}\NormalTok{, }\AttributeTok{labels=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"P03"}\OtherTok{=}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"POpt"}\OtherTok{=}\StringTok{"grey50"}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{scale\_linetype\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"P03"}\OtherTok{=}\StringTok{"solid"}\NormalTok{, }\StringTok{"POpt"}\OtherTok{=}\StringTok{"dashed"}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{coord\_polar}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{, }\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-143-1.pdf}

By using this trick, the connection between the first and last attributes is established.

\hypertarget{combining-plots}{%
\subsubsection{Combining plots}\label{combining-plots}}

When multiple plots should be generated using the same pattern on subset of data, it is possible to generate them automatically using \texttt{facet\_wrap()} or \texttt{facet\_grid()}. The difference between these two functions rely in the number of variables to use for the split: In \texttt{facet\_wrap()}, the graphics are \emph{vectorized}, meaning that each element of the split is represented independently. For \texttt{facet\_grid()} however, the graphics is represented in a matrix, meaning that two blocks of split variables are required, one for the columns and one for the rows.

An example of \texttt{facet\_wrap()} is provided in Section (REF DATA ANALYSIS).

For these two functions, the parameter \texttt{scales} is particularly interesting as it allows each separate graph to use its own axis scales (\texttt{free} or individually using \texttt{free\_x}/\texttt{free\_y}) or not (\texttt{fixed}).

To go further, consider also the function \texttt{facet\_trelliscope()} from the \texttt{\{trelliscopejs\}} package. This function generates the same type of graphs as \texttt{facet\_wrap()} or \texttt{facet\_grid()} with some powerful twists. After generating the plots, they are still editable thanks to an interactive menu that controls for the \emph{Grid}, \emph{Labels}, \emph{Filter}, and \emph{Sort}. For example, the number of plots to show per row/column can be adjusted, tables with descriptive statistics (e.g.~mean, minimum, maximum) can be added under each graph, etc. Moreover, readers that are familiar with the interactivity of \texttt{\{plotly\}} can make great use of it through the \texttt{as\_plotly\ =\ TRUE} parameter, which is then applied to each individual graph!

Such procedure is very handy to produce multiple graphs all at once\ldots when the data allow it.
When multiple plot are being generated separately (using different data set, or producing different types of plots), it can still be relevant to combine them all in one. To perform such collage, the package \texttt{\{patchwork\}} becomes handy.

\texttt{\{patchwork\}} is a package that allows combining \texttt{ggplot()} graphs using \emph{mathematical} operations. To add two elements next to each others, the \texttt{+} sign is used. To add two elements on top of each others, they should be separated using \texttt{/}. This operation can be combined with \texttt{()} to generate fancier collage.

Let's illustrate this by creating a plot with on the left side \texttt{spider\_plot}, and on the right side \texttt{bar\_p} on top of \texttt{line\_p}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ spider\_plot }\SpecialCharTok{+}\NormalTok{ (bar\_p }\SpecialCharTok{/}\NormalTok{ line\_p)}
\end{Highlighting}
\end{Shaded}

A general title can be added, as well as tag levels (handy for publications!) using \texttt{plot\_annotation()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{plot\_annotation}\NormalTok{(}\AttributeTok{title =} \StringTok{"Example of \textquotesingle{}ggplots\textquotesingle{} I\textquotesingle{}ve learned today"}\NormalTok{, }
                    \AttributeTok{tag\_levels=}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-145-1.pdf}

\hypertarget{tipsntricks}{%
\subsection{Few Additional Tips and Tricks}\label{tipsntricks}}

\hypertarget{combining-data-transformation-and-ggplot2-grammar}{%
\subsubsection{\texorpdfstring{Combining data transformation and \texttt{\{ggplot2\}} grammar}{Combining data transformation and \{ggplot2\} grammar}}\label{combining-data-transformation-and-ggplot2-grammar}}

Both the \texttt{\{tidyverse\}} and \texttt{\{ggplot2\}} are using pipes to combine lines of code or layers.
However, the pipes themselves are defined differently since \texttt{\{maggritr\}} uses \texttt{\%\textgreater{}\%} whereas \texttt{\{ggplot2\}} uses \texttt{+}.
It is however possible to combine both systems one after each other, just remember to switch from \texttt{\%\textgreater{}\%} to \texttt{+} as you transition from data transformation/tidying to building your graph (see Section REF).

\hypertarget{ordering-elements-in-a-plot}{%
\subsubsection{Ordering elements in a plot}\label{ordering-elements-in-a-plot}}

When building a graph using a categorical variables, \texttt{\{ggplot2\}} tends to represent the different levels in alphabetical order, especially if the variable is defined as character. Such situation can make the graph more difficult to read, as the categories may not be presented in a logical order (e.g.~fall, spring, summer, winter instead of spring, summer, fall, winter). To ensure that the elements are in the right order, either consider transforming the variables into factor (using \texttt{factor()} by indicating the levels order of your choice, or through \texttt{fct\_inorder()} to keep the order from the file, see Section @ref(pivot\_longer)) or by using a position variable as in the \texttt{spider\_plot} example. The former option also works for ordering elements in the legend.

If the order of the elements should be changed \emph{within the charts} (simple changes such as reverting the order), it can be done directly within the \texttt{geom\_*()} function. This is for instance the case with stacked bar chart, in which the order may be reverted using the parameter \texttt{position\ =\ position\_fill(reverse\ =\ TRUE)} for instance (suggesting here that the split was defined through \texttt{fill} in \texttt{aes()}).

\hypertarget{fixing-overlapping-axis-text}{%
\subsubsection{Fixing overlapping axis text}\label{fixing-overlapping-axis-text}}

When \texttt{ggplot()} are being built using categorical variables, the labels used on the x-axis are often overlapping (some of the labels being then unreadable). A first good/easy solution consists in reducing the size of the label, and/or to shortening them as long as it does not affect its readability. However, this might not always be possible or sufficient, and other adjustments are required. Let's use \texttt{spider\_line} as illustration to show three possible solutions.

The first option consists in using \texttt{theme()} and rotating the labels (here at 45 degrees, but use 90 degrees to get the names vertically). Note that by default, \texttt{ggplot()} center the labels: to avoid having them crossing the x-axis line, they are being left-centered using \texttt{hjust=1}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider\_line }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle=}\DecValTok{45}\NormalTok{, }\AttributeTok{hjust=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-146-1.pdf}

A second option consists in dodging one every two labels along the X-axis. This option works fine, especially when the labels are not too long. In our example unfortunately, some overlap can still be seen. Note that this option is accessible within \texttt{scale\_x\_discrete()}, not within \texttt{theme()} as we would expect:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider\_line }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{guide =} \FunctionTok{guide\_axis}\NormalTok{(}\AttributeTok{n.dodge =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-147-1.pdf}

Last option consists in transposing the graph using \texttt{coord\_flip()}. This solution works well since labels on the y-axis are written horizontally. However, this option is not always suitable due to conventions: if it is recommended for bar charts, it may not be for line charts for instance.

\hypertarget{exporting-graphs}{%
\subsubsection{Exporting graphs}\label{exporting-graphs}}

There are various ways to save or export \texttt{ggplot()} charts. To save these plots to your computer in various formats (e.g.~png, pdf, etc.), \texttt{ggsave()} is used. By default, \texttt{ggsave()} exports the last plot built and saves it in the location defined by \texttt{filename}, in the format defined by \texttt{device} (additional information regarding the width, height, dpi etc. can also be configured).

For instance, the \texttt{spiderplot} generated earlier\footnote{Since \texttt{spiderplot} is not the last plot generated, it needs to be defined in \texttt{plot}.} can be saved as following:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\AttributeTok{filename=}\StringTok{"spiderplot.png"}\NormalTok{, }\AttributeTok{plot=}\NormalTok{spiderplot, }\AttributeTok{device=}\StringTok{"png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As an alternative, \texttt{ggplot()} graphs can also be exported in PowerPoint or Word through the \texttt{\{rvg\}} package (see Section @ref(export\_plots)).

\hypertarget{additional-libraries}{%
\subsubsection{Additional libraries}\label{additional-libraries}}

\texttt{\{ggplot2\}} is a very powerful tool for data visualization. By default, it offers a very large variety of possibilities, which should cover most situations that you would encounter. If not, a quick internet search will most likely find extensions in alternative packages that wil provide you with solutions.

To help you further, here is a non-exhaustive list of relevant packages:

\begin{itemize}
\tightlist
\item
  \texttt{\{ggcharts\}}: This package provides nice and clear alternatives to some \texttt{\{ggplot2\}} options through simple functions in one line of code, including \texttt{bar\_chart()}, \texttt{line\_chart()}, \texttt{lollipop\_chart()} and \texttt{dumbbell\_chart()} just to name a few.
\item
  \texttt{\{graffify\}}: This package extends \texttt{\{ggplot2\}} by providing nice and easy functions to help data visualization and linear models for ANOVA. For example, it generates through one function bar chart with error bars through \texttt{plot\_scatterbar\_sd()}, or simultaneous box-plot and scatter plot through \texttt{plot\_scatterbox()}.
\item
  \texttt{\{factoextra\}}: Although \texttt{\{FactoMineR\}} generates its graphs in \texttt{\{ggplot2\}} and in base R, \texttt{\{factoextra\}} is a great extension as it is easy to use and provides a wide variety of options to customize your plots.
\item
  \texttt{\{ggcorrplot\}}: There are many packages that propose to visualize graphically tables of correlations, however we particularly like this one for its simplicity.
\item
  \texttt{\{ggwordcloud\}}: It is a great package for building word-clouds as it provides a large degree of control. With this package, the words can either be positioned randomly, by matching a pre-defined shape, etc. But more interestingly, words can also be positioned semi-randomly, hence giving more interpretation power to the final results (for more information, please visit \url{https://lepennec.github.io/ggwordcloud/}\href{https://lepennec.github.io/ggwordcloud/}{link}).
\item
  \texttt{\{ggraph\}}: This package provides neat solutions to build network visualization in \texttt{\{ggplot2\}}.
\item
  \texttt{\{performance\}}: This package provides pre-defined graphs that allow you evaluating the quality of your models through the single \texttt{check\_model()} function. See also \texttt{\{ggside\}} if you want to print on the margin of your regression plot the marginal distributions (or density plot) of your different categories.
\end{itemize}

To learn more about \texttt{\{ggplot2\}} basics, we recommend two additional source of information:

\begin{itemize}
\tightlist
\item
  \texttt{\{esquisse\}}: After loading the package, run the function \texttt{esquisser()}. This command opens a window in which you can select your data set (the data set should be available within you R environment), the type of plot to build, and all the relevant information to build your plot (which variable to be used as X-axis, Y-axis, etc.) through an user-friendly interface. Ultimately, the graph is being generated, but more importantly, the code used to generate the plot is provided. This is hence an educational tool to learn build graphs with \texttt{\{ggplot2\}}.
\item
  \url{https://www.data-to-viz.com/}\href{https://www.data-to-viz.com/}{link} provides a wide gallery of graphics sorted by the type of data that you have. Each graphic proposed is illustrated with an example provided in R (often in \texttt{\{ggplot2\}}) and in Python. This website is hence inspirational and educational both at the same time!
\end{itemize}

\hypertarget{auto-report}{%
\chapter{Automated Reporting}\label{auto-report}}

\begin{quote}
Learning a programming language is not only useful for the freedom it provides, it also increases largely the speed of the analysis thanks to the re-usability of the code. When many projects are built using the same base (e.g.~similar questionnaires), the same analyses are often performed, also leading to similar reports. Automating the analysis process is simple by applying the same code to different dataset. But what about the reports? Should a new report be built manually for each project?
In this section, we will show you how to build your own report directly from your code next to your analyses (or integrate your analysis within your reporting process). Such procedure has three main benefits, 1) build a new standardized report automatically for each new dataset analyzed, 2) mass-produce slides automatically, and 3) save you time for the interpretation of the results and the story-telling.
\end{quote}

\hypertarget{what-and-why-automated-reporting}{%
\section{What and why Automated Reporting?}\label{what-and-why-automated-reporting}}

Effective communication of results is among the essential duties of sensory scientists\ldots and so is the data collection, data preparation, data analysis etc. Worst, the sometimes tedious mechanics of report production together with the sheer volume of data that many scientists must process combine to make reporting design and nice story-telling an afterthought in too many cases.
Although this should preferably not be happening, it is necessary sometimes as presentation deadlines approach and time becomes limited.
Add to this work-load some last-minute changes due to either a change in the data, in the analysis, or maybe an error (e.g.~copy/paste the wrong column, etc.) you have just been detected\ldots how can we be sure that the latest version of the report is fully up-to-date, and that all the results/conclusions are correct? As the statistical software (e.g.~R) is often separated from the reporting tool (e.g.~Microsoft Office), it is easy to miss to transfer updated statistical outputs (e.g.~values, tables, figures, etc.) to the report, hence creating inconsistencies.

Let's consider another scenario, in which all the preliminary steps have been successfully done. After a week of analyses and reporting, you present your report to your manager or clients, and they come with a question such as: ``Can we deep-dive into the results by looking at a particular group of consumers (e.g.~gender split, or cluster split)?'' Do you feel like going through the entire process again?

How would you feel if we would tell you that there is a way to build your report while running the analysis, by using the same script file? This means that in few clicks, say after updating the data to analyze (e.g.~filter to the target group of consumers only), your report gets automatically re-generated with all the new updated results. Such solution seems ideal since it increases efficiency while reducing errors due to manual processing of results. More importantly, this gain in time and effort allow you designing nicer slides and building a better story.

\hypertarget{integrating-reports-within-analyses-scripts}{%
\section{Integrating reports within analyses scripts}\label{integrating-reports-within-analyses-scripts}}

In this section, let's integrate our report building process within our data analysis. By doing so, we do not focus on building a story yet. Instead, we improve our way of working by exporting directly all the statistical outputs that could\footnote{We say \emph{could} as we are in a process of mass-exportation of results, most of them being used for building the story although they may not be kept in the final deck.} be useful for our future story-telling. By doing so, we increase efficiency (especially if code can then be re-used for other studies) by \emph{killing two birds with one stone}: We simultaneously run our analysis, create usable content for our final presentation, while reducing errors due to manual processing.

Since Microsoft Office is often the tool used for sharing results, we will focus our attention in exporting results to Excel, PowerPoint, and Word.

As usual, let's start by loading the general package that we would need for our analyses (more specific packages being mentioned later.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

Note that in this chapter on automated reporting, some results (tables, figures) that are being created in one of the section may be re-used in subsequent sections. In case you do not read this chapter linearly, you might get errors as you might be calling objects that do not exist yet in your environment. If that should be the case, read through the previous sections to find the code where these elements are being generated, run it, and resume your read.

\hypertarget{excel}{%
\subsection{Excel}\label{excel}}

Although Excel is not our preferred tool for automated reporting, it is still one of the major ways to access and share data. Most data collection software offer the possibility to export data and results in Excel, while most data analysis software accept Excel format as inputs. With the large use of Excel, it is no surprise that many colleagues and/or clients like to share data and results using spreadsheets. It is even less a surprise that R provides multiple solutions to import/export results from/to Excel.

For importing Excel files, we have already presented the package \texttt{\{readxl\}} among others (see @ref(simple\_manips)). For exporting results, two complementary packages (yet again, among others!) in terms of ease of use and flexibility in the outcome are proposed: \texttt{\{writexl\}} and \texttt{\{openxlsx\}}.

As its name suggests, \texttt{\{writexl\}} is dedicated to exporting tables to Excel through the \texttt{write\_xlsx()} function. Its use is very simple as it only takes as inputs the table (or list of tables)\footnote{List of tables will generate multiple sheets within the same spreadsheet, one table being placed in each sheet.} to export to the file specified in the \texttt{path} parameter.

Let's illustrate this by using our \emph{biscuits\_sensory\_profile.xlsx} file: Let's imagine that we would like to reduce our data set by only considering products that are high in Protein:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{file.path}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{)}
\NormalTok{product\_info }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(}\AttributeTok{path=}\NormalTok{file\_path, }\AttributeTok{sheet=}\StringTok{"Product Info"}\NormalTok{, }
                                  \AttributeTok{range=}\StringTok{"A1:D12"}\NormalTok{, }\AttributeTok{col\_names=}\ConstantTok{TRUE}\NormalTok{)}

\NormalTok{high\_prot }\OtherTok{\textless{}{-}}\NormalTok{ product\_info }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Protein }\SpecialCharTok{\%in\%} \StringTok{"High"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(Product)}

\NormalTok{high\_prot\_data }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(}\AttributeTok{path=}\NormalTok{file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Product }\SpecialCharTok{\%in\%}\NormalTok{ high\_prot)}
\end{Highlighting}
\end{Shaded}

We then export this data into an excel sheet called \emph{export.xlsx} that will be contained in our folder called \emph{output}\footnote{If the \emph{output} folder does not exist, this code will return an error so make sure to create one.}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(writexl)}
\FunctionTok{write\_xlsx}\NormalTok{(high\_prot\_data, }\AttributeTok{path=}\StringTok{"output/export.xlsx"}\NormalTok{, }\AttributeTok{col\_names=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The export of tables using \texttt{\{writexl\}} is intuitive and easy, yet simplistic as it does not allow formatting the tables (except for some minor possibilities for the header), nor does it allow exporting multiple tables within the same sheet. For more advanced exporting options, the use of \texttt{\{openxlsx\}} is preferred as it allows more flexibility in structuring and formatting the Excel output.

With \texttt{\{openxlsx\}}, the procedure starts with creating a workbook object (e.g.~\texttt{wb}) using \texttt{createWorkbook()}. We can add worksheets to \texttt{wb} through \texttt{addWorksheet()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(openxlsx)}
\NormalTok{wb }\OtherTok{\textless{}{-}}\NormalTok{ openxlsx}\SpecialCharTok{::}\FunctionTok{createWorkbook}\NormalTok{()}
\FunctionTok{addWorksheet}\NormalTok{(wb, }\AttributeTok{sheetName =} \StringTok{"Mean"}\NormalTok{, }\AttributeTok{gridLines =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that with \texttt{addWorksheet()}, it is possible to control the appearance of the worksheet:

\begin{itemize}
\tightlist
\item
  show/hide grid lines using \texttt{gridLines};
\item
  color the sheet using \texttt{tabColour};
\item
  change the zoom on the sheet through \texttt{zoom};
\item
  show/hide the tab using \texttt{visible};
\item
  format the worksheet by specifying its size (\texttt{paperSize}) and orientation (\texttt{orientation}).
\end{itemize}

On a given worksheet, any table can be exported using \texttt{writeData()} or \texttt{writeDataTable()}, which controls where to write the table through the \texttt{startRow} and \texttt{startCol} options.

Let's imagine we want to compute the sensory profiles of the products, and we want to export that into Excel. Rather then simply exporting the results, we want to customize the output a little bit by applying the Excel style named \emph{TabelStyleLight9}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating the Sensory Profiles with some Product Information}
\NormalTok{p\_info }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Product Info"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Type)}
\NormalTok{sensory }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(p\_info, }\AttributeTok{by=}\StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{relocate}\NormalTok{(Protein}\SpecialCharTok{:}\NormalTok{Fiber, }\AttributeTok{.after=}\NormalTok{Product)}
\NormalTok{senso\_mean }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to=}\StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Attribute, }\AttributeTok{values\_from=}\NormalTok{Score, }
              \AttributeTok{values\_fn=}\NormalTok{mean)}

\CommentTok{\# Add the table to the Excel Sheet}
\FunctionTok{writeDataTable}\NormalTok{(wb, }\AttributeTok{sheet=}\StringTok{"Mean"}\NormalTok{, }\AttributeTok{x=}\NormalTok{senso\_mean, }
               \AttributeTok{startCol=}\DecValTok{1}\NormalTok{, }\AttributeTok{startRow=}\DecValTok{1}\NormalTok{, }
               \AttributeTok{colNames=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rowNames=}\ConstantTok{FALSE}\NormalTok{, }
               \AttributeTok{tableStyle=}\StringTok{"TableStyleLight9"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At any time, you can visualize the Excel file that is being produced without exporting it yet using \texttt{openXL()}. This function comes very handy as it allows you checking that the output looks like what you would wish for.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{openXL}\NormalTok{(wb)}
\end{Highlighting}
\end{Shaded}

As can be seen, \texttt{writeData()} and \texttt{writeDataTable()} give us a lot of control on our export. For instance, we can:

\begin{itemize}
\tightlist
\item
  control where to print the data by using \texttt{startRow} and \texttt{startCol} (or alternatively \texttt{xy}: \texttt{xy\ =\ c("B",12)} prints the table starting in cell B12), hence allowing exporting multiple tables within the same sheet;
\item
  include the row names and column names through \texttt{rowNames} and \texttt{colNames};
\item
  format the header using \texttt{headerStyle} (incl.~color of the text and/or background, font, font size, etc.);
\item
  apply a specific style to our table using \texttt{tableStyle};
\item
  shape the borders using predefined solutions through \texttt{borders}, or customizing them with \texttt{borderStyle} and \texttt{borderColour};
\item
  add a filter to the table using \texttt{withFilter};
\item
  convert missing data to ``\#N/A'' or any other string using \texttt{keepNA} and \texttt{na.string}.
\end{itemize}

Rather than using some pre-defined formatting as was the case with \texttt{tableStyle}, let's consider some more advanced options in which we control (almost) everything. Let's start with setting up the formatting style we would like to apply:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pre{-}define options to control the borders }
\FunctionTok{options}\NormalTok{(}\StringTok{"openxlsx.borderColour"} \OtherTok{=} \StringTok{"\#4F80BD"}\NormalTok{)}
\FunctionTok{options}\NormalTok{(}\StringTok{"openxlsx.borderStyle"} \OtherTok{=} \StringTok{"thin"}\NormalTok{)}

\CommentTok{\# Automatically set Number formats to 1 value after the decimal}
\FunctionTok{options}\NormalTok{(}\StringTok{"openxlsx.numFmt"} \OtherTok{=} \StringTok{"0.0"}\NormalTok{)}

\CommentTok{\# Change the font to Calibri size 10}
\FunctionTok{modifyBaseFont}\NormalTok{(wb,}\AttributeTok{fontName =} \StringTok{"Calibri"}\NormalTok{, }\AttributeTok{fontSize =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Header Style (blue background, top/bottom borders, text centered/bold)}
\NormalTok{headSty }\OtherTok{\textless{}{-}} \FunctionTok{createStyle}\NormalTok{(}\AttributeTok{fgFill=}\StringTok{"\#DCE6F1"}\NormalTok{, }\AttributeTok{border=}\StringTok{"TopBottom"}\NormalTok{, }
                       \AttributeTok{halign=}\StringTok{"center"}\NormalTok{, }\AttributeTok{textDecoration=}\StringTok{"bold"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that many more formatting options can be configured through:

\begin{itemize}
\tightlist
\item
  \texttt{options()} to pre-define number formatting, border colors and style, etc.;
\item
  \texttt{modifyBaseFont()} to define the font name and font size;
\item
  \texttt{freezePane()} to freeze the first row and/or column of the table;
\item
  \texttt{createStyle()} to pre-define a style, or \texttt{addStyle()} to apply the styling to selected cells;
\item
  \texttt{setColWidths()} to control column width;
\item
  \texttt{conditionalFormatting()} to format cells based on pre-defined conditions (see next for an example).
\end{itemize}

Let's export again the sensory profiles in a second sheet after applying these formatting options:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addWorksheet}\NormalTok{(wb, }\AttributeTok{sheetName =} \StringTok{"Mean (manual formatting)"}\NormalTok{, }\AttributeTok{gridLines =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{freezePane}\NormalTok{(wb, }\AttributeTok{sheet=}\DecValTok{2}\NormalTok{, }\AttributeTok{firstRow=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{firstCol=}\ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{writeData}\NormalTok{(wb, }\AttributeTok{sheet=}\DecValTok{2}\NormalTok{, }\AttributeTok{x=}\NormalTok{senso\_mean, }
          \AttributeTok{startCol=}\DecValTok{1}\NormalTok{, }\AttributeTok{startRow=}\DecValTok{1}\NormalTok{, }
          \AttributeTok{colNames=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rowNames=}\ConstantTok{FALSE}\NormalTok{, }
          \AttributeTok{headerStyle=}\NormalTok{headSty)}
\end{Highlighting}
\end{Shaded}

You'll notice that the same table is now presented in a different way (use \texttt{openXL(wb)} to view it).

Let's now consider a third export of the sensory profiles, with an additional twist: for a given variable (i.e.~column), the value is colored in red (resp. blue) if it is higher (resp. lower) than its mean. To do so, we need to use conditional formatting.

Let's start with creating two pre-defined parameters called \texttt{pos\_style} (red) and \texttt{neg\_style} (blue) using \texttt{createStyle()}that we will use to color the different cells. Let's also compute the overall mean per attribute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Styles for conditional formatting}
\NormalTok{pos\_style }\OtherTok{\textless{}{-}} \FunctionTok{createStyle}\NormalTok{(}\AttributeTok{fontColour =} \StringTok{"firebrick3"}\NormalTok{, }
                         \AttributeTok{bgFill =} \StringTok{"mistyrose1"}\NormalTok{)}
\NormalTok{neg\_style }\OtherTok{\textless{}{-}} \FunctionTok{createStyle}\NormalTok{(}\AttributeTok{fontColour =} \StringTok{"navy"}\NormalTok{, }
                         \AttributeTok{bgFill =} \StringTok{"lightsteelblue"}\NormalTok{)}

\CommentTok{\# Compute the overall mean}
\NormalTok{overall\_mean }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), mean))}
\end{Highlighting}
\end{Shaded}

Let's then create a new worksheet in which we print the data of interest:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addWorksheet}\NormalTok{(wb, }\AttributeTok{sheetName =} \StringTok{"Conditional Formatting"}\NormalTok{, }\AttributeTok{gridLines=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{writeDataTable}\NormalTok{(wb, }\AttributeTok{sheet=}\DecValTok{3}\NormalTok{, }\AttributeTok{x=}\NormalTok{senso\_mean, }
               \AttributeTok{startCol=}\DecValTok{1}\NormalTok{, }\AttributeTok{startRow=}\DecValTok{1}\NormalTok{, }\AttributeTok{colNames=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rowNames=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we color the cells according to the rules that was defined earlier. To do so, the decision whether \texttt{pos\_style} or \texttt{neg\_style} should be used is defined by the \texttt{rule} parameter from the \texttt{conditionalFormatting()}\footnote{In \texttt{conditionalFormatting()}, you can specify to which \texttt{rows} and \texttt{cols} the formatting applies.} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adding formatting to the second column}
\ControlFlowTok{for}\NormalTok{ (v }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(overall\_mean))\{}
  \FunctionTok{conditionalFormatting}\NormalTok{(wb, }\AttributeTok{sheet=}\DecValTok{3}\NormalTok{, }\AttributeTok{cols=}\NormalTok{v}\SpecialCharTok{+}\DecValTok{3}\NormalTok{, }
                        \AttributeTok{rows=}\DecValTok{1}\SpecialCharTok{+}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(senso\_mean), }
                        \AttributeTok{rule=}\FunctionTok{paste0}\NormalTok{(}\StringTok{"\textgreater{}"}\NormalTok{, overall\_mean[}\DecValTok{1}\NormalTok{,v]), }
                        \AttributeTok{style=}\NormalTok{pos\_style)}
  \FunctionTok{conditionalFormatting}\NormalTok{(wb, }\AttributeTok{sheet=}\DecValTok{3}\NormalTok{, }\AttributeTok{cols=}\NormalTok{v}\SpecialCharTok{+}\DecValTok{3}\NormalTok{, }
                        \AttributeTok{rows=}\DecValTok{1}\SpecialCharTok{+}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(senso\_mean), }
                        \AttributeTok{rule=}\FunctionTok{paste0}\NormalTok{(}\StringTok{"\textless{}"}\NormalTok{, overall\_mean[}\DecValTok{1}\NormalTok{,v]), }
                        \AttributeTok{style=}\NormalTok{neg\_style)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Few comments regarding this code:

\begin{itemize}
\tightlist
\item
  We want to run this for each sensory attribute, hence the \texttt{for} loop that goes from 1 to the number of columns stored in \texttt{overall\_mean} (\texttt{overall\_mean} only contains the overall mean scores for the sensory attributes);
\item
  \texttt{senso\_mean} however contains 3 extra columns: \texttt{Product}, \texttt{Protein}, and \texttt{Fiber} hence the parameter \texttt{cols\ =\ v\ +\ 3};
\item
  We apply the formatting to all the rows except the header, hence \texttt{rows\ =\ 1\ +\ 1:nrow(senso\_mean)};
\item
  Finally, we apply \texttt{pos\_style} (resp. \texttt{neg\_style}) if the value is larger (resp. lower) than the overall mean for that attribute using \texttt{rule\ =\ paste0("\textgreater{}",\ overall\_mean{[}1,v{]})} (resp. \texttt{rule\ =\ paste0("\textless{}",\ overall\_mean{[}1,v{]})}).
\end{itemize}

Once the spreadsheet is complete, we export the results using \texttt{saveWorkbook()} by specifying the name of the workbook (\texttt{wb}) and its path through \texttt{file}. In case such workbook already exists, it can be overwritten using \texttt{overwrite=TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveWorkbook}\NormalTok{(wb, }\AttributeTok{file=}\StringTok{"output/export2.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For more information regarding \texttt{\{openxlsx\}}, please visit (\url{https://rdrr.io/cran/openxlsx/})\url{https://rdrr.io/cran/openxlsx/}.

\hypertarget{powerpoint}{%
\subsection{PowerPoint}\label{powerpoint}}

\hypertarget{creating-a-powerpoint-deck}{%
\subsubsection{Creating a PowerPoint Deck}\label{creating-a-powerpoint-deck}}

Throughout the years, PowerPoint became one of the main supports for presenting results, whether it is in academia, in conference, or in companies. It is hence important to show how reports can be generated in PowerPoint from R. Many solutions exist, however the \texttt{\{officer\}} package is used here as its application is vast while still remaining easy to use.

\begin{quote}
\texttt{\{officer\}} contains a conflicting function with \texttt{\{readxl\}} in \texttt{read\_xlsx()}. To ensure you use the right function, call the function from the package of interest (e.g.~\texttt{readxl::read\_xlsx()}).{]}
\end{quote}

With \texttt{\{officer\}}, the procedure starts with creating a PowerPoint object (\texttt{pptx\_obj}) using the \texttt{read\_pptx()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(officer)}
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}} \FunctionTok{read\_pptx}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

A blank deck is set up with the \emph{Office Theme}. To use a custom theme and custom slides, a pre-defined deck from PowerPoint software can be used as input. Let's import the \emph{example.pptx} template that we created for you:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj\_custom }\OtherTok{\textless{}{-}} \FunctionTok{read\_pptx}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"example.pptx"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The content of the template can be inspected through \texttt{layout\_summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layout\_summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              layout       master
## 1       Title Slide Office Theme
## 2 Title and Content Office Theme
## 3    Section Header Office Theme
## 4       Two Content Office Theme
## 5        Comparison Office Theme
## 6        Title Only Office Theme
## 7             Blank Office Theme
\end{verbatim}

As can be seen by \texttt{layout\_summary()}, the default template imported (also called \emph{master}, which is defined here as \emph{Office Theme}) proposes 7 types of slides including \emph{Title Slide}, \emph{Title and Content}, \emph{Section Header}, \emph{Two Content}, \emph{Comparison}, \emph{Title Only}, and finally \emph{Blank}. The \emph{example.pptx} template has 11 different types of slides, and contains custom \emph{master slides} called \emph{Integral}\footnote{It is out of the scope of this book to describe how to build your own custom master slides. However, a quick internet search will provide you with all the information that you would need.}.

Each of these slides present some pre-defined properties (e.g.~a box for text of tables/images, a header etc.). Let's look at the properties of \emph{Title and Content} using \texttt{layout\_properties()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{layout\_properties}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(name }\SpecialCharTok{==} \StringTok{"Title and Content"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 13
##   master_~1 name  type  id    ph_la~2 ph     offx  offy
##   <chr>     <chr> <chr> <chr> <chr>   <chr> <dbl> <dbl>
## 1 Office T~ Titl~ body  3     Conten~ "<p:~  0.5   1.75
## 2 Office T~ Titl~ dt    4     Date P~ "<p:~  0.5   6.95
## 3 Office T~ Titl~ ftr   5     Footer~ "<p:~  3.42  6.95
## 4 Office T~ Titl~ sldN~ 6     Slide ~ "<p:~  7.17  6.95
## # ... with 1 more row, 5 more variables: cx <dbl>,
## #   cy <dbl>, rotation <dbl>, fld_id <chr>,
## #   fld_type <chr>, and abbreviated variable names
## #   1: master_name, 2: ph_label
\end{verbatim}

This code provides more details about the elements available in each type of slides, including their identifiers and positions on the slide. This information is required to export content in some specific elements.

\begin{quote}
Unfortunately, \texttt{\{officer\}} does not provide a function similar to \texttt{openxlsx::openXL()} that allows visualizing the file that is currently being build. Instead, the document needs to be saved on the disk using the \texttt{print()} function, which takes as entries the PowerPoint file to export (here \texttt{pptx\_obj}) and its output location.
\end{quote}

\hypertarget{addingmovingremoving-slides}{%
\subsubsection{Adding/Moving/Removing Slides}\label{addingmovingremoving-slides}}

With \texttt{\{officer\}}, various actions can be done on the slides. The first logical action consists in adding a new slide to a presentation, in which we will later on export some text, tables, figures, etc. Such action can be done using \texttt{add\_slide()}, in which the type of slide and the master\footnote{In practice, a unique template can contain slides from different masters.} are informed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{master }\OtherTok{\textless{}{-}} \StringTok{"Office Theme"}
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{\textquotesingle{}Title and Content\textquotesingle{}}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master)}
\end{Highlighting}
\end{Shaded}

This code adds slide of type \emph{Title and Content} to your deck.

Additional operations on the slides themselves can be done. In particular, you can re-organize your deck by changing the orders of your slides using \texttt{move\_slide()}, delete slides that are no longer needed through \texttt{remove\_slide()}, or modify a pre-existing slides by making it active using \texttt{on\_slide()} (by default, the last slide created is the active one).

For example, let's add another slide of type \emph{Two Content}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_slide}\NormalTok{(}\StringTok{"Two Content"}\NormalTok{, }\AttributeTok{master=}\NormalTok{master)}
\end{Highlighting}
\end{Shaded}

In case we would want to move this slide to eventually be first, the following code is used:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{move\_slide}\NormalTok{(}\AttributeTok{index=}\DecValTok{2}\NormalTok{, }\AttributeTok{to=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ultimately, this slide (now position as first slide) can be removed (by default, \texttt{index=NULL} and the active slide is deleted):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{remove\_slide}\NormalTok{(}\AttributeTok{index=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{positioning-information-on-the-slide}{%
\subsubsection{Positioning Information on the Slide}\label{positioning-information-on-the-slide}}

On a given slide, any type of content (text, graph, table, etc.) can be exported. To do so, we need to inform \emph{where} to write \emph{what}.

As we will see in the next sections, the \emph{what} can be any R element including simple text, tables, figures, etc. So let's ignore it for the moment, and let's focus on \emph{where}.

To inform where to print elements on the slide, the function \texttt{ph\_with()} (\emph{ph} stands for \emph{placeholder}) is used. In practice, \texttt{ph\_with()} comes with the parameter \texttt{location}, which takes as input a \emph{placeholder location object} pre-defined by the function \texttt{ph\_location()} or one of its derivative, one of the most useful one being \texttt{ph\_location\_type()}. To do so, simply provide the name stored in the column type from the \texttt{layout\_properties()} output presented before, as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"My functions are:"}\NormalTok{, }\StringTok{"ph\_with"}\NormalTok{, }\StringTok{"ph\_location\_type"}\NormalTok{)}

\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =} \StringTok{"My first title"}\NormalTok{, }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{"title"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ my\_data, }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{\textquotesingle{}body\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This code adds a title (``My first title'') and the text stored in \texttt{my\_data} to the \emph{body} of the slide (\emph{Title and Content}) created previously.

Other pre-defined alternatives to \texttt{ph\_location()} include:

\begin{itemize}
\tightlist
\item
  \texttt{ph\_location\_fullsize()} to produce an output that covers the entire slide;
\item
  \texttt{ph\_location\_left()} and \texttt{ph\_location\_right()} to write in the left/right box in \emph{Two Content} types of slide;
\item
  \texttt{ph\_location\_label()} is similar to \texttt{ph\_location\_type()} except that it uses the label rather than the type.
\end{itemize}

For a full control of the position where to print your element, \texttt{ph\_location()} is used as it allows specifying the exact location (for left/top/width/height, units are expressed in inches):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\OtherTok{\textless{}{-}} \StringTok{"My new text positioned using ph\_location()"}

\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ my\_data2, }
          \AttributeTok{location =} \FunctionTok{ph\_location}\NormalTok{(}\AttributeTok{left=}\DecValTok{2}\NormalTok{, }\AttributeTok{top=}\DecValTok{2}\NormalTok{, }\AttributeTok{width=}\DecValTok{3}\NormalTok{, }\AttributeTok{height=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To visualize the different steps done so far, let's save the results on our computers in an object called \emph{my export.pptx} stored in the folder \emph{output}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(pptx\_obj, }\StringTok{"output/my export.pptx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exporting-text}{%
\subsubsection{Exporting Text}\label{exporting-text}}

In the previous section, we already exported text to slides. Let's go a bit deeper in the process by also showing how to format the text.

By default, each new text item added to a PowerPoint via \texttt{\{officer\}} is a paragraph object. To further format the paragraph, three main functions are being used:

\begin{itemize}
\tightlist
\item
  \texttt{fpar()} (\emph{formatted paragraph}) creates the paragraph;
\item
  \texttt{ftext()} (\emph{formatted text}) allows editing the text before pasting into paragraphs. \texttt{ftext()} requires a second argument called \texttt{prop} which contains the formatting properties;
\item
  \texttt{block\_list()} allows us to wrap multiple paragraphs together.
\end{itemize}

Additionally, the text itself can be formated (font, size, color, etc.) using \texttt{fp\_text()}. Let's go through an example to illustrate the use of these functions:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Formatting option}
\NormalTok{my\_prop }\OtherTok{\textless{}{-}} \FunctionTok{fp\_text}\NormalTok{(}\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{font.size =} \DecValTok{14}\NormalTok{) }
\CommentTok{\# First line of text, formatted}
\NormalTok{my\_text }\OtherTok{\textless{}{-}} \FunctionTok{ftext}\NormalTok{(}\StringTok{"First Line in Red"}\NormalTok{, }\AttributeTok{prop =}\NormalTok{ my\_prop) }

\CommentTok{\# text into a paragraph}
\NormalTok{my\_par }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(my\_text) }
\CommentTok{\# other empty paragraph to introduce an empty line}
\NormalTok{blank\_line }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(}\StringTok{""}\NormalTok{) }

\CommentTok{\# second line of text, unformatted}
\NormalTok{my\_par2 }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(}\StringTok{"Second Line"}\NormalTok{) }
\CommentTok{\# Final block with the two lines of text separated by the empty line}
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{block\_list}\NormalTok{(my\_par, blank\_line, my\_par2) }

\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ my\_list, }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{"body"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Again, if you want to visualize the results, simply print the results as earliere:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(pptx\_obj, }\AttributeTok{target =} \StringTok{"output/my export.pptx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This add an additional slide to our previous PowerPoint deck with our formatted text.

Last element of formatting to consider is the hierarchy in bullet points. Let's add a slide containing three bullet points with a hierarchy so that the 1st and 3rd lines are primary points, and the second line is a secondary point. Such hierarchy is informed using the \texttt{level\_list} parameter, which informs the hierarchy of each element:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text1 }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(}\StringTok{"FIRST SENTENCE"}\NormalTok{)}
\NormalTok{text2 }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(}\StringTok{"second sentence"}\NormalTok{)}
\NormalTok{text3 }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(}\StringTok{"THIRD SENTENCE"}\NormalTok{)}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{block\_list}\NormalTok{(text1, text2, text3)}

\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ my\_data, }\AttributeTok{level\_list =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{), }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{\textquotesingle{}body\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{exporting-tables}{%
\subsubsection{Exporting Tables}\label{exporting-tables}}

After exporting formatted text to slides, let's export tables.

This can be done by rendering a data frame rather than text as \texttt{ph\_with()} accepts it and exports it in a default format. Let's use a subset of \texttt{senso\_mean}\footnote{\texttt{senso\_mean} was built in Section \ref{excel}.} for illustration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ft\_data }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Product, Salty, Sweet, Sour, Bitter) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), round, }\DecValTok{2}\NormalTok{)) }

\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}} \FunctionTok{read\_pptx}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ ft\_data, }\AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{"body"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Although this solution works fine, it does not allow formatting the table as much as we would want. Instead, we prefer to use another package called \texttt{\{flextable\}} (see Section \ref{flextable} for an introduction) which was developed by the same author as \texttt{\{officer\}}.

Remember that with \texttt{\{flextable\}}, the procedure starts with creating a \emph{flextable} object (here \texttt{ft\_table}) using the \texttt{flextable()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(flextable)}

\NormalTok{ft\_table }\OtherTok{\textless{}{-}}\NormalTok{ ft\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{flextable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This table can be customized in various ways such as:

\begin{itemize}
\tightlist
\item
  \texttt{align()} and \texttt{rotate()} controls for the text alignment and its rotation;
\item
  \texttt{bold()} and \texttt{italic()} writes the text in bold or italic;
\item
  \texttt{font()} and \texttt{fontsize()} controls the font type and the size to use;
\item
  \texttt{color()} and \texttt{bg()} allows changing the color of the text and of the background.
\end{itemize}

All these functions require informing the rows (parameter \texttt{i}) and the columns (\texttt{j}) as well as the \texttt{part} (\texttt{"body"}, \texttt{"header"}, \texttt{"footer"}, or \texttt{"all"}) to modify.

Additionally, further formatting can be applied to the table itself through the following functions:

\begin{itemize}
\tightlist
\item
  \texttt{height()} \& \texttt{width()} control for the row height and column width;
\item
  \texttt{border\_outer()}, \texttt{border\_inner()}, \texttt{border\_inner\_h()} \& \texttt{border\_inner\_v()} help design the table by adding borders;
\item
  \texttt{autofit()} and \texttt{padding()} are used to control the final size of the table.
\end{itemize}

For illustration, let's apply some of these functions to \texttt{ft\_table}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ft\_table }\OtherTok{\textless{}{-}}\NormalTok{ ft\_table }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fontsize}\NormalTok{(}\AttributeTok{size =} \DecValTok{11}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Formatting the header}
  \FunctionTok{font}\NormalTok{(}\AttributeTok{fontname =} \StringTok{"Roboto"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{color}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bold}\NormalTok{(}\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{align}\NormalTok{(}\AttributeTok{align =} \StringTok{"center"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bg}\NormalTok{(}\AttributeTok{bg =} \StringTok{"\#324C63"}\NormalTok{, }\AttributeTok{part =} \StringTok{"header"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Formatting the body}
  \FunctionTok{font}\NormalTok{(}\AttributeTok{fontname =} \StringTok{"Calibri"}\NormalTok{, }\AttributeTok{part =} \StringTok{"body"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bg}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(ft\_data), }\AttributeTok{bg =} \StringTok{"\#EDEDED"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Formatting the last row of the table}
  \FunctionTok{bold}\NormalTok{(}\AttributeTok{i =} \FunctionTok{nrow}\NormalTok{(ft\_data), }\AttributeTok{j =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(ft\_data)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{italic}\NormalTok{(}\AttributeTok{i =} \FunctionTok{nrow}\NormalTok{(ft\_data), }\AttributeTok{j =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Product}\SpecialCharTok{+}\NormalTok{Salty}\SpecialCharTok{+}\NormalTok{Sweet}\SpecialCharTok{+}\NormalTok{Sour}\SpecialCharTok{+}\NormalTok{Bitter) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{color}\NormalTok{(}\AttributeTok{i =}  \FunctionTok{nrow}\NormalTok{(ft\_data), }\AttributeTok{j =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Sour, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{color}\NormalTok{(}\AttributeTok{i =}  \FunctionTok{nrow}\NormalTok{(ft\_data), }\AttributeTok{j =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Sweet, }\AttributeTok{color =} \StringTok{"orange"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autofit}\NormalTok{()}

\CommentTok{\# Set up the border style}
\NormalTok{my\_border }\OtherTok{\textless{}{-}} \FunctionTok{fp\_border}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{style =} \StringTok{"solid"}\NormalTok{, }\AttributeTok{width =} \DecValTok{1}\NormalTok{)}

\NormalTok{ft\_table }\OtherTok{\textless{}{-}}\NormalTok{ ft\_table }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{border\_outer}\NormalTok{(}\AttributeTok{part =} \StringTok{"all"}\NormalTok{, }\AttributeTok{border =}\NormalTok{ my\_border) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{border\_inner}\NormalTok{(}\AttributeTok{part =} \StringTok{"body"}\NormalTok{, }\AttributeTok{border =} \FunctionTok{fp\_border}\NormalTok{(}\AttributeTok{style =} \StringTok{"dashed"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{width}\NormalTok{(}\AttributeTok{j =} \DecValTok{1}\NormalTok{, }\AttributeTok{width =} \FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is just an overview of the most relevant and used functions in \texttt{\{flextable\}}, yet there are more possibilities.
To go further, you can also consider the following functions (among many more):

\begin{itemize}
\tightlist
\item
  \texttt{merge()} merges vertically or horizontally cells with the same content;
\item
  \texttt{compose()}, \texttt{as\_chunk()}, and \texttt{as\_paragraph()} works hands in hands to create more complex text formatting (e.g.~sentence with parts of the text colored differently, or with sub/superscript);
\item
  \texttt{style()} applies a set of formatting properties to the same selection of the rows/columns.
\end{itemize}

Finally, to export a \emph{flextable} table to a PowerPoint deck, simply export it as we have seen before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ ft\_table, }
          \FunctionTok{ph\_location}\NormalTok{(}\AttributeTok{left =} \DecValTok{2}\NormalTok{, }\AttributeTok{top =} \DecValTok{2}\NormalTok{, }\AttributeTok{width =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{export_plots}{%
\subsubsection{Exporting Plots}\label{export_plots}}

The last type of R outputs to export to PowerPoint are figures. Before showing how to export them, let's build a simple bar chart from \texttt{senso\_mean} using \texttt{\{ggplot2\}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chart\_to\_export }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Product, Salty, Sweet, Sour, Bitter) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Salty}\SpecialCharTok{:}\NormalTok{Bitter, }
               \AttributeTok{names\_to =} \StringTok{\textquotesingle{}Attribute\textquotesingle{}}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{\textquotesingle{}Value\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Product, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ Attribute)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To export any \emph{ggplot2} object to PowerPoint, the package \texttt{\{rvg\}} is required.
This package provides two graphics devices that produces Vector Graphics outputs in \emph{DrawingML} format for Microsoft PowerPoint with \texttt{dml\_pptx()} and for Microsoft Excel with \texttt{dml\_xlsx()}, meaninFg the the graphics is being `rebuilt' in PowerPoint or Word. To simplify, the generic \texttt{dml()} function is used, and depending on the output format, the corresponding function is being called.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvg)}
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =}\NormalTok{ master) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =} \FunctionTok{dml}\NormalTok{(}\AttributeTok{ggobj =}\NormalTok{ chart\_to\_export), }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{\textquotesingle{}body\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

With \texttt{\{rvg\}}, the graphics are being \emph{rebuilt} in PowerPoint, meaning that they are completely editable. It is hence possible to change color, re-write text, move labels, etc.

To go further, the \texttt{\{mschart\}} package creates the graphs directly in PowerPoint or Word. These graphics have then the advantage to be interactive. However, this package is only limited to simple graphics (such as line chart, bar charts, etc.)

To produce such interact graphs, \texttt{ggplot2} graphs are not needed. Instead, functions such as \texttt{ms\_barchart()} are called to produce them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mschart)}

\NormalTok{mydata }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Product, Salty, Sweet, Sour, Bitter) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Salty}\SpecialCharTok{:}\NormalTok{Bitter, }
               \AttributeTok{names\_to =} \StringTok{\textquotesingle{}Attribute\textquotesingle{}}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{\textquotesingle{}Value\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Building the barchart using ms\_barchart()}
\NormalTok{my\_barchart }\OtherTok{\textless{}{-}} \FunctionTok{ms\_barchart}\NormalTok{(}\AttributeTok{data=}\NormalTok{mydata, }\AttributeTok{x=}\StringTok{"Product"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Value"}\NormalTok{, }
                           \AttributeTok{group=}\StringTok{"Attribute"}\NormalTok{)}

\CommentTok{\# The chart is a PowerPoint native object }
\CommentTok{\# It can be viewed using the preview option in print}
\FunctionTok{print}\NormalTok{(my\_barchart, }\AttributeTok{preview =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## * "ms_barchart" object
## 
## * original data [44,3] (sample):
##   Product Attribute  Value
## 1     P01     Salty  5.100
## 2     P01     Sweet 22.200
## 3     P01      Sour  0.000
## 4     P01    Bitter  8.000
## 5     P02     Salty  2.933
## 
## * series data [11,5] (sample):
##   Product Bitter Salty Sour Sweet
## 1     P01  8.000 5.100    0  22.2
## 2     P02  4.933 2.933    0  15.8
## 3     P03  7.800 4.667    0  10.4
## 4     P04  4.267 3.600    0  16.6
## 5     P05  6.733 5.867    3  21.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# To add the object to a PPT slide, officer\textquotesingle{}s ph\_with() function is used}
\NormalTok{pptx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ pptx\_obj }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_slide}\NormalTok{(}\AttributeTok{layout =} \StringTok{"Title and Content"}\NormalTok{, }\AttributeTok{master =} \StringTok{"Office Theme"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ph\_with}\NormalTok{(}\AttributeTok{value =}\NormalTok{ my\_barchart, }
          \AttributeTok{location =} \FunctionTok{ph\_location\_type}\NormalTok{(}\AttributeTok{type =} \StringTok{"body"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now the full deck is being created, let's save in one last time using \texttt{print()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(pptx\_obj, }\AttributeTok{target =} \StringTok{"output/my export.pptx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you open the PowerPoint just exported, on the final slide, you'll find the barchart generated by \texttt{\{mschart\}}. By clicking the graph, you'll find a `funnel' icon on the right side, which allows you filter attributes or products, hence making your graph interactive.

At last, \texttt{\{officer\}} also allows you adding images that are stored on your computer into a PowerPoint deck. This can be done through the \texttt{external\_img()} function, which takes as input the location of the file. Like for any other graph, simply apply this function within \texttt{ph\_with()} by specifying the location where to print the image.

\hypertarget{word}{%
\subsection{Word}\label{word}}

The process for building Word document directly from R is very similar to the one for PowerPoint, since it is also handled though \texttt{\{officer\}}.

To start a new Word document, the \texttt{read\_docx()} function is being used. Since Word documents are more \emph{text oriented} than PowerPoint, blocks of text are defined as paragraph. To introduce a new paragraph, the \texttt{body\_add\_par()} function is called. Note that paragraphs are automatically separated by line breaks:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{docx\_obj }\OtherTok{\textless{}{-}} \FunctionTok{read\_docx}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"My Text"}\NormalTok{, }\AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Other Text"}\NormalTok{, }\AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Conclusion"}\NormalTok{, }\AttributeTok{style =} \StringTok{"Normal"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here again, the results can be exported to your computer using \texttt{print()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(docx\_obj, }\AttributeTok{target =} \StringTok{"output/my export.docx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Of course, it is not required to use the default formatting options from the word document in use. Instead, we can format it directly from R using \texttt{body\_add\_fpar()} to add a formatted text paragraph, or apply pre-defined styles to the previous function suggested (as is the case here with \texttt{style\ =\ "heading\ 1"} to set the text as a title of level 1).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_format }\OtherTok{\textless{}{-}} \FunctionTok{fp\_text}\NormalTok{(}\AttributeTok{font.family =} \StringTok{\textquotesingle{}Calibri\textquotesingle{}}\NormalTok{, }\AttributeTok{font.size =} \DecValTok{14}\NormalTok{, }
                     \AttributeTok{bold =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)}
\NormalTok{my\_text }\OtherTok{\textless{}{-}} \FunctionTok{ftext}\NormalTok{(}\StringTok{\textquotesingle{}Here is another example of text\textquotesingle{}}\NormalTok{, my\_format)}
\NormalTok{my\_par }\OtherTok{\textless{}{-}} \FunctionTok{fpar}\NormalTok{(my\_text)}

\NormalTok{docx\_obj }\OtherTok{\textless{}{-}} \FunctionTok{read\_docx}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Document Title"}\NormalTok{, }\AttributeTok{style =} \StringTok{"heading 1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{""}\NormalTok{, }\AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_fpar}\NormalTok{(my\_par, }\AttributeTok{style =} \StringTok{"Normal"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To export tables or figures, additional functions including \texttt{body\_add\_table()} (for tables) and \texttt{body\_add\_gg()}\footnote{Note that \texttt{body\_add\_img()} and \texttt{body\_add\_plot()} can also be used.}) (for \texttt{ggplot()} figures) are used. These can be combined to \texttt{body\_add\_caption()} to add a caption to your table/figure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_num }\OtherTok{\textless{}{-}} \FunctionTok{run\_autonum}\NormalTok{(}\AttributeTok{seq\_id =} \StringTok{"tab"}\NormalTok{, }
                         \AttributeTok{pre\_label =} \StringTok{"Table "}\NormalTok{, }\AttributeTok{bkm =} \StringTok{"tables"}\NormalTok{)}
\NormalTok{figure\_num }\OtherTok{\textless{}{-}} \FunctionTok{run\_autonum}\NormalTok{(}\AttributeTok{seq\_id =} \StringTok{"fig"}\NormalTok{, }
                          \AttributeTok{pre\_label =} \StringTok{"Figure "}\NormalTok{, }\AttributeTok{bkm =} \StringTok{"figures"}\NormalTok{)}

\NormalTok{docx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ docx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Exporting Tables"}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"heading 2"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{""}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Here is my first table:"}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{""}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_table}\NormalTok{(}\AttributeTok{value =} \FunctionTok{head}\NormalTok{(mtcars)[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }
                 \AttributeTok{style =} \StringTok{"table\_template"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_caption}\NormalTok{(}\FunctionTok{block\_caption}\NormalTok{(}\StringTok{"My first table."}\NormalTok{, }
                                 \AttributeTok{style=}\StringTok{"centered"}\NormalTok{, }
                                 \AttributeTok{autonum=}\NormalTok{table\_num)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Exporting Figures"}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"heading 2"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{""}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Here is my first figure:"}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{""}\NormalTok{, }
               \AttributeTok{style =} \StringTok{"Normal"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_gg}\NormalTok{(}\AttributeTok{value =}\NormalTok{ chart\_to\_export) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_caption}\NormalTok{(}\FunctionTok{block\_caption}\NormalTok{(}\StringTok{"My first figure."}\NormalTok{, }
                                 \AttributeTok{style=}\StringTok{"centered"}\NormalTok{, }
                                 \AttributeTok{autonum=}\NormalTok{figure\_num))}
\end{Highlighting}
\end{Shaded}

As can be seen, \texttt{body\_add\_caption()} is combined to \texttt{block\_caption()}, and can have some automated numbering, as defined previously using \texttt{table\_num} for tables, and \texttt{figure\_num} for figures.

Unlike a PowerPoint file that contains separate slides, a word document is a continuous object. Hence, to emphasize a break and add content to a new page, \texttt{body\_add\_break()} needs to be called. Additionally, tables of content can be generated using \texttt{body\_add\_toc()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{docx\_obj }\OtherTok{\textless{}{-}}\NormalTok{ docx\_obj }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_break}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_par}\NormalTok{(}\AttributeTok{value =} \StringTok{"Conclusion"}\NormalTok{, }\AttributeTok{style =} \StringTok{"heading 1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_break}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{body\_add\_par}\NormalTok{(}\StringTok{"Table of Contents"}\NormalTok{, }\AttributeTok{style =} \StringTok{"heading 1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{body\_add\_toc}\NormalTok{(}\AttributeTok{level =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, let's export the final version of the word document to visualize it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(docx\_obj, }\AttributeTok{target =} \StringTok{"output/my export.docx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As can be seen, it is possible to format a nice report in Word directly from R, that integrates text, tables, figures and more. For more information regarding \texttt{\{officer\}}, and on how to export results to Word and PowerPoint, please visit (\url{https://ardata-fr.github.io/officeverse/index.html})\url{https://ardata-fr.github.io/officeverse/index.html}.

\begin{quote}
It is worth mentioning that \texttt{\{officer\}} also allows extracting information from existing reports (Word and PowerPoint). It is however outside the scope of this book and will not be further described.
\end{quote}

\hypertarget{notes-on-applying-corporate-branding}{%
\subsection{Notes on applying corporate branding}\label{notes-on-applying-corporate-branding}}

You may have noticed that we have been consistent with our approach to export results to reports, regardless of the final output:
We start with pre-defining our styling parameters that we then apply to our different tables, slides, paragraphs, etc. This is not a formal rule, yet we strongly recommend you adopting this way of working. Indeed, by creating your different styling parameters at the start of your script file, these lines of code do not interfere with your analyses. At a later stage, you will thank yourself for keeping well-structured code as it gains in clarity, and hence facilitates debugging your code in case of error or changes.

To go one step further, we would recommend you storing all these styling parameters in a separate file you load any time you need them through \texttt{source()}. This process reduces the size of your script file, hence increasing its clarity, while harmonizing all your exports by centralizing your formatting code in one unique place. The last point is crucial since any changes only need to be done once, and yet will be applied to all your reports.

As we have seen, \texttt{\{officer\}} gives you the opportunity to import pre-defined templates (PowerPoint or Word). This is very valuable as your report can easily match your corporate style.

Ultimately, to ensure optimal efficiency, we advise you to spend a bit more time when constructing your report by ensuring that as many details are being taken care of, so that later on, you can spend more time in the story building part and less on the analysis and slide generation. For instance, don't be afraid of mass-exporting results, as it is easier to remove slides, tables, or figures (in case they are not needed for your story) then it is to re-generate them at a later stage (if missing).

\hypertarget{integrating-analyses-scripts-within-your-reporting-tool}{%
\section{Integrating analyses scripts within your reporting tool}\label{integrating-analyses-scripts-within-your-reporting-tool}}

As we have just seen, we can generate reports in the Microsoft Office suit directly from our R script. Although the results are being showed, the script used to reach these results is completely hidden. Of course, we could add them as text, but the logic would suggest that the researcher can just get back to the script to decode how certain outputs have been obtained.

Let's now change our way of thinking by proposing an alternative in which we integrate our R analysis directly within a reporting tool. For that, we need to introduce another useful package for reporting and document building: \texttt{\{rmarkdown\}}.

\hypertarget{what-is-rmarkdown}{%
\subsection{\texorpdfstring{What is \texttt{\{rmarkdown\}}}{What is \{rmarkdown\}}}\label{what-is-rmarkdown}}

Markdown is an ecosystem specific to text document, in which authors \emph{script} their reports by controlling various features including:

\begin{itemize}
\tightlist
\item
  paragraphs and inline formatting (e.g.~bold, italic, etc.)
\item
  (section) headers
\item
  blocks (code, or quotations)
\item
  (un)numbered lists
\item
  horizontal rules
\item
  tables and figures (including legends)
\item
  LaTeX math expressions, formulas, and theorems
\item
  links, citations, and footnotes
\end{itemize}

Limiting the creation of Markdown document to this list of elements is more an advantage than a drawback as it suffice to create technical and non-technical documents while still keeping it simple.

In practice, R Markdown provides an authoring framework for data science, as it can be use for saving/documenting/executing code and generating high quality reports. Once the document is being created, you can then compile it to build it in the output format of your choice (e.g.~word, pdf, html, etc.)

\hypertarget{starting-with-rmarkdown}{%
\subsection{Starting with \{rmarkdown\}}\label{starting-with-rmarkdown}}

To start, you need to install the \texttt{\{rmarkdown\}} package using the \texttt{install.packages()} function. To load this package, just type \texttt{library(rmarkdown)}. If you intend to build your report in pdf, you also need to install a LaTeX library. For its simplicity, we recommend you installing the TinyTeX library using \texttt{install.packages("tinytex")}.

Let's start with a simple example that is provided by RStudio. To start a RMarkdown document, click \emph{File} \textgreater{} \emph{New File} \textgreater{} \emph{R Markdown\ldots{}} This opens a new window in which you can inform the name of your file, the author name, and the type of report to create (HTML, PDF, or Word). Once set, click \emph{OK}. A new script file of type \emph{.Rmd} opens.

In this document, there are three components: metadata, text, and code.

The document starts with the metadata. It is easily recognizable as it starts and ends with 3 successive dashes (\texttt{-\/-\/-}), and its syntax is YAML (YAML Ain't Markup Language). In this part, information regarding the properties of the final document is being stored. This includes (amongst other) the title, authorship, date, export format, etc. of the final document.

Be aware that indentation matters in YAML, so follow the rules to ensure that your document compiles correctly.

Right after the metadata is the body of document. The syntax for the text is Markdown, and the main features will be presented in the next section. Within the body, computer code can be added, either as a chunk, or within the text.

\hypertarget{rmarkdown-through-a-simple-example}{%
\subsection{\texorpdfstring{\texttt{\{rmarkdown\}} through a Simple Example}{\{rmarkdown\} through a Simple Example}}\label{rmarkdown-through-a-simple-example}}

To illustrate the use of \texttt{\{rmarkdown\}}, let's consider this simple document (inspired from REF):

The top of the document contains the metadata, which (in our case) will generate the report in an HTML document.

Next, we have a first chunk of code that sets the main options on how the code should be handled. If all the code chunks are handled in the same way, it is handy to set it at the start. However, when different chunks of code should be handled differently, it may be easier to define for each section how it should be handled.

There are mainly four ways to handle code.

The first way is defined here on the code chunk \emph{header} as \texttt{include\ =\ FALSE}\footnote{When set manually, this is where you should indicate how to handle each chunk of code}: \texttt{include} always run the code, yet it allows printing (\texttt{include\ =\ TRUE}) or not (\texttt{include\ =\ FALSE}) the code and its outputs in the final document.

The second option is \texttt{echo}. In this code chunk, we automatically set that all the code chunk should be defined as \texttt{echo\ =\ TRUE}, which means that the code will run and be printed (together with its output) in the document. This seems very similar to \texttt{include}, yet it differs from it as \texttt{echo\ =\ FALSE} runs the code, prints the outputs, but not the code.

If you only want to show some code without running it, the \texttt{eval} parameter is used (\texttt{eval\ =\ FALSE} means that the code will be displayed but will not run). This is useful for displaying example code, or for disabling large or time-consuming chunk of codes without having to set it up as comment.

Last, we can control whether outputs should be shown or hidden using \texttt{results} (printed output) and \texttt{fig.show} (plots). By default, the results are shown, unless it is set as \texttt{results\ =\ "hide"} or \texttt{fig.show\ =\ "hide"}.

The document then continues with a section header, which starts with \emph{\#}. The hierarchy of headers is defined by the number of adjacent \emph{\#} (for a header of level 3, starts the header with \emph{\#\#\#}).

In this section, a first paragraph is being written. This is plain text, except for two particular words, one written between two ```'' (backticks), and one written between 2 double ``*'' (stars). Here, the backticks are used to write text in R font (or as we will see later, to render results from R), whereas the double stars write the text in bold (double ``\_'' (underscore) could also be used). For italic, one single star (or one single underscore) are used.

If the following section and sub-section, we introduce numbered and unnumbered list of elements. For numbered list, starts with a \emph{number} followed by a \emph{.} (numbers will be incremented automatically). For unnumbered list, you can either start with a ``-'' (dash), or with ``*'' (star) for bullet points. For sub-list, indent your marker by pressing the \emph{Tab} key.

In the next section called \emph{Analysis}, we are running our first lines of code.

The first code chunk runs a regression model. In the text under the second chunk of code, we are retrieving automatically a value from R by including a \emph{r} at the starts of two \emph{backticks} followed by the element to retrieve. In our final report, this code will automatically be replaced by the value 3.93.

The second code chunk shows how the results can be printed, either directly from R, or in a nicer way using the \texttt{knitr::kable()} function.

Finally, the last code chunk of this section creates a plot with a caption, that is automatically numbered.

\hypertarget{creating-a-document-using-knitr}{%
\subsection{\texorpdfstring{Creating a document using \texttt{\{knitr\}}}{Creating a document using \{knitr\}}}\label{creating-a-document-using-knitr}}

Once the document is ready, you can neat it using the \texttt{knit} button. This will create the report in the format of interest (here HTML).

\hypertarget{example-of-applications}{%
\subsection{Example of applications}\label{example-of-applications}}

\texttt{\{rmarkdown\}} is a very powerful tool for building report, in particular in the context of reproducible research since it allows sharing code, and running analyses within the report (part of the text around the code can justify the decisions made in terms of analyses to ensure transparency). The latter point is particularly interesting since any change in the data will automatically provide updated results throughout the report, without you having to change them manually.

Its application is various, and can go from report, to teaching material, publication or even books (this book has been written in \texttt{\{rmarkdown\}} and its extension \texttt{\{bookdown\}}), emails, websites, dashboards, surveys etc. Even more interestingly, \texttt{\{rmarkdown\}} can also be combined to \texttt{\{shiny\}} to build interactive reports, dashboards, or teaching materials in which users would (say) import their data set, select the variables to analyze through buttons, chose which analyses and which options to perform, and the results will automatically be generated accordingly.

For more information on \texttt{\{rmarkdown\}} and related packages, please see:
* (\url{https://bookdown.org/yihui/bookdown/})\url{https://bookdown.org/yihui/bookdown/}
* (\url{https://bookdown.org/yihui/rmarkdown-cookbook/})\url{https://bookdown.org/yihui/rmarkdown-cookbook/}
* (\url{https://bookdown.org/yihui/rmarkdown/})\url{https://bookdown.org/yihui/rmarkdown/}

As mentioned earlier, R Markdown can also be used to generate other types of documents, including presentations. This can be done directly from the \texttt{\{rmarkdown\}} package using \emph{ioslides} presentation (\texttt{output:\ ioslides\_presentation} in the metadata), \emph{Slidy} presentation (\texttt{output:\ slidy\_presentation}), or \emph{PowerPoint} presentation (\texttt{output:\ powerpoint\_presentation} with \texttt{reference\_doc:\ my-styles.pptx} to apply your own template) just to name a few. It can also be done using additional packages such as \texttt{\{xarigan\}}.

\hypertarget{to-go-further}{%
\section{To go further\ldots{}}\label{to-go-further}}

If R allows you saving time by creating your report within your R-script, or by running your analysis within your report document, it cannot communicate the presentation to your partners/clients for you. However, if the report is very standard (say only key results, tables or figures), or running routinely (say in quality control), R could automatically run the analysis as soon as new data is available, build the report, and send it automatically to you, your manager or your colleagues and partners by email.

Such process can be done thanks to the \texttt{\{blastula\}} package (see REF).

\hypertarget{example-projects}{%
\chapter{Example Project: The Biscuit Study}\label{example-projects}}

\hypertarget{objective-of-the-test}{%
\section{Objective of the Test}\label{objective-of-the-test}}

The data set that we use as a main example throughout this book comes from a sensory study on biscuits. The study was part of project BISENS funded by the French National Research Agency (ANR, programme ALIA 2008). These biscuits were developed for breakfast consumption and specifically designed to improve satiety.

The study was conducted in France with one hundred and seven consumers who tested a total of 10 biscuit recipes (including 9 experimental products varying in their fiber and protein content), as fibers and proteins are known to increase satiety.

The study aimed to measure the liking for these biscuits, its link with eaten quantities and the evolution of hunger sensations over ad libitum consumption. All the volunteers therefore participated to ten morning sessions in order to test every product (one biscuit type per session). After they completed all the sessions, they also filled a questionnaire about food-related personality traits such as cognitive restraint and sensitivity to hunger.\\
Parallel to this, a panel of nine trained judges performed a quantitative descriptive analysis of the biscuits. They evaluated the same 10 products as well as an additional product whose recipe was optimized for liking and satiating properties.

Data from the biscuit study are gathered in three Excel files that can be accessed here {[}ADD LINK HERE{]}:

\begin{itemize}
\tightlist
\item
  biscuits\_consumer\_test.xlsx\\
\item
  biscuits\_sensory\_profile.xlsx\\
\item
  biscuits\_traits.xlsx
\end{itemize}

\hypertarget{products}{%
\section{Products}\label{products}}

In total, 11 products were considered in this study. They are all breakfast biscuits with varying contents of proteins and fibers (Table \ref{tab:biscuit-set}). Products \emph{P01} to \emph{P09} are prototypes whereas product \emph{P10} is a standard commercial biscuit without enrichment. The eleventh product \emph{Popt} is an additional optimized biscuit that has been evaluated only by the trained panel for descriptive analysis.

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.66in}|p{0.54in}|p{1.32in}}

\caption{\textcolor[HTML]{000000}{\fontsize{11}{13}\selectfont{\global\setmainfont{Arial}{Product\ set\ for\ the\ biscuit\ study}}}}\label{tab:biscuit-set}\\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Protein}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Fiber}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Type}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P01}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P02}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P03}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P04}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P05}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P06}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P07}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P08}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P09}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Trial}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{P10}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Commercial\ product}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{POpt}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.66in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{High}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.54in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Low}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 1.32in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Optimized\ trial}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

\hypertarget{consumer-test}{%
\section{Consumer test}\label{consumer-test}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

107 women who were all regular consumers of breakfast biscuits participated to the test. The \emph{biscuits\_traits.xlsx} file gives information about their Body Mass Index (BMI) (Q4-Q6) and their socio-demographics (Q7-Q11: marital status, household, income, occupation, highest degree).\\
This file also gives participants' answers to a self-assessment questionnaire (Q12-62) that evaluates eating behavioral traits with emphasis on the tendency to control food intake cognitively. The questionnaire comprises a series of assertions about various eating situations in the respondent's daily life (e.g.~``How\ldots.'' answer Y/N''). Resulting scores are loaded into three factors: cognitive restraint (conscious restriction of food intake in order to control body weight or to promote weight loss), disinhibition (or emotional eating), and susceptibility to hunger (or uncontrolled eating, i.e.~tendency to eat more than usual due to a loss of control over intake accompanied by feelings of hunger). This questionnaire is thus known as the Three-Factor Eating Questionnaire (TFEQ) \citep{stunkard1985} and is one of the most commonly used questionnaires to evaluate eating behaviors in relation to overweight or obesity \citep{blundell2010}. Calculation of these factors is detailed in chapter {[}\ref{data-analysis}{]}.

\hypertarget{test-design}{%
\subsection{Test design}\label{test-design}}

The presentation order of the different products was randomized across the panel. Again, consumers evaluated one biscuit type per day/session.

The design of the sessions is summarized in Figure \ref{fig:test-design} with main measured variables. After they first rated their appetite sensations using visual analog scales (VAS), the participants tasted and rated one biscuit for liking. They were then served with a box of the same biscuits for \emph{ad libitum} consumption (with a maximum of 10 biscuits), followed by a new questionnaire regarding their liking, pleasure and appetite sensations.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/consumer_test_design} 

}

\caption{General design for the consumer test of the biscuit study. Participants were served with a different set of biscuits every session.}\label{fig:test-design}
\end{figure}

The liking was measured with two different scales:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  with a horizontally oriented unstructured linear scale (i.e.~VAS) anchored with \emph{`I don't like this biscuit at all'} (left end) and \emph{`I like this biscuit a lot'} (right end) at two different times: after the first bite and at the end of their consumption.
\item
  with a vertically oriented semantic nine-point hedonic scale when stopping their consumption.
  VAS scales are frequently used in nutrition studies \citep{stubbs2000}, whereas the nine-point hedonic scale is more popular in sensory and consumer science \citep{peryam1957, wichchukit2015}.
\end{enumerate}

Once done, participants were asked about the reason(s) why they stopped eating (6 potential reasons rated with \emph{Likert} scales ranging from \emph{strongly disagree} to \emph{strongly agree}). They were also asked how much they would like to eat other types of foods (11 food items rated using a VAS).

The time spent in the booth and the number of biscuits eaten by each participant was recorded by the experimenters, as well as the type of drink they selected and the approximate volume they drank during each session. These data are stored in \emph{biscuits\_consumer\_test.xlsx}, in the second tab named \emph{Time Consumption}.

\hypertarget{sensory-descriptive-analysis-data}{%
\section{Sensory descriptive analysis data}\label{sensory-descriptive-analysis-data}}

A panel of 9 trained judges evaluated the 11 products on 32 sensory attributes (8 attributes for aspect, 3 for odor, 12 for flavor, 9 for texture).

For each product, the judges individually reported the perceived intensity of each attribute on an unstructured linear scale. Intensities were automatically converted by the acquisition system into a score ranging from 0 to 60. These data are stored in \emph{biscuits\_sensory\_profile.xlsx}.

\hypertarget{data-collection}{%
\chapter{Data Collection}\label{data-collection}}

\begin{quote}
Before any statistical analysis and vizualizations, robust data need to be collected. This important step often requires a \emph{proper} experimental design, i.e.~an experimental design that would assure relevant and meaningful data are obtained with maximum efficiency to answer our research questions. This chapter approaches all the required steps to reach such goal, from setting up the test (e.g.~estimation of the number of panelists, design of sensory evaluation sessions and design of experiments), to the collection of data (through valuable execution tips) and its importation in a statistical software (R, here).
\end{quote}

\hypertarget{designs-of-sensory-doe-experiments}{%
\section{Designs of sensory (DoE) experiments}\label{designs-of-sensory-doe-experiments}}

Like with any other chapter, let's start by loading the \texttt{\{tidyverse\}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{general-approach}{%
\subsection{General approach}\label{general-approach}}

Sensory and consumer science relies on experiments during which subjects usually evaluate several samples one after the other. This type of procedure is called `monadic sequential' and is common practice for all three main categories of tests (difference testing, descriptive analysis, hedonic testing). The main advantage of proceeding this way is that responses are within-subjects (data can be analyzed at the individual level) so that analysis and interpretation can account for inter-individual differences, which is a constant feature of sensory data.

However, this type of approach also comes with drawbacks\footnote{Market researchers would argue that evaluating several products in a row doesn't usually happen in real life and that proceeding this way may induce response biases. They thus advocate the use of pure monadic designs in which participants are only given one sample to evaluate. This corresponds to a between-group design that is also frequently used in fields where only one treatment per subject is possible (drug testing, nutrition studies, etc.).} as it may imply order effects and carry-over effects \citep{Macfie1989}. Fortunately, most of these effects can be controlled with a proper design of experiment (DoE). A good design ensures that order and carry-over effects are not confounded with what you are actually interested to measure (most frequently, the differences between products) by balancing these effects across the panel. However, it is important to note that the design does not eliminate these effects and that each subject in your panel may still experience an order and a carry-over effect, as well as boredom, sensory fatigue, etc.

Before going any further into the design of sensory evaluation sessions, it is important to first estimate the number of panelists needed for your study. For that, you may rely on common practices. For instance, most descriptive analysis studies with trained panelists are typically conducted with 10-20 judges, whereas 100 participants is usually considered as a minimum for hedonic tests. Of course, these are only ballpark numbers and they must be adjusted to the characteristics of the population you are interested in and to the specifics of your study objectives. In all cases, a power analysis would be wise to make sure that you have a good rationale for your proposed sample size, especially for studies involving consumers. The \texttt{\{pwr\}} package provides a very easy way to do that, as shown in the example code below for a comparison between two products on a paired basis (such as in monadic sequential design). Note that you need to provide an effect size (expressed here by Cohen's \emph{d}, which is the difference you aim to detect divided by the estimated standard deviation of your population).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pwr)}
\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{n=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{sig.level=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{type=}\StringTok{"paired"}\NormalTok{, }
           \AttributeTok{alternative=}\StringTok{"two.sided"}\NormalTok{, }\AttributeTok{power=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{d=}\FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Paired t test power calculation 
## 
##               n = 89.15
##               d = 0.3
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number of *pairs*
\end{verbatim}

For discrimination tests (e.g.~tetrad, 2-AFC, etc.), the reader may also refer to the \texttt{\{sensR\}} package and its \texttt{discrimSS()} function for the sample size calculation in both difference or similarity testing context.

\hypertarget{Crossover}{%
\subsection{Crossover designs}\label{Crossover}}

For any sensory experiment that implies the evaluation of more than one sample, first-order and/or carry-over effects should be expected. That is to say, the evaluation of a sample may affect the evaluation of the next sample even though sensory scientists try to lower such effects by asking panelists to pause between samples and use of appropriate mouth-cleansing techniques (drinking water, eating unsalted crackers, or a piece of apple, etc.). The use of crossover designs is thus highly recommended \citep{Macfie1989}.

Williams's Latin-Square designs offer a perfect solution to balance carry-over effects. They are very simple to create using the \texttt{williams()} function from the \texttt{\{crossdes\}} package. For instance, if you have five samples to test, \texttt{williams(5)} would create a 10x5 matrix containing the position at which each of three samples should be evaluated by 10 judges (the required number of judges per design block).

Alternately, the \texttt{WilliamsDesign()} function in \texttt{\{SensoMineR\}} allows you to create a matrix of samples (as numbers) with numbered \texttt{Judges} as row names and numbered \texttt{Ranks} as column names. You only have to specify the number of samples to be evaluated, as in the example below for 5 samples.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(SensoMineR)}
\NormalTok{wdes\_5P10J }\OtherTok{\textless{}{-}} \FunctionTok{WilliamsDesign}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
## Judge 1       2      1      3      4      5
## Judge 2       1      4      2      5      3
## Judge 3       4      5      1      3      2
## Judge 4       5      3      4      2      1
## Judge 5       3      2      5      1      4
## Judge 6       5      4      3      1      2
## Judge 7       3      5      2      4      1
## Judge 8       2      3      1      5      4
## Judge 9       1      2      4      3      5
## Judge 10      4      1      5      2      3
\end{verbatim}

Suppose you want to include 20 judges in the experiment, you would then need to duplicate the initial design.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wdes\_5P20J }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(rbind, }\FunctionTok{replicate}\NormalTok{(}\DecValTok{2}\NormalTok{, wdes\_5P10J, }\AttributeTok{simplify=}\ConstantTok{FALSE}\NormalTok{))}
\FunctionTok{rownames}\NormalTok{(wdes\_5P20J) }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"judge"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{sep=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
## judge1       2      1      3      4      5
## judge2       1      4      2      5      3
## judge3       4      5      1      3      2
## judge4       5      3      4      2      1
## judge5       3      2      5      1      4
## judge6       5      4      3      1      2
## judge7       3      5      2      4      1
## judge8       2      3      1      5      4
## judge9       1      2      4      3      5
## judge10      4      1      5      2      3
## judge11      2      1      3      4      5
## judge12      1      4      2      5      3
## judge13      4      5      1      3      2
## judge14      5      3      4      2      1
## judge15      3      2      5      1      4
## judge16      5      4      3      1      2
## judge17      3      5      2      4      1
## judge18      2      3      1      5      4
## judge19      1      2      4      3      5
## judge20      4      1      5      2      3
\end{verbatim}

The downside of Williams's Latin square designs is that the number of samples (\emph{k}) to be evaluated dictates the number of judges. For an even number of samples you must have a multiple of \emph{k} judges, and a multiple of \emph{2k} judges for an odd number of samples.

As the total number of judges in your study may not always be exactly known in advance (e.g.~participants not showing up to your test, extra participants recruited at the last minute), it can be useful to add some flexibility to the design. Of course, additional rows would depart from the perfectly balanced design, but it is possible to optimize them using Federov's algorithm thanks to the \texttt{optFederov()} function of the \texttt{\{AlgDesign\}} package, by specifying \texttt{augment\ =\ TRUE}. For example we can add three more judges to the Williams Latin square design that we just built for \emph{nbP=}5 products and 10 judges, hence leading to a total number of \emph{nbP=}13 judges. Note that this experiment is designed so that each judge will evaluate all the products, therefore the number of samples per judge (\emph{nbR}) equals the number of products (\emph{nbP}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AlgDesign)}
\NormalTok{nbJ}\OtherTok{=}\DecValTok{13}
\NormalTok{nbP}\OtherTok{=}\DecValTok{5}
\NormalTok{nbR}\OtherTok{=}\NormalTok{nbP}

\NormalTok{wdes\_5P10J }\OtherTok{\textless{}{-}} \FunctionTok{WilliamsDesign}\NormalTok{(nbP)}
\NormalTok{tab }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{prod=}\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{t}\NormalTok{(wdes\_5P10J)), }\AttributeTok{judge=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbJ,}\AttributeTok{each=}\NormalTok{nbR), }\AttributeTok{rank=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbR,nbJ))}
\NormalTok{optdes\_5P13J }\OtherTok{\textless{}{-}} \FunctionTok{optFederov}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{prod}\SpecialCharTok{+}\NormalTok{judge}\SpecialCharTok{+}\NormalTok{rank, }\AttributeTok{data=}\NormalTok{tab, }\AttributeTok{augment=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{nTrials=}\NormalTok{nbJ}\SpecialCharTok{*}\NormalTok{nbP, }\AttributeTok{rows=}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(nbJ}\SpecialCharTok{*}\NormalTok{nbP), }\AttributeTok{nRepeats =} \DecValTok{100}\NormalTok{)}
\FunctionTok{xtabs}\NormalTok{(optdes\_5P13J}\SpecialCharTok{$}\NormalTok{design)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      rank
## judge 1 2 3 4 5
##    1  5 1 4 3 2
##    2  1 3 5 2 4
##    3  3 2 1 4 5
##    4  2 4 3 5 1
##    5  4 5 2 1 3
##    6  2 3 4 1 5
##    7  4 2 5 3 1
##    8  5 4 1 2 3
##    9  1 5 3 4 2
##    10 3 1 2 5 4
##    11 5 1 4 3 2
##    12 1 3 5 2 4
##    13 3 2 1 4 5
\end{verbatim}

In the code above, \texttt{xtabs()} is used to arrange the design in a table format that is convenient for the experimenter.

Note that it would also be possible to start from an optimal design and expand it to add one judge at a time. The code below first builds a design for 5 products and 13 judges and then adds one judge to make the design optimal for 5 products and 14 judges.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nbJ}\OtherTok{=}\DecValTok{13}
\NormalTok{nbP}\OtherTok{=}\DecValTok{5}
\NormalTok{nbR}\OtherTok{=}\NormalTok{nbP}

\NormalTok{optdes\_5P13J }\OtherTok{\textless{}{-}} \FunctionTok{optimaldesign}\NormalTok{(nbP, nbP, nbR)}\SpecialCharTok{$}\NormalTok{design}
\NormalTok{tab }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{prod=}\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{t}\NormalTok{(optdes\_5P13J)),}
             \AttributeTok{judge=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbJ,}\AttributeTok{each=}\NormalTok{nbR),}
             \AttributeTok{rank=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbR,nbJ))}
\NormalTok{add }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{prod=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbP,nbR),}
             \AttributeTok{judge=}\FunctionTok{rep}\NormalTok{(nbJ}\SpecialCharTok{+}\DecValTok{1}\NormalTok{,nbP}\SpecialCharTok{*}\NormalTok{nbR),}
             \AttributeTok{rank=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nbR,}\AttributeTok{each=}\NormalTok{nbP))}

\NormalTok{optdes\_5P14J }\OtherTok{\textless{}{-}} \FunctionTok{optFederov}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{prod}\SpecialCharTok{+}\NormalTok{judge}\SpecialCharTok{+}\NormalTok{rank, }\AttributeTok{data=}\FunctionTok{rbind}\NormalTok{(tab,add), }
                           \AttributeTok{augment=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{nTrials=}\NormalTok{(nbJ}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{nbP,}
                           \AttributeTok{rows=}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(nbJ}\SpecialCharTok{*}\NormalTok{nbP), }\AttributeTok{nRepeats =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{BIBD}{%
\subsection{Balanced incomplete block designs (BIBD)}\label{BIBD}}

Sensory and consumer scientists may sometimes consider using incomplete designs, i.e.~experiments in which each judge evaluates only a subset of the complete product set \citep{Wakeling1995}. In this case, the number of samples evaluated by each judge remains constant but is lower than the total number of products included in the study.

You might want to choose this approach for example if you want to reduce the workload for each panelist and limit sensory fatigue, boredom and inattention. It might also be useful when you cannot ``afford'' a complete design because of sample-related constraints (limited production capacity, very expensive samples, etc.). The challenge then, is to balance sample evaluation across the panel as well as the context (i.e.~other samples) in which each sample is being evaluated. For such a design you thus want each pair of products to be evaluated together the same number of times.

The \texttt{optimaldesign()} function of \texttt{\{SensoMineR\}} can be used to search for a Balanced Incomplete Block Design (BIBD). For instance, let's imagine that 10 panelists are evaluating 3 out of 5 possible samples. The design can be defined as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{incompDesign1 }\OtherTok{\textless{}{-}} \FunctionTok{optimaldesign}\NormalTok{(}\AttributeTok{nbPanelist=}\DecValTok{10}\NormalTok{, }\AttributeTok{nbProd=}\DecValTok{5}\NormalTok{, }\AttributeTok{nbProdByPanelist=}\DecValTok{3}\NormalTok{)}
\NormalTok{incompDesign1}\SpecialCharTok{$}\NormalTok{design}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Rank 1 Rank 2 Rank 3
## Panelist 1       3      5      2
## Panelist 2       5      1      3
## Panelist 3       1      4      3
## Panelist 4       3      4      2
## Panelist 5       2      3      1
## Panelist 6       4      2      1
## Panelist 7       5      1      4
## Panelist 8       1      2      5
## Panelist 9       2      5      4
## Panelist 10      4      3      5
\end{verbatim}

BIBD are only possible for certain combinations of numbers of treatment (products), numbers of blocks (judges), and block size (number of samples per judge). Note that \texttt{optimaldesign()} will yield a design even if it is not balanced but it will also generate contingency tables allowing you to evaluate the design's orthogonality, and how well balanced are order and carry-over effects.

You can also use the \texttt{\{crossdes\}} package to generate a BIBD with this simple syntax: \texttt{find.BIB(trt,\ b,\ k,\ iter)}, with \texttt{trt} the number of products (here 5), \texttt{b} the number of judges (here 10), \texttt{k} the number of samples per judge (here 3), and \texttt{iter} the number of iteration (30 by default). Furthermore, the \texttt{isGYD()} functions evaluates whether the incomplete design generated is balanced or not. If the design is a BIBD, you may then use \texttt{williams.BIB()} to combine it with a Williams design to balance carry-over effects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(crossdes)}
\NormalTok{incompDesign2 }\OtherTok{\textless{}{-}} \FunctionTok{find.BIB}\NormalTok{(}\AttributeTok{trt=}\DecValTok{5}\NormalTok{, }\AttributeTok{b=}\DecValTok{10}\NormalTok{, }\AttributeTok{k=}\DecValTok{3}\NormalTok{)}
\FunctionTok{isGYD}\NormalTok{(incompDesign2)}
\FunctionTok{williams.BIB}\NormalTok{(incompDesign2)}
\end{Highlighting}
\end{Shaded}

Incomplete balanced designs also have drawbacks. First, from a purely statistical perspective, they are conducive to fewer observations and thus to a lower statistical power. Product and Judge effects are also partially confounded even though the confusion is usually considered as acceptable.

\hypertarget{incomplete-designs-for-hedonic-tests-sensory-informed-designs}{%
\subsection{Incomplete designs for hedonic tests: Sensory informed designs}\label{incomplete-designs-for-hedonic-tests-sensory-informed-designs}}

One may also be tempted to use incomplete balanced block designs for hedonic tests. However, proceeding this way is likely to induce framing bias. Indeed, each participant will only see part of the product set which would affect their frame of reference if the subset of product they evaluate only covers a limited area of the sensory space.

Suppose you are designing a consumer test of chocolate chip cookies in which a majority of cookies are made with milk chocolate while a few cookies are made with dark chocolate chips. If a participant only evaluates samples that have milk chocolate chips, this participant will not know about the existence of dark chocolate and will potentially give very different scores compared to what they would have if they had a full view of the product category.

To reduce the risks incurred by the use of BIBD, an alternative strategy is to use a sensory informed design. Its principle is to allocate each panelist a subset of products that best cover the sensory diversity of the initial product set. Pragmatically, this amounts to maximizing the sensory distance between drawn products \citep{Franczak2015}. Of course, this supposes that one has sensory data to rely on in the first place.

\begin{center}\includegraphics[width=0.9\linewidth]{images/sensory_doe} \end{center}

\hypertarget{product-related-designs}{%
\section{Product-related designs}\label{product-related-designs}}

Because of their contribution to product development, sensory and consumer scientists often deal with DoE other than sensory designs strictly speaking \citep[see for instance][]{gacula2008design}. Sensory-driven product development is indeed very frequent and implies strong interconnection between the measure of sensory responses and the manipulation of product variables (e.g.~ingredients) or process variables (e.g.~cooking parameters) \citep[for a review, see][]{Yu2018}.

In order to get the most of sensory experiments, it is thus essential to ensure that the products or prototypes to be tested will be conducive to sound and conclusive data. First and foremost, as in any experimental science, one wants to avoid confounding effects. In addition to this and to put it more generally, the goal of DoE is to define which trials to run in order to be able to draw reliable conclusions without spending time and resources on unnecessary trials. In other words, one seeks maximum efficiency. This is especially critical in sensory science to limit the number of products to be evaluated and to keep panelists' workload under control.

\hypertarget{factorial-designs}{%
\subsection{Factorial designs}\label{factorial-designs}}

Full factorial designs are of course commonly used and their application is usually straightforward. They won't be detailed here. However, it is worth noting that when the number of factors increases, the corresponding number of trials can quickly become daunting (e.g., \emph{2\textsuperscript{k}} trials for a two-level design with \emph{k} factors). Thus, always in the view of sparing experimental resources, incomplete and fractional designs are frequently used.

Several strategies can be used to define which experiments to conduct \citep[\citet{Lawson2014}, \citet{Rasch2011}]{Dean2017}. One option would be to build an optimal design thanks to the \texttt{\{AlgDesign\}} or the \texttt{\{OptimalDesign\}} packages that calculate experimental designs for D, A and I criteria. An example is given below in the case of a mixture design but would apply to regular factorial designs as well.

\hypertarget{mixture-designs}{%
\subsection{Mixture designs}\label{mixture-designs}}

In many projects (e.g.~in the food industry, in the personal care industry), optimizing a product's formula implies adjusting the proportions of its ingredients. In such cases, the proportions are interdependent (the total sum of all components of a mixture must be 100\%). Therefore, these factors (the proportions) must be treated as mixture components. Mixture designs are usually represented using ternary diagrams.

The \texttt{\{mixexp\}} package offers a very convenient way to do this. In addition to creating the design, \texttt{DesignPoints()} allows to display the corresponding ternary diagram. Below is the example of a simplex-lattice design for 3 components and 3 levels obtained thanks to function \texttt{SLD}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mixexp)}
\NormalTok{mdes }\OtherTok{\textless{}{-}} \FunctionTok{SLD}\NormalTok{(}\AttributeTok{fac=}\DecValTok{3}\NormalTok{, }\AttributeTok{lev=}\DecValTok{3}\NormalTok{)}
\FunctionTok{DesignPoints}\NormalTok{(mdes)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-199-1.pdf}

Suppose that we want to adjust a biscuit recipe to optimize its sensory properties, we can design an experiment in which the proportion of ingredients vary. Let's play with butter, sugar, and flour. All three combined would account for 75\% of the dough recipe and the remaining 25\% would consist of other ingredients that we won't modify here (eggs, milk, chocolate, etc.). Besides, not any amount of these three ingredients would make sense (a biscuit with 75\% of butter is not a biscuit, even in Brittany). We thus need to add constraints (ex: butter varies between 15 and 30\% of this blend, sugar varies between 25 and 40\%, and flour varies between 30 and 50\%). Given this set of constraints (defined by \texttt{uc} for the upper contraints, and \texttt{lc} for the lower constraints), we can use \texttt{mixexp::Xvert} to find the extreme vertices of our design (by also including a edge centroid using \texttt{ndm=1}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mdes2 }\OtherTok{\textless{}{-}} \FunctionTok{Xvert}\NormalTok{(}\AttributeTok{nfac=}\DecValTok{3}\NormalTok{, }\AttributeTok{uc=}\FunctionTok{c}\NormalTok{(.}\DecValTok{30}\NormalTok{, .}\DecValTok{40}\NormalTok{, .}\DecValTok{50}\NormalTok{), }\AttributeTok{lc=}\FunctionTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{30}\NormalTok{), }
               \AttributeTok{ndm =} \DecValTok{1}\NormalTok{, }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), round, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

However, this design implies creating 11 mixtures, which is more than needed to apply a Scheffé quadratic model \citep{Lawson2016}. To reduce the number of mixtures and still allow fitting a quadratic model, we can use the \texttt{optFederov()} function from \texttt{\{AlgDesign\}} to select a D-optimal subset. Here, let's limit to 9 products (\texttt{nTrials=9}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MixBiscuits }\OtherTok{\textless{}{-}} \FunctionTok{optFederov}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{+}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{x3}\SpecialCharTok{+}\NormalTok{x1}\SpecialCharTok{:}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{x1}\SpecialCharTok{:}\NormalTok{x3}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{:}\NormalTok{x3}\SpecialCharTok{+}\NormalTok{x1}\SpecialCharTok{:}\NormalTok{x2}\SpecialCharTok{:}\NormalTok{x3, }
\NormalTok{                          mdes2, }\AttributeTok{nTrials=}\DecValTok{9}\NormalTok{)}
\FunctionTok{DesignPoints}\NormalTok{(MixBiscuits}\SpecialCharTok{$}\NormalTok{design, }\AttributeTok{axislabs =} \FunctionTok{c}\NormalTok{(}\StringTok{"Butter"}\NormalTok{,}\StringTok{"Sugar"}\NormalTok{,}\StringTok{"Flour"}\NormalTok{), }
             \AttributeTok{pseudo =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: the design matrix has more than three columns; the DesignPoints function  
## only plots design points for designs with three mixture components. Component x1 is 
## assumed to to be the first column of the design, x2 the second and x3 the third. Other 
## columns are ignored. Use cornerlabs and axislabs to change variable names in the plot.
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-201-1.pdf}

Once the design is built, it could be desirable to randomize the order in which each sample is being made, to avoid further biases. Suppose that we obtain average liking scores for our 9 biscuits as given in Table \ref{tab:biscuit-mixt} and stored in \texttt{Bmixt}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bmixt }\OtherTok{\textless{}{-}}\NormalTok{ MixBiscuits}\SpecialCharTok{$}\NormalTok{design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Product =}\NormalTok{ LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{], }\AttributeTok{.before=}\NormalTok{x1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{scores =} \FunctionTok{c}\NormalTok{(}\FloatTok{7.5}\NormalTok{, }\FloatTok{5.4}\NormalTok{, }\FloatTok{5.5}\NormalTok{, }\FloatTok{7.0}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{5.8}\NormalTok{, }\FloatTok{6.8}\NormalTok{, }\FloatTok{7.9}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\StringTok{"Butter"}\OtherTok{=}\NormalTok{x1, }\StringTok{"Sugar"}\OtherTok{=}\NormalTok{x2, }\StringTok{"Flour"}\OtherTok{=}\NormalTok{x3, }\StringTok{"Liking"}\OtherTok{=}\NormalTok{scores)}
\end{Highlighting}
\end{Shaded}

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{0pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.69in}|p{0.61in}|p{0.57in}|p{0.57in}|p{0.58in}}

\caption{\textcolor[HTML]{000000}{\fontsize{11}{13}\selectfont{\global\setmainfont{Arial}{Average\ liking\ scores\ obtained\ for\ the\ biscuits\ from\ the\ mixture\ design}}}}\label{tab:biscuit-mixt}\\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Product}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Butter}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Sugar}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Flour}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{Liking}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{A}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.300}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.250}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.450}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7.5}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{B}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.150}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.400}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.450}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5.4}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{C}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.300}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.400}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.300}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5.5}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{D}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.250}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.250}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.500}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7.0}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{E}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.150}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.350}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.500}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6.0}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{F}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.300}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.325}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.375}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{8.0}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{G}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.225}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.400}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.375}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{5.8}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{H}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.200}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.300}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.500}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{6.8}}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.69in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{I}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.61in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.230}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.330}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.57in+0\tabcolsep+0\arrayrulewidth}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{0.440}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.58in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\textcolor[HTML]{000000}{\fontsize{9}{9}\selectfont{\global\setmainfont{Calibri}{7.9}}}} \\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}



\end{longtable}

Once the data are collected we can use the \texttt{mixexp::MixModel()} function to fit a linear model and \texttt{mixexp::ModelPlot()} to draw a contour plot. This simple code would allow to get a contour plot that shows where would be the optimal area for the biscuit formulation.

\begin{quote}
Regardless of the construction of the mixture design, ternary diagrams are easy to plot with packages such as \texttt{\{ggtern\}} or \texttt{\{Ternary\}}. \texttt{\{ggtern\}} is particularly interesting since it builds on \texttt{\{ggplot2\}} and uses the same syntax.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{invisible}\NormalTok{(}
  \FunctionTok{capture.output}\NormalTok{(res }\OtherTok{\textless{}{-}} \FunctionTok{MixModel}\NormalTok{(Bmixt, }\AttributeTok{response=}\StringTok{"Liking"}\NormalTok{, }
                                 \AttributeTok{mixcomps=}\FunctionTok{c}\NormalTok{(}\StringTok{"Butter"}\NormalTok{,}\StringTok{"Sugar"}\NormalTok{,}\StringTok{"Flour"}\NormalTok{), }
                                 \AttributeTok{model=}\DecValTok{4}\NormalTok{)))}

\FunctionTok{ModelPlot}\NormalTok{(}\AttributeTok{model =}\NormalTok{ res,}
          \AttributeTok{dimensions =} \FunctionTok{list}\NormalTok{(}\AttributeTok{x1=}\StringTok{"Butter"}\NormalTok{, }\AttributeTok{x2=}\StringTok{"Sugar"}\NormalTok{, }\AttributeTok{x3=}\StringTok{"Flour"}\NormalTok{),}
          \AttributeTok{lims =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{,}\FloatTok{0.30}\NormalTok{,}\FloatTok{0.25}\NormalTok{,}\FloatTok{0.40}\NormalTok{,}\FloatTok{0.30}\NormalTok{,}\FloatTok{0.50}\NormalTok{), }\AttributeTok{constraints =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{contour =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{cuts =} \DecValTok{12}\NormalTok{, }\AttributeTok{fill =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{pseudo =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{axislabs =} \FunctionTok{c}\NormalTok{(}\StringTok{"Butter"}\NormalTok{, }\StringTok{"Sugar"}\NormalTok{, }\StringTok{"Flour"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-203-1.pdf}

From these data. the optimal biscuit would have 31\% of sugar, 27\% of butter, and 42\% of flour, and would reach a predicted liking score larger than 8.

\hypertarget{screening-designs}{%
\subsection{Screening designs}\label{screening-designs}}

Product development is not a monolithic process and in the early stages of a project it could be extremely useful to use a design of experiment in combination with sensory evaluation to identify most influential factors of interest \citep{Mao2007, Pineau2019}. Factorial and mixture designs belong to the product developers' essential toolkit and could serve this purpose. In practice however, they can only include a relatively limited number of factors. By contrast, fractional factorial designs (aka screening designs) are extremely efficient at dealing with many factors, pending some sacrifices on the estimation of interactions and quadratic effects. If, for example, we want to estimate the effect of 5 factors and assume that three and four-factor interactions are negligible, we can then build a \emph{2\textsuperscript{5-1}} design (of, thus, 16 trials instead of 32) in which main effects are confounded with four-way interactions, and two-factor interactions are confounded with three-factor interactions. This design can be easily obtained with the \texttt{\{FrF2\}} package, with this simple command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(FrF2)}
\FunctionTok{FrF2}\NormalTok{(}\AttributeTok{nruns=}\DecValTok{16}\NormalTok{, }\AttributeTok{nfactors=}\DecValTok{5}\NormalTok{, }\AttributeTok{randomize=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To reduce the number of trials even further, we can go as in the example below with a quarter fraction \emph{2\textsuperscript{k-2}} design, in which each effect that can be estimated is confounded with three other interactions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{FrF2}\NormalTok{(}\AttributeTok{nruns=}\DecValTok{8}\NormalTok{, }\AttributeTok{nfactors=}\DecValTok{5}\NormalTok{, }\AttributeTok{randomize=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Although fractional designs are only scarcely used, studies have shown that they could greatly contribute to sensory-led development of food \citep{Baardseth2005, Modi2008, Rytz2017, Pineau2019} as well as non-food product \citep{Dairou2003}.

For higher number of factors, Plackett-Burman designs are the most commonly used fractional factorial designs. They can be easily obtained with the \texttt{pb()} function of the \texttt{\{FrF2\}} package. For example, \texttt{FrF2::pb(12)} yields a 12 trials design that allows to test the effects of 11 factors.

\hypertarget{sensory-informed-designs}{%
\subsection{Sensory informed designs}\label{sensory-informed-designs}}

Eventually, it is worth mentioning that, in some cases, sensory properties themselves can be used as factors and thus be implemented in a DoE. In this line of thinking, Naes and Nyvold have suggested that working this way would leave more scope for creativity \citep{Naes2004}. Naturally, this implies that product developers have (1) access to the measure of these properties and (2) can control the level of these properties and their interactions. These requirements are rarely met in food development but can be more easily implemented in some non-food applications \citep[see for example][]{Petiot2022}.

A specific applications consists in using the sensory information available to make a selection of a subset of products, as described above.

\hypertarget{execute-1}{%
\section{Execute}\label{execute-1}}

Sir Ronald Fisher famously said in his presidential address to the first Indian statistical congress (1938): \emph{``To consult the statistician after an experiment is finished is often merely to ask him to conduct a post-mortem examination. He can perhaps say what the experiment died of.''}

Hopefully, the sections above would have helped the sensory and consumer scientist designing their experiment in a way that would warrant them relevant and meaningful data that are obtained with maximum efficiency.

Fisher continues: \emph{``To utilise this kind of experience the statistician must be induced to use his imagination, and to foresee in advance the difficulties and uncertainties with which, if they are not foreseen, his investigations will be beset.''}
Fortunately, we can spare the reader some of these imagination efforts and reiterate the fundamental principles of sensory evaluation that should help avoiding major pitfalls\footnote{For a more detailed description of these principles, we refer the reader to comprehensive sensory evaluation textbooks. See for instance \citep{LawlessHeym2010, civille2015, stone2020}}.

\begin{itemize}
\item
  \textbf{Individual evaluation}\\
  Probably the most important requirement for the validity of sensory measurements is to perform individual evaluation. Sensory responses are very easily biased when judges can communicate. When this happens, observations cannot be considered independent which would rule out most statistical tests. Although this principle is generally accepted and correctly applied, some situations may be more challenging in this regard (such as project team meetings, b2b sample demonstration, tasting events, etc.). Individual evaluation is usually ensured by the use of partitioned sensory booths, but it can also be achieved by other means (table-top partitions, curtains, separate tables, separate rooms). There are some cases, in consumer research, where interactions between subjects are allowed or even encouraged because they correspond to real-life situations. But these are exceptions to the rule, and in such cases, observations are to be considered at the group level.
\item
  \textbf{Balanced order effects and treatments}\\
  We already discussed the importance of balancing the evaluation order for first-order and carry-over effects (section \ref{Crossover}). We cannot overstate how necessary this precaution is to get valid data. On top of having to deal with such effects, sensory scientists sometimes want to test how products are perceived (or liked) under different conditions (e.g.~blind vs.~branded, with/without nutritional information, in the lab vs.~at home, etc.). Choice must then be made between a within-group design (in which participants evaluate the products under the different conditions) and a between-group design (in which participants evaluate the product under one condition only). As often in consumer science, there is no perfect experiment and these two options have pros and cons. For instance, the within-group design would be more powerful and would allow data analysis at the individual level, but it would be more likely to induce response biases. Note that in both cases, participants must be randomly assigned to one group (corresponding either to a given condition, or to the order in which each condition is being experienced if the study follows a within-group design).
\item
  \textbf{Blind evaluation and controlled evaluation conditions}\\
  The primary goal of most sensory tests is to measure panelists' responses based on sensory properties only, without the interference of other variables that are seen as sources of potential biases.
  For this reason, tests are most frequently conducted on a blind-labeled basis without any information regarding the samples being tested (product identity, brand, price, nutritional facts, claims, etc.). Samples are thus usually blind labeled with random three-digit codes. This way, focus is placed on sensory perception and not on memory or expectations. Even information about the presence of duplicates or about the total number of products included in the design could induce biases. However it is not always possible to hide all information (for example when the brand is printed directly on the product). It should also be noted that information is sometimes included as part of the study design to precisely evaluate the effect of that information. Besides, when sensory evaluation is used for market research goals, evaluation of the full mix can be preferred.\\
  Along the same lines and always in the view of collecting accurate and repeatable data, sensory scientists strive to control evaluation conditions. Sensory booths serve this purpose as they allow individual evaluation under controlled and standardized conditions. Nevertheless, for consumer tests (especially for hedonic tests), researchers may value the role of context in judgement construction and decision making, and thus seek to contextualize their experimental setup for gains in ecological validity \citep{GalinanesPlaza2019}.\footnote{Sensory and consumer research facilities such as living labs or immersive spaces are used in efforts to better account for the role of context without compromising on control. For more information on this topic, see \citep{meiselman2019}}
\item
  \textbf{Separate affective from analytical tasks}\\
  For sensory evaluation, a clear distinction is usually made between analytical measurements (whereby emphasis is placed on description of sample characteristics or on differences and similarities between samples) and affective measurements (whereby focus is placed on liking, preferences, and emotions that may derive from the consumption of a product). Because the tasks involved in these two types of measurements are very different, the general recommendation is to conduct them separately (and most often, with different people). Proceeding otherwise would risk inducing cognitive biases and collecting skewed - or even meaningless - data.
  For example, if the goal of a study is to measure how much consumers like a given set of food products, it wouldn't make sense to ask trained panelists to rate their liking for the products they have been trained to describe. They can certainly do it, but their judgement of the products is likely to be changed by that training and by their extensive exposure to the product. Therefore, they can no longer be considered \emph{normal} consumers. This is relatively commonsense. However, the risk of biases can sometimes be more subtle. Indeed, it might be tempting to ask consumers to give their liking for samples and, within the same session, to describe the same samples for a number of attributes. By doing so, you risk changing participants' mindset (e.g.~by over-focusing on specific attributes) and thus altering liking scores \citep{Popper2004, Prescott2011}. There is much debate though about which type of descriptive tasks would actually lead to biased responses \citep{Jaeger2015}.
  With this in mind, experimenters might still consider conducting combined measurements for product optimization, especially to get rough estimates of product specifications to target in the first stage of product development. In this objective, Just-about-right (JAR) scales or the Ideal Profile Method (IPM) are very popular tasks. They provide a very direct way to optimize products' sensory characteristics \citep{rothman2009, Worch2013}.
  Alternately, one might expect that `untrained' consumers cannot be used for descriptive analysis. However in the past few decades, the development of descriptive methods that do not require training and that can be achieved in a single session has made consumer-based descriptive analysis possible, reliable, and accepted \citep{Varela2012, ares2017, Pineau2022}.
\item
  \textbf{Sample availability}\\
  An obvious, but essential, condition for conducting sensory evaluation, is to have samples available for testing. It is surprising to see how many sensory studies fail simply because the experimenters have not anticipated the production of experimental samples in sufficient quantities or procurement of commercial products. Especially, remember that for many sensory tests, samples are needed for training in addition to the evaluation itself. No data analysis can make up for a lack of samples, no matter how sophisticated it may be. We therefore strongly advise experimenters to review their need for samples when they design a study and, if they do not make the samples themselves, to discuss with their clients or project teams (R\&D, pilot plant, suppliers, etc.) to ensure that samples will be available over the course of the study.
\end{itemize}

\begin{itemize}
\item
  \textbf{Regulations for studies with human subjects}
  Running a sensory or a consumer study implies working with human subjects at some point (online surveys and simple passive observation count!). Therefore, experimenters must ensure that their protocol complies with local and international rules. Most often, research projects should be approved by an Institutional Review Board (IRB) or an appropriate ethical committee. As far as data are concerned, it is also important to ensure that data collection, use, and storage comply with applicable regulations such as EU's General Data Protection Regulation (GDPR), or the California Consumer Privacy Act (CCPA)).
\item
  \textbf{Quantification}\\
  Finally, it is critical that sensory and consumer scientists anticipate what type of analysis they will conduct in accordance with the exact information they are looking for and thus define what data type and scaling method they will adopt \citep[see][ and \citet{LawlessHeym2010}]{omahony1986}. The means of quantification (counts, sorting, ranking, scaling, mapping, reaction time, etc.) has usually been set long before execution, when the study was designed. When time comes to run the tests, the experimenter will have to rely on a proper and reliable way to collect data. Nowadays, commercial sensory software solutions allow to collect any type of data, including temporal information. However, in some cases, the experimenter may choose to ask panelists to use paper and pencil, or just to give a verbal answer, or to arrange the samples physically on a bench. Care must then be taken to ensure proper coding scheme and data entry. At this stage, it is important to keep as much information as possible on the experimental details, such as who evaluated which sample, in which order, at what time, etc. It is usually advised to try to enter data in a single spreadsheet with one column per variable and one line per observation, but in some rare cases, it might be more convenient to enter each panelist's data in separate tabs. This could be the case, for example for methods like free sorting or napping. Note that entering data is prone to mistakes and typos, especially when entered manually into a spreadsheet. In the next sections we will see how to import data from that spreadsheet into R (Section \ref{data-import}) and how to check for outliers and missing values (Section \ref{data-prep}).
\end{itemize}

\hypertarget{data-import}{%
\section{Import}\label{data-import}}

It is a truism, but to analyze data we first need \emph{data}. If this data is already available in R, then the analysis can be performed directly. However, in most cases, the data is stored outside the R environment, and needs to be imported.

In practice, the data might be stored in as many format as one can imagine, whether it ends up being a fairly common solution (\emph{.txt} file, \emph{.csv} file, or \emph{.xls(x)} file), or software specific (e.g.~Stata, SPSS, etc.).

Since it is very common to store the data in Excel spreadsheets (\emph{.xls(x)}) due to its simplicity, the emphasis is on this solution. Fortunately, most generalities presented for Excel files also apply to other formats through \texttt{base::read.table()} for \emph{.txt} files, \texttt{base::read.csv()} and \texttt{base::read.csv2()} for \emph{.csv} files, or through the \texttt{\{read\}} package (which is part of the \texttt{\{tidyverse\}}).

For other (less common) formats, you may find alternative packages that would allow importing your files in R. Particular interest can be given to the package \texttt{\{rio\}} (\emph{rio} stands for \emph{R} \emph{I}nput and \emph{O}utput) which provides an easy solution that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Handles a large variety of files,
\item
  Guess the type of file it is,
\item
  Provides tools to import, export, and convert almost any type of data format, including \emph{.csv}, \emph{.xls(x)}, or data from other statistical software such as SAS (\emph{.sas7bdat} and \emph{.xpt}), SPSS (\emph{.sav} and \emph{.por}), or Stata (\emph{.dta}).
\end{enumerate}

Similarly, the package \texttt{\{foreign\}} provides functions that allow importing data stored from other statistical software (incl.~Minitab, S, SAS, Stata, SPSS, etc.).

Although Excel is most likely one of the most popular way of storing data, there are no \texttt{\{base\}} functions that allow importing such files directly. Fortunately, many packages have been developed for that purpose, including \texttt{\{XLConnect\}}, \texttt{\{xlsx\}}, \texttt{\{gdata\}}, and \texttt{\{readxl\}}. Due to its convenience and speed of execution, we will focus on \texttt{\{readxl\}}.

\hypertarget{importing-structured-excel-file}{%
\subsection{Importing Structured Excel File}\label{importing-structured-excel-file}}

First, let's import the \emph{biscuits\_sensory\_profile.xlsx} workbook using \texttt{readxl::read\_xlsx()} by informing as parameter the \texttt{location} of the file and the \texttt{sheet} where it is stored. For convenience, we are using the \texttt{\{here\}}\footnote{The package \texttt{\{here\}} is very handy as it provides an easy way to retrieve your file's path (within your working directory) by simply giving the name of the file and folder in which they are stored in.} package to retrieve the path of the file (stored in \texttt{file\_path}).

This file is called \emph{structured} as all the relevant information is already stored in the same sheet in a structured way. In other words, no decoding is required here, and there are no `unexpected' rows or columns (e.g.~empty lines, or lines with additional information regarding the data that is not data):

\begin{itemize}
\tightlist
\item
  The first row within the \emph{Data} sheet of \emph{biscuits\_sensory\_profile.xlsx} contains the headers;
\item
  From the second row onward, only data is being stored.
\end{itemize}

Since this data will be used for some analyses, it is assigned data to an R object called \texttt{sensory}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{) }

\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{sensory }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To ensure that the importation went well, it is advised to print \texttt{sensory} after importation. Since \texttt{\{readxl\}} has been developed by Hadley Wickham and colleagues, its functions follow the \texttt{\{tidyverse\}} principles and the data thus imported is stored in a \texttt{tibble}. Let's take advantage of the printing properties of a \texttt{tibble} to evaluate \texttt{sensory}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01      52.8      30      22.8     9.6    22.8
## 2 J01   P02      48.6      30      13.2    10.8    13.2
## 3 J01   P03      48        45.6    17.4     7.8    14.4
## 4 J01   P04      46.2      45.6    37.8     0      48.6
## # ... with 95 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

\texttt{sensory} is a tibble with 99 rows and 35 columns that includes the \texttt{Judge} information (first column, defined as character), the \texttt{Product} information (second column, defined as character), and the sensory attributes (third column onward, defined as numerical or \texttt{dbl}).

\hypertarget{importing-unstructured-excel-file}{%
\subsection{Importing Unstructured Excel File}\label{importing-unstructured-excel-file}}

In some cases, the data are not so well organized/structured, and may need to be \emph{decoded}. This is the case for the workbook entitled \emph{biscuits\_traits.xlsx}.

In this file:

\begin{itemize}
\tightlist
\item
  The variables' name have been coded and their corresponding names (together with some other valuable information we will be using in Section \ref{data-analysis}) are stored in a different sheet entitled \emph{Variables};
\item
  The different levels of each variable (including their code and corresponding names) are stored in another sheet entitled \emph{Levels}.
\end{itemize}

To import and decode this data set, multiple steps are required:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import the variables' name only;
\item
  Import the information regarding the levels;
\item
  Import the data without the first line of header, but by providing the correct names (obtained in the step 1.);
\item
  Decode each question (when needed) by replacing the numerical code by their corresponding labels.
\end{enumerate}

Let's start with importing the variables' names from \emph{biscuits\_traits.xlsx} (sheet \emph{Variables})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_traits.xlsx"}\NormalTok{) }
\NormalTok{var\_names }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Variables"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 62 x 5
##   Code  Name        Direction Value `Full Question`
##   <chr> <chr>       <chr>     <dbl> <chr>          
## 1 Q1    Living area <NA>         NA <NA>           
## 2 Q2    Housing     <NA>         NA <NA>           
## 3 Q3    Judge       <NA>         NA <NA>           
## 4 Q4    Height      <NA>         NA <NA>           
## # ... with 58 more rows
\end{verbatim}

In a similar way, let's import the information related to the levels of each variable, stored in the \emph{Levels} sheet.
A deeper look at the \emph{Levels} sheet shows that only the coded names of the variables are available. In order to include the final names, \texttt{var\_names} is joined (using \texttt{inner\_join}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_labels }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Levels"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(var\_names, Code, Name), }
             \AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\AttributeTok{Question=}\StringTok{"Code"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 172 x 4
##   Question  Code Levels      Name       
##   <chr>    <dbl> <chr>       <chr>      
## 1 Q1           1 Urban Area  Living area
## 2 Q1           2 Rurban Area Living area
## 3 Q1           3 Rural Area  Living area
## 4 Q2           1 Apartment   Housing    
## # ... with 168 more rows
\end{verbatim}

Ultimately, the data (\emph{Data}) is imported by substituting the coded names with their corresponding names. This process can be done by skipping reading the first row of the data that contains the coded header (\texttt{skip=1}), and by passing \texttt{Var\_names} as header or column names (after ensuring that the names' sequence perfectly match across the two tables!).

Alternatively, you can import the data by specifying the range in which the data is being stored (here `range=``A2:BJ108''``).

The data has now the right headers, however each variable is still coded numerically. This step to convert the numerical values with their corresponding labels is described in Section \ref{data-prep}.

\begin{quote}
It can happen that the data include extra information regarding the levels of a factor as sub-header. In such case, a similar approach should be used:
1. Start with importing the first \emph{n} rows of the data that contain this information using the parameter \texttt{n\_max} from `readxl::read\_xlsx``.
2. From this subset, extract the column names.
3. For each variable (when information is available), store the additional information as a list of tables that contains the code and their corresponding label.
4. Re-import the data by skipping these \emph{n} rows, and by manually informing the headers.
\end{quote}

\hypertarget{import-mult-sheet}{%
\subsection{Importing Data Stored in Multiple Sheets}\label{import-mult-sheet}}

It can happen that the data that need to be analyzed is stored in different files, or in different sheets within the same file. Such situation could happen if the same test involving the same samples is repeated over time, or has been run simultaneously in different locations, or simply for convenience for the person who manually collected the data.

Since the goal here is to highlight the possibilities in R to handle such situations, we propose to use a small fake example where 12 panelists evaluated 2 samples on 3 attributes in 3 sessions, each session being stored in a different sheet in \emph{excel\_scrap.xlsx}.

A first approach to tackle this problem could be to import each file separately, and to combine them together using the \texttt{bind\_rows()} function from the \texttt{\{dplyr\}} package. However, this solution is not optimal since it is very tedious when a larger number of sheets is involved, and it is not automated since the code will no longer run (or be incomplete) when the number of session changes.

Instead, we prefer to fully automate the importation. To do so, let's first introduce \texttt{excel\_sheets()} from \texttt{\{readxl\}}: this function provides the name of all the sheets that are available in the file of interest in a list. Then, through \texttt{map()} from the \texttt{\{purrr\}} package, we apply \texttt{read\_xlsx()} to all the elements one by one of obtained with \texttt{excel\_sheets()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path }\OtherTok{\textless{}{-}} \FunctionTok{file.path}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"excel\_scrap.xlsx"}\NormalTok{)}
\NormalTok{files }\OtherTok{\textless{}{-}}\NormalTok{ path }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{excel\_sheets}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_names}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{map}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(path, }\AttributeTok{sheet =}\NormalTok{ .))}
\end{Highlighting}
\end{Shaded}

As can be seen, this procedure creates a list of tables, with as many elements are there are sheets in the excel file.

\begin{quote}
As an alternative, consider using \texttt{import\_list()} from \texttt{\{rio\}} as it imports automatically all the sheets from a spreadsheet with one single command.
\end{quote}

To convert this list of data tables into one unique data frame, we first extend the previous code and \texttt{enframe()} it by informing that the separation was based on \texttt{Session}. Once done, the data (stored in \texttt{data}) is still nested in a list, and should be \emph{unfolded}. Such operation is done with the \texttt{unnest()} function from \texttt{\{tidyr\}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{files }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{enframe}\NormalTok{(}\AttributeTok{name =} \StringTok{"Session"}\NormalTok{, }\AttributeTok{value =} \StringTok{"data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 72 x 6
##   Session   Subject Sample Sweet  Sour Bitter
##   <chr>     <chr>   <chr>  <dbl> <dbl>  <dbl>
## 1 Session 1 J1      P1     46.6   82.6   25.5
## 2 Session 1 J2      P1      1.28  60.1   13.9
## 3 Session 1 J3      P1     29.1   48.5   62.8
## 4 Session 1 J4      P1     29.9   79.2   52.7
## # ... with 68 more rows
\end{verbatim}

This procedure finally returns a tibble with 72 rows and 6 columns, ready to be analyzed!

\begin{quote}
Few additional remarks regarding the last set of code:
1. Instead of \texttt{enframe()}, we could have used \texttt{reduce()} from \texttt{\{purrr\}}, or \texttt{map()} combined with \texttt{bind\_rows()}. However, both these solutions have the drawbacks that the information regarding the \texttt{Session} would be lost since it is not part of the data set itself.
2. The functions \texttt{enframe()} and \texttt{unnest()} have their alter-ego in \texttt{deframe()} and \texttt{nest()} which aim in transforming a data frame into a list of tables, and in nesting data by creating a list-column of data frames.
3. In case the different sets of data are stored in different excel files (rather than different sheets within a file), we could apply a similar procedure by using \texttt{list.files()} (instead of \texttt{excel\_sheets()}) from the \texttt{\{base\}} package, together with \texttt{pattern\ =\ "xlsx"} to limit the search to Excel files present in a pre-defined folder. Such solution becomes handy when many similarly structured files are stored in the same folder and need to be combined.
\end{quote}

\hypertarget{data-prep}{%
\chapter{Data Preparation}\label{data-prep}}

\begin{quote}
After importing the data, the next crucial step is to ensure that the data as it is now available is of good quality and is the correct representation of reality. As an example, during importation, software (such as R) tends to guess (from reading the file) the nature of each variable. If such guess is correct in 99\% of the case, there are situations in which it is erroneous, and ignoring such error can have huge consequences on the final results and conclusions. The goal of this section is hence to perform some pre-check of the data and to prepare them for future analyses.
\end{quote}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\emph{Data Preparation}, which consists of \emph{data inspection} and \emph{data cleaning}, is a critical step before any further \emph{Data Manipulation} or \emph{Data Analysis}. Having a good data preparation procedure ensures a good understanding of the data, and avoids what could be very critical mistakes.

To illustrate the importance of the later point, let's imagine a study in which the samples are defined by their 3-digits code.
During importation, R would recognize them as number, and hence defines the \emph{Product} column as numerical. Without inspection and correction, any ANOVA that include the product effect would be replaced by a linear regression (or analysis of covariance) which of course does not provide the results required (although the analysis would run without error). Worst, if this procedure is automated and the \emph{p-value} associated to the product effect is extracted, the conclusions would rely on the wrong analysis! A good data preparation procedure is hence important to avoid such unexpected results.

So what consists of data preparation, and how does that differ from data manipulation?
There is clearly a thin line between data preparation (and particularly \emph{data cleaning}) and data manipulation, as both these steps share many procedures in common (same applies to data manipulation and data analysis for instance). Although multiple definitions of each step exist, we decided to follow the following rule:

\emph{Data Preparation} includes all the required steps to ensure that the data is matching its intrinsic nature. These steps include inspecting the data at hand (usually through simple descriptive statistics of the data as a whole) and cleaning the data by eventually correcting importation errors (including the imputation of missing data). Although some descriptive statistics are being produced for data inspection, these analyses have no interpretation value besides ensuring that the data are in the right range, or following the right distribution. For instance, with our sensory data, we would ensure that all our sensory scores are included between 0 and 100 (negative scores would not be permitted), but we would not look at the mean or the distribution of the score per product which would belong to data analyses as it would often lead to interpretation (e.g.~P01 is sweeter than P02).

The \emph{Data Manipulation} is an optional step that adjust or convert the data into a structure that is usable for further analysis. This of course may lead to \emph{interpretation} of the results as it may involve some analyses.

The \emph{Data Analysis} step ultimately converts the data into results (through values, graphics, tables, etc.) that provide more insights (through interpretation) about the data.

The data used in this chapter corresponds to the \emph{biscuits\_sensory\_profile.xlsx} that you already imported in Section \ref{data-collection} but with few missing values. This new data set is stored in \emph{biscuits\_sensory\_profile\_with\_NA.xlsx}.

As usual, we start this chapter by loading the main packages we need and by importing this data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readxl)}
\FunctionTok{library}\NormalTok{(here)}

\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"biscuits\_sensory\_profile\_with\_NA.xlsx"}\NormalTok{)}
\NormalTok{sensory }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{inspect}{%
\section{Inspect}\label{inspect}}

\hypertarget{data-inspection}{%
\subsection{Data Inspection}\label{data-inspection}}

To inspect the data, different steps can be used.
First, since \texttt{read\_xlsx()} returns a tibble, let's take advantage of its printing properties to get a fill of the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01      52.8      30      22.8     9.6    22.8
## 2 J01   P02      48.6      30      13.2    10.8    13.2
## 3 J01   P03      48        45.6    17.4     7.8    14.4
## 4 J01   P04      46.2      45.6    37.8     0      48.6
## # ... with 95 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

Other informative solutions consists in printing a summary of the data through the \texttt{summary()} or \texttt{glimpse()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Judge             Product              Shiny     
##  Length:99          Length:99          Min.   : 0.0  
##  Class :character   Class :character   1st Qu.: 9.3  
##  Mode  :character   Mode  :character   Median :21.0  
##                                        Mean   :23.9  
##                                        3rd Qu.:38.4  
##                                        Max.   :54.0  
##  External color intensity Color evenness
##  Min.   : 6.6             Min.   : 6.6  
##  1st Qu.:27.0             1st Qu.:19.5  
##  Median :34.8             Median :26.4  
##  Mean   :33.7             Mean   :28.2  
##  3rd Qu.:42.6             3rd Qu.:37.2  
##  Max.   :55.2             Max.   :53.4  
##  Qty of inclusions
##  Min.   : 0.0     
##  1st Qu.:13.8     
##  Median :19.8     
##  Mean   :20.6     
##  3rd Qu.:29.1     
##  Max.   :40.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 99
## Columns: 10
## $ Judge                      <chr> "J01", "J01", "J01~
## $ Product                    <chr> "P01", "P02", "P03~
## $ Shiny                      <dbl> 52.8, 48.6, 48.0, ~
## $ `External color intensity` <dbl> 30.0, 30.0, 45.6, ~
## $ `Color evenness`           <dbl> 22.8, 13.2, 17.4, ~
## $ `Qty of inclusions`        <dbl> 9.6, 10.8, 7.8, 0.~
## $ `Surface defects`          <dbl> 22.8, 13.2, 14.4, ~
## $ `Print quality`            <dbl> 48.6, 54.0, 49.2, ~
## $ Thickness                  <dbl> 38.4, 35.4, 25.8, ~
## $ `Color contrast`           <dbl> 37.8, 40.2, 17.4, ~
\end{verbatim}

These functions provide relevant yet basic views of each variable present in the data including their types, the range of values, means, and medians, as well as the first values of each variables.

Such view might be sufficient for some first conclusions (e.g.~Are my panelists considered as numerical or nominal data? Do I have missing values?), yet it is not sufficient to fully ensure that the data is ready for analysis. For the latter, more extensive analyses can be performed automatically in different ways. These analyses include looking at the distribution of some variables, or the frequencies of character levels.

A first solution comes from the \texttt{\{skimr\}} package and its \texttt{skim()} function. By applying it to data, an automated extended summary is directly printed on screen by separating \texttt{character} type variables from \texttt{numeric} type variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{skim}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

Another approach consists in generating automatically an html report with some pre-defined analyses using \texttt{create\_report()} from the \texttt{\{DataExplorer\}} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DataExplorer)}
\FunctionTok{create\_report}\NormalTok{(sensory)}
\end{Highlighting}
\end{Shaded}

Unless specified otherwise through \texttt{output\_file}, \texttt{output\_dir}, and \texttt{output\_format}, the report will be saved as an html file on your active directory as \emph{report.html}. This report provides many statistics on your data, including some simple statistics (e.g.~raw counts, percentages), informs you on the structure of your data, as well as on eventual missing data. It also generates graphics to describe your variables (e.g.~univariate distribution, correlation and PCA).

\begin{quote}
Note that the analyses performed to build this report can be called directly within R. For instance, \texttt{introduce()} and \texttt{plot\_intro()} generates the first part of the report, whereas \texttt{plot\_missing()} and \texttt{profile\_missing()} provide information regarding missing data just to name those.
\end{quote}

\hypertarget{missing-data}{%
\subsection{Missing Data}\label{missing-data}}

In the previous section on \ref{data-inspection}, it can be seen that the data set contain missing values. It concerns for instance the attribute \texttt{Light}, for which one missing value has been detected. There are different ways in which we can handle such missing values. But first, let's try to find out where these missing values are, and which impact they may have on our analyses (are they structured or unstructured, etc.)

\hypertarget{visualization-of-missing-values}{%
\subsubsection{Visualization of Missing Values}\label{visualization-of-missing-values}}

A first approach to inspect and visualize where the missing values are is by representing them visually. To do so, the \texttt{\{visdat\}} package provides a neat solution as it represents graphically the data by highlighting where missing values are located. Such visual representation is obtained using the \texttt{vis\_miss()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visdat)}
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vis\_miss}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-220-1.pdf}

As can be seen, missing values are only present in few variables. However, \texttt{Sour} contains up to 10\% of missing data, which can be quite critical in some situations.

If we would want to dig deeper and assess for which products (say) data are missing, we could re-create the same plots per product. The following code would generate that for you:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{map}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(data)\{}
    \FunctionTok{vis\_miss}\NormalTok{(data)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

Of course, such approach could also be applied per panelist for instance.

Once we've investigated where the missing values are located, we can go further by understanding if there are some sorts of relationship between missing values. In other words, are the missing values random? Or are they somewhat structured?
To answer these questions, the \texttt{\{naniar\}} package provides an interesting function called \texttt{gg\_miss\_upset()} which studies the relationship between missing values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gg\_miss\_upset}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-222-1.pdf}

It seems here that the only connection between NAs is observed between \texttt{Light} and \texttt{Color\ contrast}.

Such relational structure can also be visualized in a scatter plot using the \texttt{geom\_miss\_point()} function from the same package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(sensory, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Product, }\AttributeTok{y=}\NormalTok{Sour))}\SpecialCharTok{+}
  \FunctionTok{geom\_miss\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-223-1.pdf}

Here, the relationship between \texttt{Product} and \texttt{Sour} is shown. Such plot may help decide what to do with missing values, whether it is ignoring, removing, or predicting them.

\hypertarget{ignoring-missing-values}{%
\subsubsection{Ignoring Missing Values}\label{ignoring-missing-values}}

A first solution to handle missing values is to simply \emph{ignore} them, as many analyses handle them well. For instance, an ANOVA could be run for such attribute, and results are being produced:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(}\FunctionTok{aov}\NormalTok{(Light }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data=}\NormalTok{sensory))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 6
##   term         df sumsq meansq statistic  p.value
##   <chr>     <dbl> <dbl>  <dbl>     <dbl>    <dbl>
## 1 Product      10 2379.  238.       4.73  2.71e-5
## 2 Judge         8 4100.  513.      10.2   1.16e-9
## 3 Residuals    79 3975.   50.3     NA    NA
\end{verbatim}

This solution may work fine when the number of missing values is small, but be aware that it can also provide erroneous results in case they are not handled the way the analyst is expecting them to be handled.

For some other analyses, \emph{ignoring} the presence of missing values may simply provide unwanted results. To illustrate this, let's compute the simple mean per product for \texttt{Light}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Light =} \FunctionTok{mean}\NormalTok{(Light)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 2
##   Product Light
##   <chr>   <dbl>
## 1 P01      29.6
## 2 P02      30.9
## 3 P03      28.3
## 4 P04      NA  
## # ... with 7 more rows
\end{verbatim}

As can be seen, since \texttt{P04} contains the missing value, its corresponding mean is defined as \texttt{NA}.

\hypertarget{removing-missing-values}{%
\subsubsection{Removing Missing Values}\label{removing-missing-values}}

To enforce the mean to be computed, we need to inform R to remove any missing values beforehand. Such procedure can be done manually by simply filtering out any missing data (here for \texttt{Sour}) before running the analysis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Sour))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 89 x 34
##   Judge Product Shiny Externa~1 Color~2 Qty o~3 Surfa~4
##   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
## 1 J01   P01      52.8      30      22.8     9.6    22.8
## 2 J01   P02      48.6      30      13.2    10.8    13.2
## 3 J01   P03      48        45.6    17.4     7.8    14.4
## 4 J01   P04      46.2      45.6    37.8     0      48.6
## # ... with 85 more rows, 27 more variables:
## #   `Print quality` <dbl>, Thickness <dbl>,
## #   `Color contrast` <dbl>,
## #   `Overall odor intensity` <dbl>,
## #   `Fatty odor` <dbl>, `Roasted odor` <dbl>,
## #   `Cereal flavor` <dbl>, `RawDough flavor` <dbl>,
## #   `Fatty flavor` <dbl>, `Dairy flavor` <dbl>, ...
\end{verbatim}

However, this latter solution is not always satisfactory as it also deletes real data since the data set went from 99 to 89 rows. This means that for variables that did not have missing values for instance, existing data have been removed.

Hence, we prefer another alternative which consists in removing missing values within the analysis procedure (here \texttt{mean()}) through the parameter \texttt{na.rm=TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Light =} \FunctionTok{mean}\NormalTok{(Light, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 2
##   Product Light
##   <chr>   <dbl>
## 1 P01      29.6
## 2 P02      30.9
## 3 P03      28.3
## 4 P04      40.7
## # ... with 7 more rows
\end{verbatim}

Using \texttt{na.rm=TRUE} is equivalent to removing the missing values from the data before performing the analysis, but only for the variable of interest. A similar approach consists in first rotating (using \texttt{pivot\_longer()}) the data before removing missing values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to=}\StringTok{"Variables"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Scores)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product, Variables) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Means =} \FunctionTok{mean}\NormalTok{(Scores)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Variables, }\AttributeTok{values\_from =}\NormalTok{ Means) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Product, Sour, Light)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Product'. You can
## override using the `.groups` argument.
\end{verbatim}

\begin{verbatim}
## # A tibble: 11 x 3
##   Product  Sour Light
##   <chr>   <dbl> <dbl>
## 1 P01         0  29.6
## 2 P02         0  30.9
## 3 P03         0  28.3
## 4 P04         0  40.7
## # ... with 7 more rows
\end{verbatim}

If this solution seems satisfactory as the means were computed without using \texttt{na.rm=TRUE} for both \texttt{Sour} and \texttt{Light} (who contained missing values), its use is limited since converting the data to its original format (i.e.~performing \texttt{pivot\_wider()} after \texttt{pivot\_longer()} without computing the mean in between) will reintroduce the missing values\footnote{Missing values do not need to be visible to exist: Incomplete designs are a good example showing that although the data do not have empty cells, it does contain a lot of missing data (the samples that were not evaluated by each panelist).}.

It should be noted that removing missing values has the impact of unbalancing the data. By taking the example of \texttt{Light} and \texttt{Sour}, let's print the number of panelist evaluating each product:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }\AttributeTok{names\_to=}\StringTok{"Variables"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Scores),}
\NormalTok{         Variables }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Light"}\NormalTok{,}\StringTok{"Sour"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product, Variables) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Variables, }\AttributeTok{values\_from=}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 3
##   Product Light  Sour
##   <chr>   <int> <int>
## 1 P01         9     8
## 2 P02         9     7
## 3 P03         9     8
## 4 P04         8     8
## # ... with 7 more rows
\end{verbatim}

Here for example, the only missing value detected for \texttt{Light} is related to \texttt{P04}. For \texttt{Sour}, \texttt{P02}, \texttt{P07}, and \texttt{P09} only have 7 observations out of 9.

The solution of \emph{blindly} removing missing values is a solution that you may sometime use. However, it is not the only strategy, and we can consider other approaches that are more in-line with the nature of the data.

Rather than removing the missing values only, we could consider removing blocks of data, whether it is attributes, products, or panelists that present missing data. This solution is particularly handy when tests are performed in multiple sessions and some respondents did not manage to attend them all. It can then be relevant to remove completely those respondents from your data.

The procedure presented below show the procedure on how to remove attributes with missing data, but could easily be adapted to panelists or products:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_long }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to=}\StringTok{"Variables"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Scores"}\NormalTok{)}

\NormalTok{attr\_rmv }\OtherTok{\textless{}{-}}\NormalTok{ sensory\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Scores)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(Variables) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unique}\NormalTok{()}

\NormalTok{sensory\_clean }\OtherTok{\textless{}{-}}\NormalTok{ sensory\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(Variables }\SpecialCharTok{\%in\%}\NormalTok{ attr\_rmv)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Variables, }\AttributeTok{values\_from=}\NormalTok{Scores)}
\end{Highlighting}
\end{Shaded}

This procedure removed the 7 attributes that contained missing values (and stored in \texttt{attr\_rmv}), leading to a table with 99 rows and 29 columns (instead of 36).

\hypertarget{imputing-missing-values}{%
\subsubsection{Imputing Missing Values}\label{imputing-missing-values}}

Rather than removing missing data, another strategy consists in imputing missing values. Here again, many strategies can be considered, starting with replacing them with a fixed value. Such approach is usually not the most suitable one, yet it can be relevant in certain cases. For instance, in a CATA task, missing values are often replaced with 0s (not ticked).

To replace missing values with a fixed value, \texttt{replace\_na()} can be used. When applied to a tibble, this function requires you defining using \texttt{list()} the columns to apply it to, and which values to use (each column being treated separately).

For convenience, let's apply it to \texttt{sensory} by replacing missing values for \texttt{Sour} by the value \texttt{888} and for \texttt{Light} with \texttt{999} (we use these extreme values to track changes more easily):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{replace\_na}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{Sour =} \DecValTok{888}\NormalTok{, }\AttributeTok{Light =} \DecValTok{999}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Sour, Light)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 4
##   Judge Product  Sour Light
##   <chr> <chr>   <dbl> <dbl>
## 1 J01   P01         0  22.8
## 2 J01   P02         0  21  
## 3 J01   P03         0  20.4
## 4 J01   P04         0 999  
## # ... with 95 more rows
\end{verbatim}

When dealing with intensity scale, it is more frequent to replace missing values by the mean score for that product and attribute. When the test is duplicated, the mean provided by the \emph{panelist x product x attribute} combination across the different repetitions available is even preferred as it maintains individual variability within the scores.

Such approach is a 2-steps process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the mean (since we do not have duplicates, we use the mean per product);
\item
  Combine it to the data
\end{enumerate}

For simplicity, \texttt{sensory\_long} is used as starting point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prod\_mean }\OtherTok{\textless{}{-}}\NormalTok{ sensory\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product, Variables) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Mean =} \FunctionTok{mean}\NormalTok{(Scores, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Product'. You can
## override using the `.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sensory\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{full\_join}\NormalTok{(prod\_mean, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Product"}\NormalTok{,}\StringTok{"Variables"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Scores =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Scores), Mean, Scores)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\StringTok{"Mean"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Variables, }\AttributeTok{values\_from=}\NormalTok{Scores) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Sour, Light)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 4
##   Judge Product  Sour Light
##   <chr> <chr>   <dbl> <dbl>
## 1 J01   P01         0  22.8
## 2 J01   P02         0  21  
## 3 J01   P03         0  20.4
## 4 J01   P04         0  40.7
## # ... with 95 more rows
\end{verbatim}

As can be seen, the missing value associated to \texttt{J01} for \texttt{Light} and \texttt{P04} has been replaced by \texttt{40.7}. In fact, any missing values related to \texttt{P04} and \texttt{Light} would automatically be replaced by \texttt{40.7} here. For other products (and other attributes), their respective means would be used.

When the model used to impute missing values is fairly simple (here, replacing by the mean correspond to a simple 1-way ANOVA), the imputation can be done directly through the \texttt{impute\_lm()} function from the \texttt{\{simputation\}} package. To mimic the previous approach, the one-way ANOVA is being used\footnote{It is worth noticing that the individual differences could also be included by simple adding the Judge effect in the model.}. Here, missing data for both \texttt{Sour} and \texttt{Light} are being imputed independently using the same model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(simputation)}

\NormalTok{sensory }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{impute\_lm}\NormalTok{(Sour }\SpecialCharTok{+}\NormalTok{ Light }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Sour, Light)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 4
##   Judge Product  Sour Light
##   <chr> <chr>   <dbl> <dbl>
## 1 J01   P01         0  22.8
## 2 J01   P02         0  21  
## 3 J01   P03         0  20.4
## 4 J01   P04         0  40.7
## # ... with 95 more rows
\end{verbatim}

As can be seen, this procedure provides the same results as before, but in less steps!

In some situations, implementing missing values using such ANOVA (or regression) model can lead to aberrations. It is for instance the case when the imputed values falls outside the scale boundaries. To avoid such situations, \texttt{\{simputation\}} also provides other more advanced alternatives including (amongst others) \texttt{impute\_rf()} which uses random forest to impute the missing values.

Last but not least, imputation of missing values could also be done in a multivariate way, by using the structure of the data (e.g.~correlation) to predict the missing values. This is the approach proposed in the \texttt{\{missMDA\}} package. Since our data are numeric, the imputation is done through PCA with the \texttt{imputePCA()} function. Note that here, the imputed values are stored in the object \texttt{.\$completeObs} (here, \texttt{sensory} is used):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missMDA)}

\FunctionTok{imputePCA}\NormalTok{(sensory, }\AttributeTok{quali.sup=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{method=}\StringTok{"EM"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{completeObs }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, Sour, Light)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 99 x 4
##   Judge Product  Sour Light
##   <chr> <chr>   <dbl> <dbl>
## 1 J01   P01         0  22.8
## 2 J01   P02         0  21  
## 3 J01   P03         0  20.4
## 4 J01   P04         0  33.7
## # ... with 95 more rows
\end{verbatim}

In this case, it can be seen that the missing value for \texttt{J01}\emph{x}\texttt{P04}\emph{x}\texttt{Light} has been replaced by the value \texttt{33.7}.

\hypertarget{limitations}{%
\subsubsection{Limitations}\label{limitations}}

As we have seen, there are different ways to implement missing values, and the different algorithms will likely impute them with different values. Hence, the overall results can be affected, and there is no way to know which solution is the most suitable for our study. Still, it is recommended to treat the missing values, and to chose the right strategy that is the most adapted to the data.

However, since most imputation methods involve modeling, applying them to variables with a high missing values rate can introduce bias in the data. Let's consider a situation in which assessors are evaluating half the product set using a BIB. Hence, half of the data are missing. By imputing the missing values, each prediction is proportionally based on one unique value. And ultimately, any further analyses on this data would be based on half measured and half \emph{fictive} data.

\hypertarget{design-inspection}{%
\subsection{Design Inspection}\label{design-inspection}}

The next point of interest - quite specific to sensory and consumer data - is to ensure that the design is well balanced, and handles correctly the first-order and carry-over effects. This step is particularly important for those who analyze the data but were not involved from the start in that study (and hence were not involved during the test set-up).

Let's show a simple procedure that would check part of the quality of a design. Since our data set stored in \emph{biscuits\_sensory\_profile.xlsx} does not contain any information regarding the experimental design, let's use \texttt{sensochoc} from \texttt{\{SensoMineR\}} instead.

To load (and clean) the data, let's run these lines of code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(SensoMineR)}

\FunctionTok{data}\NormalTok{(chocolates)}

\NormalTok{dataset }\OtherTok{\textless{}{-}}\NormalTok{ sensochoc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(Panelist, Session, Rank, Product), as.character))}
\end{Highlighting}
\end{Shaded}

The data consist of 6 products (\texttt{Product}) evaluated by 29 panelists (\texttt{Panelist}) in duplicates (\texttt{Session}). The presentation order is stored in \texttt{Rank}.

To evaluate whether the products have been equally presented at each position, a simple cross-count between \texttt{Product} and \texttt{Rank} is done. This can be done using the \texttt{xtabs()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Product }\SpecialCharTok{+}\NormalTok{ Rank, }\AttributeTok{data=}\NormalTok{dataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Rank
## Product  1  2  3  4  5  6
##   choc1  9  9 10  9 11 10
##   choc2 11  9  9 11  7 11
##   choc3  9 11 10  9  9 10
##   choc4  9 10  9 10 10 10
##   choc5 11  8 11 10 10  8
##   choc6  9 11  9  9 11  9
\end{verbatim}

Such table can also be obtained using \texttt{group\_by()} and \texttt{count()} to get the results in a tibble:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Rank) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Rank, }\AttributeTok{values\_from=}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

As we can see, the design is not perfectly balanced, as \texttt{choc2} is evaluated 11 times in the 1st, 4th, and 6th position, but only 7 times in the 5th position.

To make sure that the design is well balanced in terms of carry-over effect, we need to count how often each product is tested before each of the other products. Since this information is not directly available in the data, it needs to be added.

Let's start with extracting the information available, i.e.~the order of each product for each panelist and session:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{current }\OtherTok{\textless{}{-}}\NormalTok{ dataset }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Panelist, Product, Session, Rank) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Rank =} \FunctionTok{as.numeric}\NormalTok{(Rank))}
\end{Highlighting}
\end{Shaded}

An easy way to add the \texttt{Previous} product information as a new column in the data is by replacing \texttt{Rank} by \texttt{Rank\ +\ 1} in \texttt{current} (all new positions larger than the number of products are filtered).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{previous }\OtherTok{\textless{}{-}}\NormalTok{ current }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Previous =}\NormalTok{ Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Rank =}\NormalTok{ Rank }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Rank }\SpecialCharTok{\textless{}=} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(dataset}\SpecialCharTok{$}\NormalTok{Product)))}
\end{Highlighting}
\end{Shaded}

This new data is merged to \texttt{current} by \texttt{Panelist}, \texttt{Session}, and \texttt{Rank}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cur\_prev }\OtherTok{\textless{}{-}}\NormalTok{ current }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(previous, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Panelist"}\NormalTok{, }\StringTok{"Session"}\NormalTok{, }\StringTok{"Rank"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

As can be seen, the products that are evaluated first get \texttt{NA} in \texttt{Previous}, and for each rank r (r \textgreater{} 1), \texttt{Previous} gets the product that was evaluated at rank r-1.

To evaluate whether the carry-over effect is well balance, the only thing left to do is cross-count \texttt{Product} and \texttt{Previous} (here, the results are split per \texttt{Session}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cur\_prev }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Session, Product, Previous) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Product =} \FunctionTok{factor}\NormalTok{(Product, }
                          \AttributeTok{levels=}\FunctionTok{paste0}\NormalTok{(}\StringTok{"choc"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)),}
         \AttributeTok{Previous =} \FunctionTok{factor}\NormalTok{(Previous, }
                           \AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\StringTok{"NA"}\NormalTok{,}\FunctionTok{paste0}\NormalTok{(}\StringTok{"choc"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Previous) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Previous, }\AttributeTok{values\_from=}\NormalTok{n, }\AttributeTok{values\_fill=}\DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Session)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $`1`
## # A tibble: 6 x 9
##   Session Product choc1 choc2 choc3 choc4 choc5 choc6
##   <chr>   <fct>   <int> <int> <int> <int> <int> <int>
## 1 1       choc1       0     5     6     4     5     5
## 2 1       choc2       3     0     5     6     4     5
## 3 1       choc3       5     4     0     5     6     5
## 4 1       choc4       5     5     5     0     4     5
## # ... with 2 more rows, and 1 more variable:
## #   `NA` <int>
## 
## $`2`
## # A tibble: 6 x 9
##   Session Product choc1 choc2 choc3 choc4 choc5 choc6
##   <chr>   <fct>   <int> <int> <int> <int> <int> <int>
## 1 2       choc1       0     4     5     5     6     4
## 2 2       choc2       3     0     5     5     6     5
## 3 2       choc3       5     4     0     6     5     4
## 4 2       choc4       5     5     5     0     4     6
## # ... with 2 more rows, and 1 more variable:
## #   `NA` <int>
\end{verbatim}

As expected, the table shows that a product is never evaluated twice in a row (the diagonal contains 0s). Here again, the design is not optimal since \texttt{choc1} has been evaluated 3 times before \texttt{choc2} and 6 times before \texttt{choc5} in the first session.

\begin{quote}
The last column defined as \texttt{NA} refers to the number of time that products did not have a product tested before, in other words that they were evaluated first.
\end{quote}

\hypertarget{clean}{%
\section{Clean}\label{clean}}

As mentioned in the introduction of this chapter, there is a thin line between \emph{Data Inspection} and \emph{Data Manipulation}, as both steps share many common practices. Here, we are limiting ourselves on handling variables and their type. For a full overview, we encourage the readers to look at Section \ref{data-manip} to see other practices on how to handle data.

\hypertarget{handling-data-type}{%
\subsection{Handling Data Type}\label{handling-data-type}}

The data used in this section is stored in \emph{bisuits\_traits.xlsx}. So let's start with importing it in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"biscuits\_traits.xlsx"}\NormalTok{)}

\NormalTok{demo\_var }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Variables"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Code, Name)}

\NormalTok{demo\_lev }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Levels"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Question, Code, Levels) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(demo\_var, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Question"}\OtherTok{=}\StringTok{"Code"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Question)}

\NormalTok{demographic }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet=}\StringTok{"Data"}\NormalTok{, }\AttributeTok{skip=}\DecValTok{1}\NormalTok{, }
                         \AttributeTok{col\_names=}\FunctionTok{unlist}\NormalTok{(demo\_var}\SpecialCharTok{$}\NormalTok{Name))}
\end{Highlighting}
\end{Shaded}

In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another.

Remember that when your data set is stored in a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables' type can be accessed at any moment. In case the data is not in a tibble, the use of the \texttt{str()} function becomes handy as it provides this information (here we limit ourselves to the first 5 columns).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(demographic[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tibble [107 x 5] (S3: tbl_df/tbl/data.frame)
##  $ Living area: num [1:107] 1 1 2 1 1 1 1 1 3 1 ...
##  $ Housing    : num [1:107] 1 1 2 2 1 1 1 1 2 2 ...
##  $ Judge      : chr [1:107] "J48" "J61" "J60" "J97" ...
##  $ Height     : num [1:107] 1.45 1.6 1.62 1.6 1.69 1.62 1.58 1.6 1.56 1.67 ...
##  $ Weight     : num [1:107] 43 65 52 60 70 56 62 55 55 53 ...
\end{verbatim}

In sensory and consumer research, the four most common types are:

\begin{itemize}
\tightlist
\item
  Numerical (incl.~integer {[}\texttt{int}{]}, decimal {[}\texttt{dcl}{]}, and double {[}\texttt{dbl}{]});
\item
  Logical {[}\texttt{lgl}{]};
\item
  Character {[}\texttt{char}{]};
\item
  Factor {[}\texttt{fct}{]}.
\end{itemize}

R still has plenty of other types, for more information please visit: \url{https://tibble.tidyverse.org/articles/types.html}

\hypertarget{numerical-data}{%
\subsubsection{Numerical Data}\label{numerical-data}}

Since a large proportion of the research done is quantitative, it is no surprise that our data are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g.~1, 2, -16, etc.), or decimal value (or double, e.g.~1.6, 2.333333, -3.2 etc.).

By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double.

\hypertarget{binary-data}{%
\subsubsection{Binary Data}\label{binary-data}}

Another common type that seems to be numerical in appearance, but that has additional properties is the binary type.
Binary data is data that takes two possible values (\texttt{TRUE} or \texttt{FALSE}), and are often the results of a \emph{test} (e.g.~is \texttt{x\textgreater{}3}? Or is \texttt{MyVar} numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires.

\begin{quote}
Intrinsically, binary data is \emph{numerical}, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the \texttt{sum()} function, as shown in the simple example below:
\texttt{\#\ Generate\ 10\ random\ values\ between\ 1\ and\ 10\ (uniform\ distribution)}
\texttt{x\ \textless{}-\ runif(10,\ 1,\ 10)}
\texttt{\#\ Test\ whether\ the\ values\ generated\ are\ strictly\ larger\ than\ 5}
\texttt{test\ \textless{}-\ x\textgreater{}5}
\texttt{\#\ Counting\ the\ number\ of\ values\ strictly\ larger\ than\ 5}
\texttt{sum(test)}
\end{quote}

\hypertarget{nominal-data}{%
\subsubsection{Nominal Data}\label{nominal-data}}

Nominal data is any data that are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes. But since the software cannot guess that those numbers are \emph{identifiers} rather than \emph{numbers}, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables is explained in the next section.

For nominal data, two particular types of data are of interest:

\begin{itemize}
\tightlist
\item
  Character or \texttt{char};
\item
  Factor or \texttt{fct}.
\end{itemize}

Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels:

\begin{itemize}
\tightlist
\item
  For \texttt{character}, there are no particular structure, and the variables can take any values (e.g.~open-ended question);
\item
  For \texttt{factor}, the inputs of the variables are structured into \texttt{levels}.
\end{itemize}

To evaluate the number of levels, different procedures are required:

\begin{itemize}
\tightlist
\item
  For \texttt{character}, one should count the number of unique element using \texttt{length()} and \texttt{unique()};
\item
  For \texttt{factor}, the levels and the number of levels are directly provided by \texttt{levels()} and \texttt{nlevels()}.
\end{itemize}

Let's compare a variable set as \texttt{factor} and \texttt{character} by using a simple hand-made example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge\_fct =} \FunctionTok{as.factor}\NormalTok{(Judge))}

\FunctionTok{summary}\NormalTok{(example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Judge             Judge_fct  
##  Length:107         J1     :  1  
##  Class :character   J10    :  1  
##  Mode  :character   J100   :  1  
##                     J101   :  1  
##                     J103   :  1  
##                     J105   :  1  
##                     (Other):101
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#unique(example$Judge)}
\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Judge))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 107
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#levels(example$Judge\_fct)}
\FunctionTok{nlevels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Judge\_fct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 107
\end{verbatim}

\begin{quote}
Although \texttt{Judge} and \texttt{Judge\_fct} look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or for building tables and graphs.
\end{quote}

When set as \texttt{character}, the number of levels of a variable is directly read from the data, and its levels' order matches the way they appear in the data (or sometimes are re-arranged in alphabetical order). This means that any data collected using a structured scale will often lose its natural order.

When set as \texttt{factor}, the factor levels (including their order) are informed, and does not depend necessarily on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. In our case, the levels are read from the data and are reordered alphabetically (note that \texttt{J10} and \texttt{J100} appear before \texttt{J2} for instance.)

To illustrate this, let's re-arrange the levels from \texttt{Judge\_fct} by ordering them numerically in such a way \texttt{J2} follows \texttt{J1} rather than \texttt{J10}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge\_fct =} \FunctionTok{factor}\NormalTok{(Judge, }\FunctionTok{str\_sort}\NormalTok{(Judge, }\AttributeTok{numeric=}\ConstantTok{TRUE}\NormalTok{)))}
\FunctionTok{levels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Judge\_fct)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "J1"  "J2"  "J3"  "J4"  "J5"  "J6"  "J7"  "J8" 
##  [9] "J9"  "J10"
\end{verbatim}

Now the levels are sorted, let's filter respondents by only keeping J1 to J20. We then re-run the previous code that count the number of elements in each variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example\_reduced }\OtherTok{\textless{}{-}}\NormalTok{ example }\SpecialCharTok{\%\textgreater{}\%}  
  \FunctionTok{filter}\NormalTok{(Judge }\SpecialCharTok{\%in\%} \FunctionTok{paste0}\NormalTok{(}\StringTok{"J"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{))}

\CommentTok{\# unique(example\_reduced$Judge)}
\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(example\_reduced}\SpecialCharTok{$}\NormalTok{Judge))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# levels(example\_reduced$Judge\_fct)}
\FunctionTok{nlevels}\NormalTok{(example\_reduced}\SpecialCharTok{$}\NormalTok{Judge\_fct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 107
\end{verbatim}

After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements (\texttt{J18} doesn't exist in the data), whereas the column set as factor still contains the 107 entries (most of them not having any recordings).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example\_reduced }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Judge, }\AttributeTok{.drop=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 19 x 2
##   Judge     n
##   <chr> <int>
## 1 J1        1
## 2 J10       1
## 3 J11       1
## 4 J12       1
## # ... with 15 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example\_reduced }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Judge\_fct, }\AttributeTok{.drop=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 107 x 2
##   Judge_fct     n
##   <fct>     <int>
## 1 J1            1
## 2 J2            1
## 3 J3            1
## 4 J4            1
## # ... with 103 more rows
\end{verbatim}

This property can be seen as an advantage or a disadvantage depending on the situation:

\begin{itemize}
\tightlist
\item
  For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of \texttt{factor}).
\item
  For hypothesis testing (e.g.~ANOVA) on subset of data, the \texttt{Judge} variable set as \texttt{character} would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would still use the original count (so 106 here)!
\end{itemize}

The latter point is particularly critical since the analysis is incorrect and will either return an error or (worse!) return erroneous results!

Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily.
Let's consider the \texttt{Living\ area} variable from \texttt{demographic} as an example. From the original excel file, it can be seen that it has three levels, \texttt{1} corresponding to \emph{urban area}, \texttt{2} to \emph{rurban area}, and \texttt{3} to \emph{rural area}. Let's start by renaming its levels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{=}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Area =} \FunctionTok{factor}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Living area}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }
                       \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Urban"}\NormalTok{, }\StringTok{"Rurban"}\NormalTok{, }\StringTok{"Rural"}\NormalTok{)))}

\FunctionTok{levels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Urban"  "Rurban" "Rural"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nlevels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(example}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Living area}\StringTok{\textasciigrave{}}\NormalTok{, example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     Urban Rurban Rural
##   1    72      0     0
##   2     0     12     0
##   3     0      0    23
\end{verbatim}

As can be seen, the variable \texttt{Area} is the factor version (including its labels) of \texttt{Living\ area}.
Let's now regroup \texttt{Rurban} and \texttt{Rural} together under \texttt{Rural}, and change the order by ensure that \texttt{Rural} appears before \texttt{Urban}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{=}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Area =} \FunctionTok{factor}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Living area}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{), }
                       \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Rural"}\NormalTok{, }\StringTok{"Rural"}\NormalTok{, }\StringTok{"Urban"}\NormalTok{)))}

\FunctionTok{levels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Rural" "Urban"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nlevels}\NormalTok{(example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(example}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Living area}\StringTok{\textasciigrave{}}\NormalTok{, example}\SpecialCharTok{$}\NormalTok{Area)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     Rural Urban
##   1     0    72
##   2    12     0
##   3    23     0
\end{verbatim}

This approach of renaming and re-ordering factor levels is very important as it can simplify the readability of tables and figures.
Some other transformations can be applied to factors thanks to the \texttt{\{forcats\}} package. Particular attention should be given to the following functions:

\begin{itemize}
\tightlist
\item
  \texttt{fct\_reorder()}/\texttt{fct\_reorder2()} and \texttt{fct\_relevel()} reorder the levels of a factor;
\item
  \texttt{fct\_recode()} renames the factor levels (as an alternative to \texttt{factor()} used in the previous example);
\item
  \texttt{fct\_collapse()} and \texttt{fct\_lump()} aggregate different levels together (\texttt{fct\_lump()} regroups automatically all the rare levels);
\item
  \texttt{fct\_inorder()} uses the order read in the data (particularly useful with \texttt{pivot\_longer()} for instance);
\item
  \texttt{fct\_rev()} reverses the order of the levels (particularly useful in graphs).
\end{itemize}

Although it hasn't been done here, manipulating strings is also possible through the \texttt{\{stringr\}} package, which provides interesting functions such as:

\begin{itemize}
\tightlist
\item
  \texttt{str\_to\_upper()}/\texttt{str\_to\_lower()} to convert strings to uppercase or lowercase;
\item
  \texttt{str\_c()}, \texttt{str\_sub()} combine or subset strings;
\item
  \texttt{str\_trim()} and \texttt{str\_squish()} remove white spaces;
\item
  \texttt{str\_extract()}, \texttt{str\_replace()}, \texttt{str\_split()} extract, replace, or split strings or part of the strings;
\item
  \texttt{str\_sort()} to order alphabetically (or by respecting numbers, as shown previously) its elements.
\end{itemize}

Many of these functions will be used later in Section \ref{text-analysis}

\hypertarget{converting-between-types}{%
\subsection{Converting between Types}\label{converting-between-types}}

Since each variable type has its own properties, it is important to be able to switch from one to another if needed. This can be critical (converting from numerical to character or factor and reversely) or purely practical (converting from character to factor and reversely).

In the previous section, we have already seen how to convert from character to factor. Let's now consider two other conversions, namely:

\begin{itemize}
\tightlist
\item
  from numerical to character/factor;
\item
  from character/factor to numerical.
\end{itemize}

The conversion from numerical to character or factor is simply done using \texttt{as.character()} and \texttt{as.factor()} respectively. An example in the use of \texttt{as.character()} and \texttt{as.factor()} was provided in the previous section when we converted the \texttt{Respondent} variables to character and factor. The use of \texttt{factor()} was also used earlier when the variable \texttt{Living\ area} was converted from numerical to factor (called \texttt{Area}) with labels.

\begin{quote}
\texttt{as.factor()} only converts into factors without allowing to chose the order of the levels, nor to rename them. Instead, \texttt{factor()} should be used as it allows specifying the \texttt{levels} (and hence the order of the levels) and their corresponding \texttt{labels}.
\end{quote}

To illustrate the conversion from character to numeric, let's start with creating a tibble with two variables, one containing strings made of numbers, and one containing strings made of text.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Numbers =} \FunctionTok{c}\NormalTok{(}\StringTok{"2"}\NormalTok{,}\StringTok{"4"}\NormalTok{,}\StringTok{"9"}\NormalTok{,}\StringTok{"6"}\NormalTok{,}\StringTok{"8"}\NormalTok{,}\StringTok{"12"}\NormalTok{,}\StringTok{"10"}\NormalTok{),}
                  \AttributeTok{Text =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{,}\StringTok{"Science"}\NormalTok{,}\StringTok{"4"}\NormalTok{,}\StringTok{"Sensory"}\NormalTok{,}
                           \StringTok{"and"}\NormalTok{,}\StringTok{"Consumer"}\NormalTok{,}\StringTok{"Research"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The conversion from character to numerical is straight forward and requires the use of the function \texttt{as.numeric()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NumbersN =} \FunctionTok{as.numeric}\NormalTok{(Numbers), }\AttributeTok{TextN =} \FunctionTok{as.numeric}\NormalTok{(Text))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in mask$eval_all_mutate(quo): NAs introduced by
## coercion
\end{verbatim}

\begin{verbatim}
## # A tibble: 7 x 4
##   Numbers Text    NumbersN TextN
##   <chr>   <chr>      <dbl> <dbl>
## 1 2       Data           2    NA
## 2 4       Science        4    NA
## 3 9       4              9     4
## 4 6       Sensory        6    NA
## # ... with 3 more rows
\end{verbatim}

As can be seen, when strings are made of numbers, the conversion works fine. However, any non-numerical string character cannot be converted and hence returns NAs.

Now let's apply the same principle to a variable of the type factor. To do so, the same example in which the variables are now defined as factor is used:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}}\NormalTok{ example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Numbers =} \FunctionTok{as.factor}\NormalTok{(Numbers)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Text =} \FunctionTok{factor}\NormalTok{(Text, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{,}\StringTok{"Science"}\NormalTok{,}\StringTok{"4"}\NormalTok{,}\StringTok{"Sensory"}\NormalTok{,}\StringTok{"and"}\NormalTok{,}\StringTok{"Consumer"}\NormalTok{,}\StringTok{"Research"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Let's apply as.numeric() to these variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NumbersN =} \FunctionTok{as.numeric}\NormalTok{(Numbers), }\AttributeTok{TextN =} \FunctionTok{as.numeric}\NormalTok{(Text))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 4
##   Numbers Text    NumbersN TextN
##   <fct>   <fct>      <dbl> <dbl>
## 1 2       Data           3     1
## 2 4       Science        4     2
## 3 9       4              7     3
## 4 6       Sensory        5     4
## # ... with 3 more rows
\end{verbatim}

We can notice here that the outcome is not really as expected as the numbers 2-4-9-6-8-12-10 becomes 3-4-7-5-6-2-1, and Data-Science-4-Sensory-and-Consumer-Research becomes 1-2-3-4-5-6-7. The rationale behind this conversion is that the numbers do not reflects the string itself, but the position of that level in the factor level order.

To convert properly numerical factor levels to number, the variable should first be converted into character:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Numbers =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(Numbers)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 2
##   Numbers Text   
##     <dbl> <fct>  
## 1       2 Data   
## 2       4 Science
## 3       9 4      
## 4       6 Sensory
## # ... with 3 more rows
\end{verbatim}

As can be seen, it is very important to verify the type of each variable (and convert if needed) to ensure that the data is processed as it should be. Since each type has its own advantages and drawbacks, it is convenient to regularly transit from one to another. Don't worry, you will get quickly familiarized with this as we will be doing such conversions regularly in the next sections.

\hypertarget{data-analysis}{%
\chapter{Data Analysis}\label{data-analysis}}

\begin{quote}
Although the data-science workflow suggests a clear separation between data manipulation and data analysis, in practice such separation is not that obvious. Indeed, most analyses require data manipulation. In fact, some data transformation can be seen as a part of both data transformation and data analysis. Yet, this Section is somewhat more dedicated to the analysis of data, by 1) presenting how some of the most common analyses in sensory and consumer reserach are performed, 2) integrating the analysis part to your script, and most importantly 3) providing applications and alternatives or extension to all the procedures presented in \ref{data-manip} and in \ref{data-viz}. With that in mind, the emphasis is not on the results and interpretation of the results, but on the path to get such results.
For practical reasons, this chapter is divided in 3 sub-sections, one dedicated to the sensory data, one to the consumer data, and one combining both.
\end{quote}

\hypertarget{sensory-analysis}{%
\section{Sensory Data}\label{sensory-analysis}}

As one may expect, this chapter is mostly built around the \texttt{\{tidyverse\}}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

Let's start with the analysis of our sensory data stored in \emph{biscuits\_sensory\_profile.xlsx}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"biscuits\_sensory\_profile.xlsx"}\NormalTok{)}
\NormalTok{p\_info }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Product Info"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Type)}

\NormalTok{sensory }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Data"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(p\_info, }\AttributeTok{by =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{(Protein}\SpecialCharTok{:}\NormalTok{Fiber, }\AttributeTok{.after =}\NormalTok{ Product)}
\end{Highlighting}
\end{Shaded}

Typically, sensory scientists first seek to determine whether there are differences between samples for the different attributes. This is done through Analysis of Variance (ANOVA) and can be done using the \texttt{lm()} or \texttt{aov()} functions.

Let's start by running the ANOVA for the attribute \texttt{Sweet}. Since the test has not been duplicated, the 2-way ANOVA (including the Product and Assessor effects) without interaction is used. This is done using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sweet\_aov }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sweet }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data =}\NormalTok{ sensory)}
\FunctionTok{anova}\NormalTok{(sweet\_aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: Sweet
##           Df Sum Sq Mean Sq F value  Pr(>F)    
## Product   10   2654     265    7.27 4.5e-08 ***
## Judge      8   4451     556   15.25 2.3e-13 ***
## Residuals 80   2918      36                    
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{quote}
The results provided here by \texttt{anova()} are not handy to manipulate as the output is not stored in a matrix or data frame. Instead, and as illustrated later, we apply the \texttt{tidy()} function from \texttt{\{broom\}} as it tidies the statistical outputs from most testing/modelling functions into a user-friendly tibble.
\end{quote}

We could duplicate this code for each single attribute, but this would be quite tedious for large number of attributes. Moreover, this code is sensitive to the way the variables are named, and hence might not be suitable for other data sets. Instead, we propose two solutions, one using \texttt{split()} combined with \texttt{map()} and one involving \texttt{nest\_by()} to run this analysis automatically.

For both these solutions, the data should be stored in the long and thin form, which can be obtained using \texttt{pivot\_longer()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso\_aov\_data }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From this structure, the first approach consists in splitting the data by attribute. Once done, we run the ANOVA for each subset (the model is then defined as \texttt{Score\ \textasciitilde{}\ Product\ +\ Judge}) automatically using \texttt{map()}\footnote{The \texttt{map()} function applies the same function to each element of a list automatically: It is hence equivalent to a \texttt{for\ ()} loop, but in a neater and more efficient way.}, and we extract the results of interest using the \texttt{\{broom\}} package.

Ultimately, the results can be combined again using \texttt{enframe()} and \texttt{unnest()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso\_aov1 }\OtherTok{\textless{}{-}}\NormalTok{ senso\_aov\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{    res }\OtherTok{\textless{}{-}}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(}\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data =}\NormalTok{ data)))}
    \FunctionTok{return}\NormalTok{(res)}
\NormalTok{  \}) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{enframe}\NormalTok{(}\AttributeTok{name =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{value =} \StringTok{"res"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

The second approach uses the advantage of tibbles and nests the analysis by attribute (meaning the analysis is done for each attribute separately, a bit like \texttt{group\_by()}). In this case, we store the results of the ANOVA in a new variable called \texttt{mod}.

Once the analysis is done, we summarize the info stored in \texttt{mod} by converting it into a tibble using \texttt{\{broom\}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso\_aov2 }\OtherTok{\textless{}{-}}\NormalTok{ senso\_aov\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest\_by}\NormalTok{(Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mod =} \FunctionTok{list}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data =}\NormalTok{ data))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(}\FunctionTok{anova}\NormalTok{(mod))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Attribute'. You
## can override using the `.groups` argument.
\end{verbatim}

The two approaches return the exact same results:

\begin{verbatim}
## # A tibble: 96 x 7
##   Attribute  term     df sumsq meansq stati~1   p.value
##   <chr>      <chr> <int> <dbl>  <dbl>   <dbl>     <dbl>
## 1 Astringent Prod~    10  870.   87.0    1.62  1.16e- 1
## 2 Astringent Judge     8 5041.  630.    11.7   6.94e-11
## 3 Astringent Resi~    80 4302.   53.8   NA    NA       
## 4 Bitter     Prod~    10 1005.  101.     3.95  2.11e- 4
## # ... with 92 more rows, and abbreviated variable name
## #   1: statistic
\end{verbatim}

Let's dig into the results by extracting the attributes that do not show significant differences at 5\%. Since the \texttt{tidy()} function from \texttt{\{broom\}} tidies the data into a tibble, all the usual data transformation can be performed. Let's filter only the \texttt{Product} effect under \texttt{term}, and let's order the \texttt{p.value} decreasingly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_aov }\OtherTok{\textless{}{-}}\NormalTok{ senso\_aov1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{==} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Attribute, statistic, p.value) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(p.value)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p.value =} \FunctionTok{round}\NormalTok{(p.value, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_aov }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(p.value }\SpecialCharTok{\textgreater{}=} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
##   Attribute     statistic p.value
##   <chr>             <dbl>   <dbl>
## 1 Cereal flavor      1.22   0.294
## 2 Roasted odor       1.40   0.193
## 3 Astringent         1.62   0.116
## 4 Sticky             1.67   0.101
\end{verbatim}

As can be seen, the products do not show any significant differences at 5\% for 4 attributes: \texttt{Cereal\ flavor} (p=0.294), \texttt{Roasted\ odor} (p=0.193), \texttt{Astringent} (p=0.116), and \texttt{Sticky} (p=0.101).

Rather than showing the results in a table, let's visualize them graphically as a bar-chart by representing the F-values. The attributes are ordered decreasingly, and colour-coded based on their significance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_aov }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Signif =} \FunctionTok{ifelse}\NormalTok{(p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{, }\StringTok{"Signif."}\NormalTok{, }\StringTok{"Not Signif."}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Signif =} \FunctionTok{factor}\NormalTok{(Signif, }
                         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Signif."}\NormalTok{, }\StringTok{"Not Signif."}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(Attribute, statistic), }\AttributeTok{y=}\NormalTok{statistic, }
             \AttributeTok{fill=}\NormalTok{Signif)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"Signif."} \OtherTok{=} \StringTok{"forestgreen"}\NormalTok{, }
                               \StringTok{"Not Signif."} \OtherTok{=} \StringTok{"orangered2"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Sensory Attributes"}\NormalTok{, }
          \StringTok{"(The attributes are sorted based on F{-}values (product))"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"F{-}values"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-255-1.pdf}

It appears that the evaluated biscuits differ the most (top 5) for \texttt{Initial\ hardness}, \texttt{Shiny}, \texttt{Dairy\ flavor}, \texttt{External\ color\ intensity}, and \texttt{Thickness}.

\begin{quote}
As an alternative, the \texttt{decat()} function from the \texttt{\{SensoMineR\}} package would do the same job, as it automatically performs ANOVA on a set of attributes (presented in subsequent columns). Additionally, it also perform some t-tests that highlight which samples are significantly more (or less) intense than average for each attribute.
\end{quote}

Once the significant differences have been checked, a follow-up analysis consists in visualizing these differences in a multivariate way. Such visualization is often done through Principal Component Analysis (PCA). In practice, PCA is performed on the sensory profiles. Let's start with building such table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso\_mean }\OtherTok{\textless{}{-}}\NormalTok{ sensory }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Attribute, }\AttributeTok{values\_from =}\NormalTok{ Score, }
              \AttributeTok{values\_fn =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

Such table is then submitted to PCA. R proposes many solutions to run such analysis, including the \texttt{prcomp()} and \texttt{princomp()} functions from the \texttt{\{stats\}} package. However, we prefer to use \texttt{PCA()} from the \texttt{\{FactoMineR\}} as it is more complete as it proposes many options that are very useful in sensory and consumer research (e.g.~it generates the graphics automatically, and allows projecting supplementary individuals and/or variables).

It should however be noted that the \texttt{PCA()} function does not accept tibbles. Instead, the table should be stored in a matrix or data frame which contain the individuals' names (here the product names) as row names. Fortunately, there is an easy solution that allows converting a tibble into a data frame (\texttt{as.data.frame()}) and passing the \texttt{Product} column into row names (\texttt{column\_to\_rownames(var="Product")}).

Since the data also contain two qualitative variables in \texttt{Protein} and \texttt{Fiber}, they should either be removed prior to running the analysis, or better be projected as supplementary through the \texttt{quali.sup} parameter from \texttt{PCA()}. Finally, since \texttt{POpt} is an optimized product, let's not include it in the analysis per se (it is not contributing to the construction of the dimensions). Instead we project it as supplementary (through \texttt{ind.sup}) to illustrate where it would be located on the space if it were.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(FactoMineR)}

\NormalTok{senso\_pca }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{PCA}\NormalTok{(., }\AttributeTok{ind.sup =} \FunctionTok{nrow}\NormalTok{(.), }\AttributeTok{quali.sup =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{graph =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since we set the option \texttt{graph=FALSE}, the PCA plots are not yet being generated. Although \texttt{PCA()} can generate the plots either in \texttt{\{base\}} R language, or in \texttt{\{ggplot2\}}, we prefer to use a complementary package called \texttt{\{factoextra\}} which re-creates most plots from \texttt{\{FactoMineR\}} (and some other packages) as a \texttt{ggplot()} object. This comes in very handy as you can benefit from the flexibility offered by \texttt{ggplot()}.

The score map (the product map) from \texttt{PCA()} is created trough \texttt{fviz\_pca\_ind()}, whereas the variables' representation is created with \texttt{fviz\_pca\_var()}. \texttt{fviz\_pca\_biplot()} is used to produce the so-called \emph{biplot}.

To illustrate this, let's reproduce the product map by coloring the products using the supplementary variables (\texttt{Protein} and \texttt{Fiber} content). This can easily be done through the \texttt{habillage} parameter from \texttt{fviz\_pca\_ind()}, which can either take a numerical value (position) of the name of the qualitative variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_pca\_ind}\NormalTok{(senso\_pca, }\AttributeTok{habillage =} \StringTok{"Protein"}\NormalTok{, }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{fviz\_pca\_ind}\NormalTok{(senso\_pca, }\AttributeTok{habillage =} \DecValTok{2}\NormalTok{, }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{fviz\_pca\_var}\NormalTok{(senso\_pca)}
\FunctionTok{fviz\_pca\_biplot}\NormalTok{(senso\_pca)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-260-1.pdf}

Here, \texttt{repel=TRUE} uses \texttt{geom\_text\_repel()} from \texttt{\{ggrepel\}} (rather than \texttt{geom\_text()} from \texttt{\{ggplot2\}}) to avoid having labels overlapping.

On the first dimension, \texttt{P10} is opposed to \texttt{P09} and \texttt{P03} as it is more intense for attributes such as \texttt{Sweet}, and \texttt{Dairy\ flavor} for example, and less intense for attributes such as \texttt{Dry\ in\ mouth} and \texttt{External\ color\ intensity}. On the second dimension, \texttt{P08}and \texttt{P06} are opposed to \texttt{P02} and \texttt{P07} as they score higher for \texttt{Qty\ of\ inclusions}, and \texttt{Initial\ hardness}, and score lower for \texttt{RawDough\ flavor} and \texttt{Shiny}. \texttt{POpt} is located between \texttt{P05} and \texttt{P06}.

\begin{quote}
Many more visualizations can be produced. Amongst others, let's mention:
* Scree plot showing the evolution of the eigenvalues across dimensions to help decide how many dimensions to consider;
* The representation of the product space on other dimensions (by default, dimension 1 and dimension 2 are shown);
* Representations of the (product or attribute) space in which the contribution or quality of representation of the elements are showcased.
\end{quote}

For more information regarding the various options offered by \texttt{\{factoextra\}}, see (REF BOOK \texttt{\{factoextra\}}).

\hypertarget{demographic-and-questionnaire-data}{%
\section{Demographic and Questionnaire Data}\label{demographic-and-questionnaire-data}}

The \emph{biscuits\_traits.xlsx} file contains descriptive (i.e.~\emph{demographic}) information regarding the consumers and their food-related behavioral traits (i.e.~psychometric \emph{TFEQ} data, see Section \ref{example-projects} for more information). This file has three tabs denoted as \emph{Data}, \emph{Variables}, and \emph{Levels}:

\begin{itemize}
\tightlist
\item
  \emph{Data} contains the data, which is coded;
\item
  \emph{Variables} provides information (e.g.~name, information) related to the different variables present in \emph{Data};
\item
  \emph{Levels} provides information about the different levels each variable can take.
\end{itemize}

Let's start with importing this data set. The importation is done in multiple steps as following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"biscuits\_traits.xlsx"}\NormalTok{)}
\FunctionTok{excel\_sheets}\NormalTok{(file\_path)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Data"      "Variables" "Levels"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo\_var }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Variables"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Code, Name)}

\NormalTok{demo\_lev }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Levels"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Question, Code, Levels) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(demo\_var, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"Question"} \OtherTok{=} \StringTok{"Code"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Question)}

\NormalTok{demographic }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Data"}\NormalTok{, }\AttributeTok{skip =} \DecValTok{1}\NormalTok{, }
                         \AttributeTok{col\_names =} \FunctionTok{unlist}\NormalTok{(demo\_var}\SpecialCharTok{$}\NormalTok{Name))}
\end{Highlighting}
\end{Shaded}

\hypertarget{demographic-data-frequency-and-proportion}{%
\subsection{Demographic Data: Frequency and Proportion}\label{demographic-data-frequency-and-proportion}}

For this demographic data file, let's start by having a look at the partition of consumers for each of the descriptive variables. This is done by computing the frequency and proportion (in percentage) attached to each level of \texttt{Living\ area}, \texttt{Housing}, \texttt{Income\ range}, and \texttt{Occupation}. To obtain such a table, let's start by selecting only the columns corresponding to these variables together with \texttt{Judge}.

Since data from surveys and questionnaires are often coded (here, answer \emph{\#6} to question \texttt{Q10} means \emph{Student}, while answer \emph{\#7} to the same question means \emph{Qualified worker}), they first need to be decoded. In our case, the key to decode the data is stored in \texttt{demo\_lev}.

Different strategies to decode the data are possible. One straight-forward strategy consists in automatically decoding each variable using \texttt{mutate()} and \texttt{factor()}. Another approach is considered here: Let's start with building a long thin tibble with \texttt{pivot\_longer()} that we merge to \texttt{demo\_lev} by \texttt{Question} and \texttt{Response} using \texttt{inner\_join()}. We prefer this solution here as it is simpler, faster, and independent of the number of variables to decode.

Once done, we can aggregate the results by \texttt{Question} and \texttt{Levels} (since we want to use the level information, not their code) and compute the frequency (\texttt{n()}) and the proportion (\texttt{N/sum(N)})\footnote{We use the package \texttt{\{formattable\}} to print the results in percentage using one decimal. As an alternative, we could have used \texttt{percent()} from the \texttt{\{scales\}} package.}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(formattable)}

\NormalTok{demog\_reduced }\OtherTok{\textless{}{-}}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, }\StringTok{\textasciigrave{}}\AttributeTok{Living area}\StringTok{\textasciigrave{}}\NormalTok{, Housing, }
                \StringTok{\textasciigrave{}}\AttributeTok{Income range}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Occupation}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge, }
               \AttributeTok{names\_to =} \StringTok{"Question"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(demo\_lev, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"Question"} \OtherTok{=} \StringTok{"Name"}\NormalTok{, }
                              \StringTok{"Response"} \OtherTok{=} \StringTok{"Code"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Question, Levels) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{N =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Pct =} \FunctionTok{percent}\NormalTok{(N }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(N), }\AttributeTok{digits =}\NormalTok{ 1L)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Histograms are a nice way to visualize proportions and to compare them over several variables. Such histograms can be obtained by splitting \texttt{demog\_reduced} by \texttt{Question} and by creating them using either \texttt{N} or \texttt{Pct} (we are using \texttt{Pct} here). For simplicity, let's order the levels decreasingly (\texttt{reorder}) and let's represent them horizontally (\texttt{coord\_flip()}). Of course, such graphs are automated across all questions using \texttt{map()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demog\_reduced }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Question) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{    var }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{pull}\NormalTok{(Question) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{unique}\NormalTok{()}

    \FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(Levels, Pct), }\AttributeTok{y =}\NormalTok{ Pct, }\AttributeTok{label =}\NormalTok{ Pct)) }\SpecialCharTok{+}
      \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Pct }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{ggtitle}\NormalTok{(var) }\SpecialCharTok{+}
      \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{coord\_flip}\NormalTok{()}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $Housing
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-263-1.pdf}

\begin{verbatim}
## 
## $`Income range`
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-263-2.pdf}

\begin{verbatim}
## 
## $`Living area`
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-263-3.pdf}

\begin{verbatim}
## 
## $Occupation
\end{verbatim}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-263-4.pdf}

\hypertarget{regex}{%
\subsection{Eating behavior traits: TFEQ data}\label{regex}}

In the same data set, consumers also answered some questions that reflect their relation to food \citep{stunkard1985}. These questions can be categorized into three groups (also known as factors):

\begin{itemize}
\tightlist
\item
  Disinhibition (variables starting with \texttt{D});
\item
  Restriction (variables starting with \texttt{R});
\item
  Sensitivity to Hunger (variables starting with \texttt{H}).
\end{itemize}

In order to analyze these three factors separately, we first need to select the corresponding variables. As we have seen earlier, such selection could be done by combining \texttt{dplyr::select()} to \texttt{starts\_with("D")}, \texttt{starts\_with("R")}, and/or \texttt{starts\_with("H")}. However, this solution is not satisfactory as it also selects other variables that would start with any of these letters (e.g.~Housing).

Instead, let's take advantage of the fact that variable names have a recurring pattern (they all start with the letters D, R, or H, followed by a number) to introduce the notion of \emph{regular expressions}.

Regular expressions are coded expression that allows finding patterns in names. In practice, generating a regular expression can be quite complex as it is an abstract concept which follows very specific rules. Fortunately, the package \texttt{\{RVerbalExpression\}} is a great assistant as it generates the regular expression for you thanks to understandable functions.
To create a regular expression using \texttt{\{RVerbalExpression\}}, we should first initiate it by calling the function \texttt{rx()} to which any relevant rules can be added. In our case, the variables must start with any of the letter R, D, or H, followed by a number (or more, as values go from 1 to 21). This can be done using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RVerbalExpressions)}

\NormalTok{rdh }\OtherTok{\textless{}{-}} \FunctionTok{rx}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_either\_of}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"R"}\NormalTok{, }\StringTok{"D"}\NormalTok{, }\StringTok{"H"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_digit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_one\_or\_more}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\texttt{rdh} is defined as (R\textbar D\textbar H)\d+ which corresponds to the regular expression we were looking for. We can then reduce (through \texttt{dplyr::select()}) the table to the variables that fit our regular expression by using the function \texttt{matches()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demographic }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{matches}\NormalTok{(rdh))}
\end{Highlighting}
\end{Shaded}

For each variable, let's create a frequency table. Although we could use already build in functions, let's customize our table (including raw frequency and percentages) as we want by creating our own function (called here \texttt{myfreq()}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myfreq }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, info) \{}
\NormalTok{  var }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{TFEQ))}
\NormalTok{  info }\OtherTok{\textless{}{-}}\NormalTok{ info }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(Name }\SpecialCharTok{==}\NormalTok{ var)}

\NormalTok{  res }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Response =} \FunctionTok{factor}\NormalTok{(Response, }\AttributeTok{levels =}\NormalTok{ info}\SpecialCharTok{$}\NormalTok{Code, }
                             \AttributeTok{labels =}\NormalTok{ info}\SpecialCharTok{$}\NormalTok{Levels)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(Response) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Response) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\AttributeTok{N =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Pct =} \FunctionTok{percent}\NormalTok{(N }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(N), }\AttributeTok{digits =}\NormalTok{ 1L)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{()}

  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then apply this function to each variable separately using \texttt{map()} after pivoting all these variables of interest (\texttt{pivot\_longer()}) and splitting the data by \texttt{TFEQ} question:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TFEQ\_freq }\OtherTok{\textless{}{-}}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, }\FunctionTok{matches}\NormalTok{(rdh)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge, }\AttributeTok{names\_to =} \StringTok{"TFEQ"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{TFEQ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{(myfreq, }\AttributeTok{info =}\NormalTok{ demo\_lev) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{enframe}\NormalTok{(}\AttributeTok{name =} \StringTok{"TFEQ"}\NormalTok{, }\AttributeTok{value =} \StringTok{"res"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(res) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFEQ =} \FunctionTok{factor}\NormalTok{(TFEQ, }\AttributeTok{levels =} \FunctionTok{unique}\NormalTok{(}
    \FunctionTok{str\_sort}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{TFEQ, }\AttributeTok{numeric=}\ConstantTok{TRUE}\NormalTok{)))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(TFEQ)}
\end{Highlighting}
\end{Shaded}

From this table, histograms representing the frequency distribution for each variable can be created. But let's suppose that we only want to display variables related to Disinhibition. To do so, we first need to generate the corresponding regular expression (only selecting variables starting with ``D'') to filter the results before creating the plots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{\textless{}{-}} \FunctionTok{rx}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_find}\NormalTok{(}\StringTok{"D"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_digit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rx\_one\_or\_more}\NormalTok{()}

\NormalTok{TFEQ\_freq }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(TFEQ, d)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Response, }\AttributeTok{y =}\NormalTok{ Pct, }\AttributeTok{label =}\NormalTok{ Pct)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Pct }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \DecValTok{1}\NormalTok{, }\AttributeTok{angle =} \DecValTok{30}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{TFEQ, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-268-1.pdf}

Structured questionnaires such as the TFEQ are very frequent in sensory and consumer science. They are used to measure individual patterns as diverse as personality traits, attitudes, food choice motives, engagement, social desirability bias, etc. Ultimately, the TFEQ questionnaire consists in a set of structured questions whose respective answers combine to provide a TFEQ score (actually, three scores, one for Disinhibition, one for Restriction and one for sensitivity to Hunger). This TFEQ scores translate into certain food behavior tendencies.

However, computing the TFEQ scores is slightly more complicated than adding the scores of all TFEQ questions together. Instead, they follow certain rules that are stored in the \emph{Variables} sheet in \emph{biscuits\_traits.xlsx}. For each TFEQ question, the rule to follow is provided by \texttt{Direction} and \texttt{Value}, and works as following: if the condition provided by \texttt{Direction} and \texttt{Value} is met, then the respondent gets a 1, else a 0. Ultimately, the TFEQ score is the sum of all these evaluations.

Let's start by extracting this information (\texttt{Direction} and \texttt{Value}) from the \emph{Variables} sheet for all the variables involved in the computation of the TFEQ scores. We store this in \texttt{var\_drh}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_rdh }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Variables"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Name, rdh)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Name, Direction, Value)}
\end{Highlighting}
\end{Shaded}

This information is added to \texttt{demographic}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TFEQ }\OtherTok{\textless{}{-}}\NormalTok{ demographic }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, }\FunctionTok{matches}\NormalTok{(rdh)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge, }\AttributeTok{names\_to =} \StringTok{"DHR"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(var\_rdh, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"DHR"} \OtherTok{=} \StringTok{"Name"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Since we need to evaluate each assessors' answer to the TFEQ questions, we create a new variable \texttt{TFEQValue} which takes a 1 if the corresponding condition is met, and a 0 otherwise. Such approach is done through \texttt{mutate()} combined with a succession of intertwined \texttt{ifelse()} functions.\footnote{The function \texttt{ifelse()} takes three parameters: 1. the condition to test, 2. the value or code to run if the condition is met, and 3. the value or code to run if the condition is not met.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TFEQ\_coded }\OtherTok{\textless{}{-}}\NormalTok{ TFEQ }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFEQValue =} \FunctionTok{ifelse}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"Equal"} \SpecialCharTok{\&}\NormalTok{ Score }\SpecialCharTok{==}\NormalTok{ Value, }\DecValTok{1}\NormalTok{,}
    \FunctionTok{ifelse}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"Superior"} \SpecialCharTok{\&}\NormalTok{ Score }\SpecialCharTok{\textgreater{}}\NormalTok{ Value, }\DecValTok{1}\NormalTok{,}
      \FunctionTok{ifelse}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"Inferior"} \SpecialCharTok{\&}\NormalTok{ Score }\SpecialCharTok{\textless{}}\NormalTok{ Value, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Factor =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{str\_starts}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{DHR, }\StringTok{"D"}\NormalTok{), }\StringTok{"Disinhibition"}\NormalTok{,}
    \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{str\_starts}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{DHR, }\StringTok{"H"}\NormalTok{), }\StringTok{"Hunger"}\NormalTok{, }\StringTok{"Restriction"}\NormalTok{)}
\NormalTok{  )) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Factor =} \FunctionTok{factor}\NormalTok{(Factor, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Restriction"}\NormalTok{, }
                                            \StringTok{"Disinhibition"}\NormalTok{, }
                                            \StringTok{"Hunger"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Ultimately, we compute the TFEQ Score by summing across all \texttt{TFEQValue} per respondent, by maintaining the distinction between each category. Note that the final score is stored in \texttt{Total}, which corresponds to sum across categories:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TFEQ\_score }\OtherTok{\textless{}{-}}\NormalTok{ TFEQ\_coded }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Judge, Factor) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{TFEQ =} \FunctionTok{sum}\NormalTok{(TFEQValue)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge =} \FunctionTok{factor}\NormalTok{(Judge, }\AttributeTok{levels =} \FunctionTok{unique}\NormalTok{(}
    \FunctionTok{str\_sort}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Judge, }\AttributeTok{numeric =} \ConstantTok{TRUE}\NormalTok{)))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Judge) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Factor, }\AttributeTok{values\_from =}\NormalTok{ TFEQ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Total =} \FunctionTok{sum}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` has grouped output by 'Judge'. You can
## override using the `.groups` argument.
\end{verbatim}

Such results can then be visualized graphically, for instance by representing the distribution of \texttt{TFEQ\_score} for the 3 TFEQ factors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TFEQ\_score }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Total) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Judge, }\AttributeTok{names\_to =} \StringTok{"Factor"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Scores"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Scores, }\AttributeTok{color =}\NormalTok{ Factor)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{lwd =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{key\_glyph =} \StringTok{"path"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"TFEQ Score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{color =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{override.aes =} \FunctionTok{list}\NormalTok{(}\AttributeTok{linetype =} \DecValTok{1}\NormalTok{))) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Distribution of the Individual TFEQ{-}factor Scores"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-273-1.pdf}

\hypertarget{tibble-use}{%
\section{Consumer Data}\label{tibble-use}}

The analysis of consumer data usually involves the same type of analysis as the ones for sensory data (e.g.~ANOVA, PCA, etc.), but the way the data is being collected (absence of duplicates) and its underlying nature (affect vs.~descriptive) require some adjustments.

Let's start by importing the consumer data that is stored in \emph{biscuits\_consumer\_test.xlsx}. Here, we import two sheets, one with the consumption time and number of biscuits (stored in \texttt{Nbiscuit}), and one with different consumer evaluations of the samples (stored in \texttt{consumer})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"biscuits\_consumer\_test.xlsx"}\NormalTok{)}

\NormalTok{Nbiscuit }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Time Consumption"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Product =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"P"}\NormalTok{, Product)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{N =} \StringTok{\textasciigrave{}}\AttributeTok{Nb biscuits}\StringTok{\textasciigrave{}}\NormalTok{)}

\NormalTok{consumer }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path, }\AttributeTok{sheet =} \StringTok{"Biscuits"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Judge =}\NormalTok{ Consumer, }\AttributeTok{Product =}\NormalTok{ Samples) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"J"}\NormalTok{, Judge), }\AttributeTok{Product =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"P"}\NormalTok{, Product)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(Nbiscuit, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"Judge"}\NormalTok{, }\StringTok{"Product"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Similarly to the sensory data, let's start with computing the mean liking score per product after the first bite (\texttt{1stbite\_liking}) and at the end of the evaluation (\texttt{after\_liking}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{consumer }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\StringTok{\textasciigrave{}}\AttributeTok{1stbite\_liking}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{after\_liking}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 3
##   Product `1stbite_liking` after_liking
##   <chr>              <dbl>        <dbl>
## 1 P1                  6.30         6.26
## 2 P10                 7.40         7.57
## 3 P2                  5.53         5.38
## 4 P3                  3.94         3.49
## # ... with 6 more rows
\end{verbatim}

A first glance at the table shows that there are clear differences between the samples (within a liking variable), but little difference between liking variables (within a sample).

Of course, we want to know if differences between samples are significant. We thus need to perform an 2-way ANOVA (testing for the product effect by also taking into account the individual differences) followed up by a paired comparison test (here Tukey's HSD). For the latter, the \texttt{\{agricolae\}} package is a good solution, as it is simple to use and has all its built-in tests working in the same way.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agricolae)}

\NormalTok{liking\_start }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{1stbite\_liking}\StringTok{\textasciigrave{}} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data =}\NormalTok{ consumer)}
\NormalTok{liking\_start\_hsd }\OtherTok{\textless{}{-}} \FunctionTok{HSD.test}\NormalTok{(liking\_start, }\StringTok{"Product"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{groups }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Product"}\NormalTok{)}

\NormalTok{liking\_start\_hsd}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 3
##   Product `1stbite_liking` groups
##   <chr>              <dbl> <chr> 
## 1 P10                 7.40 a     
## 2 P1                  6.30 b     
## 3 P5                  5.78 b     
## 4 P2                  5.53 bc    
## # ... with 6 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liking\_end }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{after\_liking}\StringTok{\textasciigrave{}} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Product }\SpecialCharTok{+}\NormalTok{ Judge, }\AttributeTok{data =}\NormalTok{ consumer)}
\NormalTok{liking\_end\_hsd }\OtherTok{\textless{}{-}} \FunctionTok{HSD.test}\NormalTok{(liking\_end, }\StringTok{"Product"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{groups }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Product"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Both at the start and at the end of the evaluation, significant differences (at 5\%) in liking between samples are observed according to Tukey's HSD test.

To further compare the liking assessment of the samples after the first bite and at the end of the tasting, the results obtained from \texttt{liking\_start\_hsd} and \texttt{liking\_end\_hsd} are combined. We then represent the results in a bar-chart:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list}\NormalTok{(}\AttributeTok{Start =}\NormalTok{ liking\_start\_hsd }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Liking =} \StringTok{\textasciigrave{}}\AttributeTok{1stbite\_liking}\StringTok{\textasciigrave{}}\NormalTok{),}
     \AttributeTok{End =}\NormalTok{ liking\_end\_hsd }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Liking =} \StringTok{\textasciigrave{}}\AttributeTok{after\_liking}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{enframe}\NormalTok{(}\AttributeTok{name =} \StringTok{"Moment"}\NormalTok{, }\AttributeTok{value =} \StringTok{"res"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(res) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Moment =} \FunctionTok{factor}\NormalTok{(Moment, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Start"}\NormalTok{, }\StringTok{"End"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(Product, }\SpecialCharTok{{-}}\NormalTok{Liking), }\AttributeTok{y =}\NormalTok{ Liking, }\AttributeTok{fill =}\NormalTok{ Moment))}\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Comparison of the liking scores (start vs. end evaluation)"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-277-1.pdf}

As can be seen, the pattern of liking scores across samples is indeed very stable across the evaluation, particularly in terms of rank. At the individual level, such linear relationship is also observed (here for the first 12 consumers):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{consumer }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\AttributeTok{Start =} \StringTok{\textasciigrave{}}\AttributeTok{1stbite\_liking}\StringTok{\textasciigrave{}}\NormalTok{, }
                \AttributeTok{End =} \StringTok{\textasciigrave{}}\AttributeTok{after\_liking}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Judge }\SpecialCharTok{\%in\%} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"J"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge =} \FunctionTok{factor}\NormalTok{(Judge, }\AttributeTok{levels =} \FunctionTok{unique}\NormalTok{(}
    \FunctionTok{str\_sort}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{Judge, }\AttributeTok{numeric =} \ConstantTok{TRUE}\NormalTok{)))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Start, }\AttributeTok{y =}\NormalTok{ End)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =} \StringTok{"y\textasciitilde{}x"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Overall Liking"}\NormalTok{, }
          \StringTok{"(Assessment after first bite vs. end of the tasting)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Judge)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-278-1.pdf}

\begin{quote}
For your own curiosity, we invite you to re-create the same graph by comparing the Liking score at the end of the evaluation (\texttt{after\_liking}) with the liking score measured on the 9pt categorical scale (\texttt{end\_liking\ 9pt}), and to reflect on the results obtained. Are the consumers consistent in their evaluations?
\end{quote}

Another interesting relationship to study involves the liking scores\footnote{We would like to remind the reader that the liking scores measured on the categorical scale was reverted since 1 defined ``I like it a lot'' and 9 ``I dislike it a lot''. To simplify the readability, this scale is reverted so that 1 corresponds to a low liking score, and 9 to a high liking score (in practice, we will take as value 10 - score given).} and the number of cookies eaten by each consumer. We could follow the same procedure as before, but prefer to add here a filter to only show consumers with a significant regression line at 5\%.

Let's start by creating a function called \texttt{run\_reg()} that runs the regression analysis of the number of biscuits (\texttt{N}) in function of the liking score (\texttt{Liking}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run\_reg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
\NormalTok{  output }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(N }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Liking, }\AttributeTok{data =}\NormalTok{ df)}
  \FunctionTok{return}\NormalTok{(output)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

After transforming the data, we apply this function to each consumer separately.

Here, we take advantage of the flexibility of tibbles as it allows storing results as list by saving three sorts of outputs per consumer:

\begin{itemize}
\tightlist
\item
  \texttt{data} contains the individual data;
\item
  \texttt{lm\_obj} corresponds to the results of the linear model (obtained with `run\_reg()``);
\item
  \texttt{glance} contains some general results of the model incl.~R2, the p-value, etc.
\end{itemize}

\begin{quote}
These three outputs contain completely different information for the same analysis (here regressions). The fact that tibbles allow storing outputs as list is very handy since all the results are tidied in one unique R object, which can then easily be accessed by unfolding the output needed.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_liking\_reg }\OtherTok{\textless{}{-}}\NormalTok{ consumer }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\AttributeTok{Liking =} \StringTok{\textasciigrave{}}\AttributeTok{end\_liking 9pt}\StringTok{\textasciigrave{}}\NormalTok{, N) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Liking =} \DecValTok{10} \SpecialCharTok{{-}}\NormalTok{ Liking) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Judge) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lm\_obj =} \FunctionTok{map}\NormalTok{(data, run\_reg)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{glance =} \FunctionTok{map}\NormalTok{(lm\_obj, broom}\SpecialCharTok{::}\NormalTok{glance))}
\end{Highlighting}
\end{Shaded}

Since we only want to represent consumers with a significant regression line, we unfold the results stored in \texttt{glance} so that we can access the \texttt{p.value} of each regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_liking }\OtherTok{\textless{}{-}}\NormalTok{ N\_liking\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(glance) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(p.value) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Judge =} \FunctionTok{fct\_reorder}\NormalTok{(Judge, p.value)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

Ultimately, the relationship between the liking score and the number of biscuits eaten is represented in a line chart:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(N\_liking, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Liking, }\AttributeTok{y =}\NormalTok{ N)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =} \StringTok{"y\textasciitilde{}x"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of Biscuits vs. Liking"}\NormalTok{, }
          \StringTok{"Consumers with a signif. (5\%) regression model are shown."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Judge, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-282-1.pdf}

\hypertarget{combining-sensory-and-consumer-data}{%
\section{Combining Sensory and Consumer Data}\label{combining-sensory-and-consumer-data}}

\hypertarget{internal-preference-mapping}{%
\subsection{Internal Preference Mapping}\label{internal-preference-mapping}}

Now we've analyzed the sensory and the consumer data separately, it is time to combine both data sets and analyze them conjointly. A first analysis that can then be performed is the internal preference mapping, i.e.~a PCA on the consumer liking scores in which the sensory attributes are projected as supplementary.

Such analysis is split in 3 steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The consumer data is re-organized in a wide format with the samples in rows and the consumers in columns;
\item
  The sensory mean table is joined to the consumer data (make sure that the product names perfectly match in the two files);
\item
  A PCA is performed on the consumer data, the sensory descriptors being projected as supplementary variables.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{consumer\_wide }\OtherTok{\textless{}{-}}\NormalTok{ consumer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(Product, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"P"}\NormalTok{, }\StringTok{"Number"}\NormalTok{), }\AttributeTok{sep =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Number =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(Number) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }
                         \FunctionTok{str\_c}\NormalTok{(}\StringTok{"0"}\NormalTok{, Number), Number)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(Product, P, Number, }\AttributeTok{sep =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\AttributeTok{Liking =} \StringTok{\textasciigrave{}}\AttributeTok{end\_liking 9pt}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Liking =} \DecValTok{10} \SpecialCharTok{{-}}\NormalTok{ Liking) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Judge, }\AttributeTok{values\_from =}\NormalTok{ Liking)}

\NormalTok{data\_mdpref }\OtherTok{\textless{}{-}}\NormalTok{ senso\_mean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(consumer\_wide, }\AttributeTok{by =} \StringTok{"Product"}\NormalTok{)}

\NormalTok{res\_mdpref }\OtherTok{\textless{}{-}}\NormalTok{ data\_mdpref }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{PCA}\NormalTok{(., }\AttributeTok{quali.sup =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{quanti.sup =} \DecValTok{3}\SpecialCharTok{:}\DecValTok{34}\NormalTok{, }\AttributeTok{graph =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{fviz\_pca\_ind}\NormalTok{(res\_mdpref, }\AttributeTok{habillage =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-283-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_pca\_var}\NormalTok{(res\_mdpref, }\AttributeTok{label =} \StringTok{"quanti.sup"}\NormalTok{, }
             \AttributeTok{select.var =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cos2 =} \FloatTok{0.5}\NormalTok{), }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-283-2.pdf}

As can be seen, the consumers are quite in agreement as all the black arrows are pointing in a similar direction.
In overall, they seem to like biscuits that are sweet, with cereal flavor, and fatty/dairy flavor and odor, and dislike biscuits defined as astringent, dry in mouth, uneven and with dark external color.

\hypertarget{hac}{%
\subsection{Consumers Clustering}\label{hac}}

Although the data show a fairly good agreement between consumers, let's cluster them in more homogeneous groups based on liking.

Various solutions for clustering exist, depending on the type of distance (similarity or dissimilarity), the linkage (single, average, Ward, etc.), and of course the algorithm itself (e.g.~AHC, k-means, etc.).

Here, we opt for the Agglomerative Hierarchical Clustering (AHC) with Euclidean distance (dissimilarity) and Ward criterion, as it is a fairly common approach in Sensory and Consumer research. Such analysis can be done using \texttt{stats::hclust()} or \texttt{cluster::agnes()}.

\begin{quote}
Before computing the distance between consumers, it is advised to at least center their liking scores (subtracting their mean liking scores to each of their individual scores) as it allows grouping consumers based on their respective preferences, rather than on their scale usage (otherwise, consumers who scored high on all samples are grouped together, and separated from consumers who scored low on all samples, which isn't so much informative). Such transformation can be done automatically using the \texttt{scale()}\footnote{\texttt{scale()} allows centering (\texttt{center=TRUE}) and standardizing (\texttt{scale=TRUE}) data automatically in columns, hence generating \emph{z-scores}.} function.
\end{quote}

Let's start with computing the euclidean distance between each pair of consumers by using the \texttt{dist()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{consumer\_dist }\OtherTok{\textless{}{-}}\NormalTok{ consumer\_wide }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{scale}\NormalTok{(., }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{t}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{dist}\NormalTok{(., }\AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The AHC is performed using the \texttt{hclust()} function and the \texttt{method\ =\ "ward.D2"} parameter, which is the equivalent to \texttt{method\ =\ "ward"} for \texttt{agnes()}. To visualize the dendrogram, the \texttt{factoextra::fviz\_dend()} function is used (here we propose to visualize the 2-clusters solution by setting \texttt{k=2}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_hclust }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(consumer\_dist, }\AttributeTok{method =} \StringTok{"ward.D2"}\NormalTok{)}
\FunctionTok{fviz\_dend}\NormalTok{(res\_hclust, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-285-1.pdf}

\begin{quote}
An interesting option to visualize clusters and proposed by \texttt{fviz\_dend()} is the \emph{phologenic} representation (\texttt{type="phylogenic"}). We invite you to give it a try to see how it represents the clusters as an alternative to the classical dendrogram tree.
\end{quote}

Since we are satisfied with the 2 clusters solution, we cut the tree (using \texttt{cutree()}) at this level using, hence generating a group of 74 and a group of 33 consumers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_clust }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(res\_hclust, }\AttributeTok{k =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Judge"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Cluster =}\NormalTok{ value) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Cluster =} \FunctionTok{as.character}\NormalTok{(Cluster))}

\NormalTok{res\_clust }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Cluster)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   Cluster     n
##   <chr>   <int>
## 1 1          74
## 2 2          33
\end{verbatim}

Lastly, we compare visually the preference patterns between clusters by representing in a line chart the average liking score for each product provided by each cluster.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_cluster }\OtherTok{\textless{}{-}}\NormalTok{ consumer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(Product, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"P"}\NormalTok{, }\StringTok{"Number"}\NormalTok{), }\AttributeTok{sep =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Number =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(Number) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }
                         \FunctionTok{str\_c}\NormalTok{(}\StringTok{"0"}\NormalTok{, Number), Number)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(Product, P, Number, }\AttributeTok{sep =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Judge, Product, }\AttributeTok{Liking =} \StringTok{\textasciigrave{}}\AttributeTok{end\_liking 9pt}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Liking =} \DecValTok{10} \SpecialCharTok{{-}}\NormalTok{ Liking) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{full\_join}\NormalTok{(res\_clust, }\AttributeTok{by =} \StringTok{"Judge"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Product, Cluster) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Liking =} \FunctionTok{mean}\NormalTok{(Liking), }\AttributeTok{N =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Cluster =} \FunctionTok{str\_c}\NormalTok{(Cluster, }\StringTok{" ("}\NormalTok{, N, }\StringTok{")"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}

\FunctionTok{ggplot}\NormalTok{(mean\_cluster, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Product, }\AttributeTok{y =}\NormalTok{ Liking, }
                         \AttributeTok{colour =}\NormalTok{ Cluster, }\AttributeTok{group =}\NormalTok{ Cluster)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =}\NormalTok{ Cluster), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Average Liking Score"}\NormalTok{, }
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Cluster differences in the appreciation of the Products"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-287-1.pdf}

It appears that cluster 1 (74 consumers) particularly likes \texttt{P10}, \texttt{P01}, and \texttt{P05}, and has a fairly flat liking pattern otherwise. On the other hand, the cluster 2 (33 consumers) expressed strong rejections towards \texttt{P04} and \texttt{P08}, and like \texttt{P10} and \texttt{P01} the most.

The fact that both clusters agree on the best samples (\texttt{P10} and \texttt{P01}) goes with our original assumption from the Internal Preference Mapping that the panel of consumers is fairly homogeneous in terms of preferences.

\begin{quote}
In \texttt{\{FactoMineR\}}, the \texttt{HCPC()} function also performs AHC but takes as starting point the results of a multivariate analysis (HCPC stands for Hierarchical Clustering on Principal Components). This would typically be the results of the PCA performed on the Consumer (rows) \emph{x} Product (columns) matrix of liking scores, in which the scores are (at least) centered in row.
Although results should be identical in most cases, it can happen that results slightly diverge from \texttt{agnes()} and \texttt{hclust()} as it also depends on the number of dimensions kept in the multivariate analysis and on the treatment of in-between clusters consumers. But more interestingly, \texttt{HCPC()} offers the possibility to \emph{consolidate} the clusters by performing k-means on the solution obtained from the AHC (\texttt{consol=TRUE}).
\end{quote}

\hypertarget{drivers-of-liking}{%
\subsection{Drivers of Liking}\label{drivers-of-liking}}

When combining sensory and consumer data collected on the same product, it is also relevant to understand which sensory properties of the products drive the consumers' liking and disliking. Such evaluation can be done at the panel level, at a group level (e.g.~clusters, users vs.~non-users, gender, etc.), or even at the individual level. Unless stated otherwise, the computations will be done for cluster 1, but could easily be adapted to other groups if needed.

\hypertarget{correlation}{%
\subsubsection{Correlation}\label{correlation}}

Let's start by evaluating the simplest relationship between the sensory attributes and overall liking by looking at correlations. Here, we are combining the average liking score per cluster to the sensory profile of the products. The correlations are then computed using the \texttt{cor()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_cor }\OtherTok{\textless{}{-}}\NormalTok{ mean\_cluster }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{N) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Cluster, }\AttributeTok{values\_from =}\NormalTok{ Liking) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(senso\_mean }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{               dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Protein,Fiber)), }\AttributeTok{by=}\StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{)}

\NormalTok{res\_cor }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(data\_cor)}
\end{Highlighting}
\end{Shaded}

Various packages can be used to visualize these correlations. We opt here for \texttt{ggcorrplot()} from the \texttt{\{ggcorrplot\}} package as it provides many interesting visualization in \texttt{\{ggplot2\}}. This package also provides the function \texttt{cor\_pmat()} which compute the p-value associated to each correlation. This matrix of p-value can be used to hide correlations that are not significant at the level defined by the parameter \texttt{sig.level}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggcorrplot)}
\NormalTok{res\_cor\_pmat }\OtherTok{\textless{}{-}} \FunctionTok{cor\_pmat}\NormalTok{(data\_cor)}
\FunctionTok{ggcorrplot}\NormalTok{(res\_cor, }\AttributeTok{type =} \StringTok{"full"}\NormalTok{, }\AttributeTok{p.mat =}\NormalTok{ res\_cor\_pmat, }
           \AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{insig =} \StringTok{"blank"}\NormalTok{, }
           \AttributeTok{lab =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lab\_size =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-289-1.pdf}

The average liking scores for cluster 1 (defined as \texttt{1\ (74)}) are positively correlated with \texttt{Overall\ odor\ intensity}, \texttt{Fatty\ odor}, \texttt{Cereal\ flavor}, \texttt{Fatty\ flavor}, \texttt{Dairy\ flavor}, \texttt{Overall\ flavor\ persistence}, \texttt{Salty}, \texttt{Sweet}, \texttt{Warming}, \texttt{Fatty\ in\ mouth}, and \texttt{Melting}. They are also negatively correlated to \texttt{External\ color\ intensity}, \texttt{Astringent}, and \texttt{Dry\ in\ mouth}.
Finally, it can be noted that the correlation between clusters is high with a value of 0.72.

\hypertarget{linear-and-quadratic-regression}{%
\subsubsection{Linear and Quadratic Regression}\label{linear-and-quadratic-regression}}

Although the correlation provides a first good idea of which attributes are linked to liking, it only measures linear relationships and it does not allow for inference. To overcome this particular limitations, linear and quadratic regressions are used.

Let's start by combining the sensory data to the average liking score per product for cluster 1. To simplify the analysis, all the sensory attributes are structured in the longer format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_reg }\OtherTok{\textless{}{-}}\NormalTok{ mean\_cluster }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Cluster }\SpecialCharTok{==} \StringTok{"1 (74)"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{N) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(senso\_mean }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{               dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Protein, Fiber)), }\AttributeTok{by =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(Shiny}\SpecialCharTok{:}\NormalTok{Melting, }
               \AttributeTok{names\_to =} \StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Attribute =} 
           \FunctionTok{factor}\NormalTok{(Attribute, }
                  \AttributeTok{levels=}\FunctionTok{colnames}\NormalTok{(senso\_mean)[}\DecValTok{4}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(senso\_mean)]))}
\end{Highlighting}
\end{Shaded}

Both the linear regression and the quadratic regression are then run on \texttt{Liking} per attribute:

\begin{quote}
To add a quadratic model, two options are possible:
1. In \texttt{data\_reg}, we could add a column (using \texttt{mutate()}) called \texttt{Score2} that is defined as \texttt{Score2\ =\ Score\^{}2}. The model for the quadratic regression is then defined as \texttt{Liking\ \textasciitilde{}\ Score\ +\ Score2};
2. The quadratic model is informed directly using the \texttt{poly()} function by informing which polynomial degrees to consider (here \texttt{2}).
For its concision, we opt for the second option.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_reg }\OtherTok{\textless{}{-}}\NormalTok{ data\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest\_by}\NormalTok{(Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lin\_mod =} \FunctionTok{list}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Liking }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Score, }\AttributeTok{data=}\NormalTok{data)),}
         \AttributeTok{quad\_mod =} \FunctionTok{list}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Liking }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(Score, }\DecValTok{2}\NormalTok{), }\AttributeTok{data=}\NormalTok{data)))}
\end{Highlighting}
\end{Shaded}

We extract the attributes that are significantly linked to liking (at 5\%, and we accept 6\% for quadratic effects). To do so, the results stored in \texttt{lin\_mod} and \texttt{quad\_mod} need unfolding (\texttt{summarize()}) and restructuring (\texttt{broom::tidy()}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lin }\OtherTok{\textless{}{-}}\NormalTok{ res\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(lin\_mod)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{==} \StringTok{"Score"}\NormalTok{, p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.character}\NormalTok{()}

\NormalTok{quad }\OtherTok{\textless{}{-}}\NormalTok{ res\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(quad\_mod)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{==} \StringTok{"poly(Score, 2)2"}\NormalTok{, p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.06}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Attribute) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.character}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

These attributes are then represented graphically against the liking Scores.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggrepel)}

\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ data\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Attribute }\SpecialCharTok{\%in\%} \FunctionTok{unique}\NormalTok{(}\FunctionTok{c}\NormalTok{(lin, quad)))}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Score, }\AttributeTok{y =}\NormalTok{ Liking, }\AttributeTok{label =}\NormalTok{ Product)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Attribute, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's now add a regression line to the model. To do so, \texttt{geom\_smooth()} is being used with as \texttt{method\ =\ lm} combined to \texttt{formula\ =\ \textquotesingle{}y\ \textasciitilde{}\ x\textquotesingle{}} for linear relationships, and \texttt{formula\ =\ \textquotesingle{}y\ \textasciitilde{}\ x\ +\ I(x\^{}2)\textquotesingle{}} for quadratic relationships (when both the linear and quadratic models are significant, the quadratic model is used).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.mod }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, quad) \{}
  \FunctionTok{ifelse}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Attribute }\SpecialCharTok{\%in\%}\NormalTok{ quad, }\StringTok{"y\textasciitilde{}x+I(x\^{}2)"}\NormalTok{, }\StringTok{"y\textasciitilde{}x"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We apply this function to our data by applying to each attribute (here we set \texttt{se=FALSE} to remove the confidence intervals around the regression line):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_smooth }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(}
\NormalTok{  df, df}\SpecialCharTok{$}\NormalTok{Attribute,}
  \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ x, }\AttributeTok{method =}\NormalTok{ lm, }
                          \AttributeTok{formula =} \FunctionTok{lm.mod}\NormalTok{(x, }\AttributeTok{quad =}\NormalTok{ quad), }
                          \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{)}
\NormalTok{p }\SpecialCharTok{+}\NormalTok{ p\_smooth}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-295-1.pdf}

All attributes except \texttt{Astringent} are linearly linked to liking. For \texttt{Astringent}, the curvature is U-shaped: this does not show an effect of saturation as it would have been represented as an inverted U-shape. Although the quadratic effect shows a better fit than the linear effect, having a linear effect would have been a good predictor as well in this situation.

\hypertarget{prefmap}{%
\subsection{External Preference Mapping}\label{prefmap}}

Ultimately, one of the goals of combining sensory and consumer data is to find within the sensory space the area that are liked/accepted by consumers. Since this approach is based on modeling and prediction, it may suggest area of the space with high acceptance potential which are not filled in by products yet. This would open doors to new product development.

To perform such analysis, the External Preference Mapping (PrefMap) could be used amongst other techniques. For more information on the principles of PrefMap, please refer to (ANALYZING SENSORY DATA WITH R or OTHER REFERENCES\ldots).

To run the PrefMap analysis, the \texttt{carto()} function from \texttt{\{SensoMineR\}} is being used. This function takes as parameter the sensory space to consider (stored in \texttt{senso\_pca\$ind\$coord}, here we will consider dimension 1 and dimension 2), the table of liking scores (as stored in \texttt{consumer\_wider}), and the model to consider (here we consider the quadratic model, so we use \texttt{regmod=1}). For convenience, we run the analysis on the full panel since \texttt{consumer\_wider} is (almost) already structured as needed.

Since \texttt{carto()} requires matrix or data frame with row names for the analysis, the data needs to be slightly adapted (we also need to ensure that the products are in the same order in both files).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso }\OtherTok{\textless{}{-}}\NormalTok{ senso\_pca}\SpecialCharTok{$}\NormalTok{ind}\SpecialCharTok{$}\NormalTok{coord[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Product"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{)}

\NormalTok{consu }\OtherTok{\textless{}{-}}\NormalTok{ consumer\_wide }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var =} \StringTok{"Product"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(SensoMineR)}
\NormalTok{PrefMap }\OtherTok{\textless{}{-}} \FunctionTok{carto}\NormalTok{(}\AttributeTok{Mat =}\NormalTok{ senso, }\AttributeTok{MatH =}\NormalTok{ consu, }\AttributeTok{regmod =} \DecValTok{1}\NormalTok{, }
                 \AttributeTok{graph.tree =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{graph.corr =} \ConstantTok{FALSE}\NormalTok{, }
                 \AttributeTok{graph.carto =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-296-1.pdf}

From this map, we can see that the optimal area (dark red) is located on the positive side of dimension 1, between \texttt{P01}, \texttt{P05}, and \texttt{P10} (as expected by the liking score).

Let's now re-build this plot using \texttt{\{ggplot2\}}.

The sensory space is stored in \texttt{senso}, whereas the surface response plot information is split between:

\begin{itemize}
\tightlist
\item
  \texttt{PrefMap\$f1}: contains the coordinates on dimension 1 in which predictions have be made;
\item
  \texttt{PrefMap\$f2}: contains the coordinates on dimension 2 in which predictions have be made;
\item
  \texttt{PrefMap\$depasse}: contains the percentage of consumers that accept a product at each point of the space. This matrix is defined in such a way that \texttt{PrefMap\$f1} links to the rows of the matrix, and \texttt{PrefMap\$f2} links to the columns.
\end{itemize}

Last but not least, \texttt{POpt} (which coordinates are stored in \texttt{senso\_pca\$ind.sup\$coord}) can be projected on that space in order to see how this optimized sample is considered in terms of consumers' liking/preference.

Let's start with preparing the data by transforming everything back into a tibble:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{senso }\OtherTok{\textless{}{-}}\NormalTok{ senso }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Product"}\NormalTok{)}

\NormalTok{senso\_sup }\OtherTok{\textless{}{-}}\NormalTok{ senso\_pca}\SpecialCharTok{$}\NormalTok{ind.sup}\SpecialCharTok{$}\NormalTok{coord }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Product"}\NormalTok{)}

\FunctionTok{dimnames}\NormalTok{(PrefMap}\SpecialCharTok{$}\NormalTok{nb.depasse) }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\FunctionTok{round}\NormalTok{(PrefMap}\SpecialCharTok{$}\NormalTok{f1, }\DecValTok{2}\NormalTok{), }
                                     \FunctionTok{round}\NormalTok{(PrefMap}\SpecialCharTok{$}\NormalTok{f2, }\DecValTok{2}\NormalTok{))}
\NormalTok{PrefMap\_plot }\OtherTok{\textless{}{-}}\NormalTok{ PrefMap}\SpecialCharTok{$}\NormalTok{nb.depasse }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"Dim1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Dim1, }
               \AttributeTok{names\_to =} \StringTok{"Dim2"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Acceptance (\%)"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), as.numeric))}
\end{Highlighting}
\end{Shaded}

To build the plot, different layers involving different source of data (\texttt{senso}, \texttt{senso\_sup}, and \texttt{PrefMap\_plot} that is) are required. Hence, the initiation of the plot through \texttt{ggplot()} does not specify any data. Instead, the data used in each step are included within the \texttt{geom\_*()} of interest. In this example, \texttt{geom\_tile()} (coloring) and \texttt{geom\_contour()} (contour lines) are used to build the surface plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}\AttributeTok{data =}\NormalTok{ PrefMap\_plot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim1, }\AttributeTok{y =}\NormalTok{ Dim2, }
                                     \AttributeTok{fill =} \StringTok{\textasciigrave{}}\AttributeTok{Acceptance (\%)}\StringTok{\textasciigrave{}}\NormalTok{, }
                                     \AttributeTok{color =} \StringTok{\textasciigrave{}}\AttributeTok{Acceptance (\%)}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_contour}\NormalTok{(}\AttributeTok{data =}\NormalTok{ PrefMap\_plot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim1, }\AttributeTok{y =}\NormalTok{ Dim2, }
                                        \AttributeTok{z =} \StringTok{\textasciigrave{}}\AttributeTok{Acceptance (\%)}\StringTok{\textasciigrave{}}\NormalTok{), }
               \AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ senso, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ Dim}\FloatTok{.2}\NormalTok{), }
             \AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{cex =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ senso, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ Dim}\FloatTok{.2}\NormalTok{, }
                                    \AttributeTok{label =}\NormalTok{ Product)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ senso\_sup, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ Dim}\FloatTok{.2}\NormalTok{), }
             \AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{col =} \StringTok{"white"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ senso\_sup, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dim}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ Dim}\FloatTok{.2}\NormalTok{, }
                                        \AttributeTok{label =}\NormalTok{ Product), }
                  \AttributeTok{col =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"red"}\NormalTok{, }
                       \AttributeTok{midpoint =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"red"}\NormalTok{, }
                        \AttributeTok{midpoint =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\FunctionTok{str\_c}\NormalTok{(}\StringTok{"Dimension 1("}\NormalTok{, }\FunctionTok{round}\NormalTok{(senso\_pca}\SpecialCharTok{$}\NormalTok{eig[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], }\DecValTok{1}\NormalTok{), }\StringTok{"\%)"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\FunctionTok{str\_c}\NormalTok{(}\StringTok{"Dimension 2("}\NormalTok{, }\FunctionTok{round}\NormalTok{(senso\_pca}\SpecialCharTok{$}\NormalTok{eig[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{], }\DecValTok{1}\NormalTok{), }\StringTok{"\%)"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"External Preference Mapping applied to the biscuits data"}\NormalTok{, }
          \StringTok{"(The PrefMap is based on the quadratic model)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-298-1.pdf}

As can be seen, \texttt{POpt} is quite far from the optimal area suggested by the PrefMap. This suggests that prototypes with higher success chances could be developed.

\hypertarget{value-delivery}{%
\chapter{Value Delivery}\label{value-delivery}}

\begin{quote}
Most of this book focus on handling data through the procedure of data cleaning, transformation, analyses, or representation. This makes sense since the aim is to present data science in the context of sensory and consumer research. But these steps may be irrelevant if analysts or reseachers are not able to communicate their findings efficiently. Effective communication is as much important as any other step and so, we will guide you through that, providing some valuable tips and practical examples. This chapter approaches important topics to help you reach the goal of successful communication, from understanding the distinction between different analysis and audience awareness to methods to communicate, the art of storytelling, and ultimately reformulate.
\end{quote}

\hypertarget{how-to-communicate}{%
\section{How to Communicate?}\label{how-to-communicate}}

Sensory and consumer scientists often act as consultant whether it be for their own company or for customers. Being able to communicate effectively is perhaps one of the most important skills they should master. Communication is a simple act of transferring information and although undervalued by many, plays a key role in any business's success. Let's start this chapter reminding that there are different ways to communicate, and this process usually includes a combination of two or more types of languages, which are:

\begin{itemize}
\tightlist
\item
  \textbf{Vocal} -- the language produced by articulate sounds. It is the language used in client's meetings and presentations for instance.
\item
  \textbf{Non-verbally} -- is related to the body language, gestures, and the tone and pitch of the voice.
\item
  \textbf{Written} -- is the representation of a spoken language in a writing system, like proposals, technical documents, or final reports.
\item
  \textbf{Visual} -- is the communication using visual elements, such as the visual quality of presentations or other written documents, including formatting, logo, colors, figures, plots, etc.
\end{itemize}

It is through effective communication that you will bring potential clients' attention and interest in your company and the services you provide, that will make you truly understand your client's needs, gain their trust and provide the right solutions that will ultimately bring to a long-term partnership. Efficacious communication will be responsible for keeping a friendly relationship and your clients' commitment throughout the project development and will also help you to properly convey the outcomes of a project in a way that will at least meet (or better surpass) your clients' expectations, and opens possibilities for follow-up engagements and/or recommendations.

The ability to communicate accurately, clearly and as intended, is definitely something that consultants should not overlook because although it seems straightforward, it involves a number of skills that may take several years of practice to master. You will find plenty of materials on the Internet and books to help you to understand better and develop your very basic skills for effective communication. We will not focus on that in this chapter, but it is worth highlighting some important aptitudes for vocal communication, which may configure one of the most powerful types of communication with your client:

\begin{itemize}
\tightlist
\item
  \textbf{Confidence}: Being confident makes you be to be seen as an expert on the topic and as having the situation under control. The audience will be more likely to trust, believe, be connected, and give credit to a confident person.
\item
  \textbf{Passion and Enthusiasm}: Be passionate about what you do, and convinced/enthusiastic about the solution you provide. The audience can easily capture that on your vocally and non-verbally language and will be much more interested if they can see and feel your passion.
\item
  \textbf{Ability to be succinct}. No matter how interesting you feel about a topic, you must know that the audience will lose interest after some time, especially if there is a lot of technical and detailed information. Be aware that the attention span of your audience isn't long, so use your time wisely during the presentation, keeping it short and at the point.
\item
  \textbf{Feeling}. This is a skill that definitely one needs time to master, but it is crucial that you pick what is going on with your client, if they seem to be understanding and following you, or if they seem to be confused or not sure about what you are talking about.
\end{itemize}

In this chapter, we focus on four topics that we believe any successful consultant should have in mind which are: Exploratory, Explanatory and Predictive Analysis; Audience Awareness; Method to Communicate; and Storytelling.

\hypertarget{exploratory-explanatory-and-predictive-analysis}{%
\section{Exploratory, Explanatory and Predictive Analysis}\label{exploratory-explanatory-and-predictive-analysis}}

As a consultant in the field, you likely are in a position where you get data from your client and review it, do the analysis, and ultimately, convey the results. And here is where it is important to make a clear distinction between exploratory and explanatory analysis. \textbf{Exploratory analysis} is the stage where you dig into the data, get to understand them, figure out patterns and things that may be interesting or important to highlight. The \textbf{explanatory analysis} is the ability, from the learning from the previous step, to select and/or reorganize your data (by remaking your tables, plots, or charts) in a way you can easily convey the message to your audience, and ultimately make them understand and focus on the things that are worthy. Let's discuss that a little bit more.

The hard work starts once you get the data. This is the time you will likely analyze it in multiple ways, make several plots, and look at the data from multiple angles. This is what we call, exploratory analysis! After understanding all the analysis, it may be tempting to show the audience everything, all the steps, decisions, different plots, and approaches you have taken, but \textbf{do not do that}. You don't want to overload your audience making them go through the same tough path you went. Instead of showing your handwork, the robustness of your analysis, and building up your credibility, you will make your audience confused, bored, and lacking interest.

Once you have done all the hard work on data analysis, it is the moment to take some time to stand back and look at the key findings and the message(s) you want to convey. It is important to keep in mind that there is always a balance to find between presenting quantified, accurate, and credible information (i.e.~with sufficient details) and presenting information that makes sense, is relevant and that is easily readable and understandable. This challenging phase is what we call explanatory analysis! This is the moment you need to use your ability to translate an extensive, detailed, and complex version of your data analysis to a more concise/holistic version, to a version that will easily and clearly convey the message and highlight the main points. Keep in mind that the explanatory analysis has to be tailored according to your audience (as discussed in the next topic), which means that the way you present the data analysis and the level of details provided vary if you are presenting it to a group of experts in the field, including statisticians and mathematicians or in a lecture for a very diverse audience, as in a conference for instance. You need to find the right balance!

Some examples in the field to exemplify the two extremes (too complicated or too simple):

\begin{itemize}
\tightlist
\item
  \emph{Factorial maps} -- The overuse of factorial maps is a common practice in the sensory and consumer science field. It's a great tool to explore data, to make or confirm hypotheses, but maybe not the best to communicate since not so many people can correctly read and interpret them. Therefore, a good approach would be to initially work with the factorial map to interpret and draw conclusions, but then, find another way through tables or alternative charts (that may be simpler to understand) to communicate the findings to your audience.
\item
  \emph{Spider Plots} -- This is the other extreme when consultants can fail but not because they present a very complex and extensive analysis, but because they decide to show the data in such an easy way that puts at risk important information that should be captured. The use of spider plots is still a common practice that many people can easily understand, but the problem is that this analysis is so simple that it can mask sensory complexity.
\end{itemize}

It is worth noticing that there is a third type of analysis in the data science field, \textbf{predictive analysis}. This is a hot topic in the area that involves techniques such as data modeling, machine learning, AI, and deep learning. Instead of being focused on exploring or explaining the data, predictive analysis is concerned with making successful predictions, in ensuring that the predictions are accurate. Examples of this approach include face recognition and text-speech transcription. Eventually, some models can be studied to provide insights, but this is not always the case.

\hypertarget{audience-awareness}{%
\section{Audience Awareness}\label{audience-awareness}}

One of the most important things about being a successful consultant is Audience Awareness! No matter how good you, your team, and the service or product your company offers, if you fail to communicate with your target audience, the message will not get through. Knowing the audience, who the target people are is the cornerstone of any successful business. Knowing your audience makes you be better able to connect to them.

In order to know your audience, you must gather some information about them beforehand, such as:

\begin{itemize}
\tightlist
\item
  \textbf{Background}: Do they have a sensory science background? Statistical background? Do they have experience in data science, including R language? Do they have experience in automated reporting dashboards, machine learning, etc? If so, are they juniors, specialists, seniors?
\item
  \textbf{Role}: What is the role your audience plays in the project? Are they the decisions makers? Are they the final users of a dashboard, for instance?
\item
  \textbf{Vocabulary}: Will your audience understand very technical terms, or do you need to use simplified terms to convey the same message? This topic is closely related to the audience's background.
\item
  \textbf{Expectations}: What is your audience expecting in a presentation or final report? A short summary of the project's outcomes? A detailed explanation of the statistical analysis including appendixes with further details? Recommendations for follow-up projects? Interpretation and conclusion of the analysis?
\end{itemize}

In general, according to the profile, the audience will likely fall into one of the three categories: \textbf{Technical Audience}, \textbf{Management (Decisions Makers)}, and \textbf{General Interest}. There is no magical formula on how to deal exactly with each of these types of audience, but in general, based on our experience, we must highlight that the key differences are in the focus, language, level of technical and detailed information you need to provide for each of those target public. In general, it tends to be necessary a higher level of details and technical information and a lower level of the big picture once you move from your general interest audience to the management, and further down to the technical audience (Figure \ref{fig:trade-off}). We will further discuss the main differences between each audience above.

\begin{figure}
\includegraphics[width=0.9\linewidth]{images/trade-off_value_delivery} \caption{Trade-off curve based on the level of technical details and the big picture for each type of audience.}\label{fig:trade-off}
\end{figure}

\hypertarget{technical-audience}{%
\subsection{Technical Audience}\label{technical-audience}}

The technical audience refers to the ones who will likely have a significant background and experience in or related to the field you are providing consulting service (e.g.~Sensory Scientists, Statisticians, Data Analysts, Data Architecture, or your client's IT group). They are the team that will likely be working closely with you throughout the project development, at different stages. This type of audience is usually more exigent and/or engaged and because of its expertise in the field, will likely be expecting a presentation, report, or any other technical document in a higher level of details and with a more technical vocabulary, otherwise, you will sound that you are not an expert in the topic. This audience usually needs a lower sense of the big picture of the project, which means, that they are less interested in the details like the timelines, main outcomes, etc. But be aware, it is very important to still be able to distinguish between different technical audience (e.g.~don't use sensory technical language to talk to the IT team).

\hypertarget{management}{%
\subsection{Management}\label{management}}

Although a person in a management position (e.g.~Sensory Manager or Director and Principal/Senior Scientist) likely has a broad experience and background in the field, they tend to be more interested in the whole picture, which means timelines, progress of the project, potential issues, outcomes, applicability, next steps, etc. A person in a management position has many other projects and roles in a company and will not have time to be involved in the details. Instead, they likely designate a team (your technical audience) to be closely involved. In this case, you should be more concise in a meeting, presentation, or report, for instance. It is advised to keep a certain level of technical language, but it is better to present things in a simpler way and in a lower level of details than you would do for the technical audience. Additionally, the focus should be different, since as we mentioned, this audience is likely to be more interested in the whole picture instead of the specifics of the project.

Another distinct type of audience that falls into the management audience would be the executives, as a VP of Research \& Regulatory for instance. This public is not necessarily from the field and has even less time and/or background to absorb the specifics. The focus should be the same (whole picture) but with even fewer technical details. The approach and language of this audience tend to be closer to the general interest.

\hypertarget{general-interest}{%
\subsection{General Interest}\label{general-interest}}

The general audience usually refers to the ones that are likely the final users or are somehow related, contributed, or are interested in the project. In this way, this public is usually the least interested in the details and the most interested in the whole picture. The general audience usually refers to a larger group of people with different backgrounds and distinct levels of expertise, for instance, an R\&D internship, a Chemistry Researcher, and a Senior Sensory Specialist can be all final users of a dashboard you developed. In this case, to make sure everyone follows you in a training (say), you must use less technical language and a lower level of details, otherwise you will lose part of your audience's attention. But at the same time, you may need to consider covering things that sound obvious to you, you ought to be careful about not skipping topics assuming that everyone knows about that or using certain terms and expressions considering that is evident for all. This can be the most challenging audience to deal with due to its diversity, but in a meeting, training session, or presentation, you should be very attentive and use your feeling to capture what's going on and maybe change your position to better connect with the audience

A valuable tip shared by Cole Nussbaumer, in her book Storytelling with data, is to avoid general audiences, such as the technical and management team at the same time, or general audience such as ``anyone related to the field that might be interested in the project''. Having a broad audience will put you in a position where you can't communicate effectively to any of them as you would be if the audience was narrowed down.

\begin{itemize}
\tightlist
\item
  \textbf{Example}
\end{itemize}

We will use the PCA biplot from the biscuits sensory study shown in the previous chapter and point out the main differences in the approach according to the audience. As a quick reminder, 11 breakfast biscuits with varying contents of proteins and fibers were evaluated in this study. Products \texttt{P01} to \texttt{P09} are prototypes, product \texttt{P10} is a standard commercial biscuit without enrichment and the eleventh product (\texttt{Popt}) is an additionally optimized biscuit.

Let's picture a situation where the R\&D team has been developing multiple trials for the biscuit formulation, changing the concentration/ratio of protein and fiber, with the objective to have a product with a sensory profile as close as possible to the commercial biscuit. For this exercise, your role as a consultant was to support the R\&D team designing the study and conducting the analysis and ultimately analyzing ad interpreting the results to make the final conclusion (Figure \ref{fig:pca}).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PCA} 

}

\caption{PCA Biplot Biscuit Study.}\label{fig:pca}
\end{figure}

We won't go deep into the interpretation since it's not the focus of this example, but rather point out the approach we would recommend for each type of audience as shown in the Table \ref{tab:summarytable1}).

Following those recommendations, your PCA for the management or general audience may look like Figure \ref{fig:pcamanagement}.

Note that the PCA was simplified for a more straightforward understanding:
- the PCA variance explanation and grid lines were removed,
- attributes were slightly moved to avoid overlap,
- the samples and attributes with lower interest were given a lighter color
- the attributes and samples we want our audience to focus their attention were given a different, stronger color,
- pictures and a more appealing description were used instead of the samples codes
- some strategies as to circle the important area/group of samples and attributes helps the audience to focus on what we deem most important to extract from this analysis were adopted.

In this example, the idea may be to highlight to the audience that the optimized formulation is in fact closer to the commercial one, and to increase even more this similarity, some attributes, like sour, salty, overall flavor persistence, fatty flavor, and fatty in mouth, has to be increased.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/PCA_management} 

}

\caption{PCA Biplot Biscuit Study Modified.}\label{fig:pcamanagement}
\end{figure}

\hypertarget{methods-to-communicate}{%
\section{Methods to Communicate}\label{methods-to-communicate}}

How will you communicate to your audience? Are you going to deliver a live presentation? Are you going to present a proposal in a live meeting? Or will the communication be a written document you will send through email?

What is the format you will be using to communicate? Word, Excel, or PowerPoint? Are you going to send the document in PDF format? Are you going to present a dashboard? Are you going to share R Scripts?

As we will discuss in more detail later, the way and the format you use to communicate to your clients or audience have a huge impact on successful communication, and you should be well aware of that!

\hypertarget{consider-the-mechanism}{%
\subsection{Consider the Mechanism}\label{consider-the-mechanism}}

You should be aware that the primary method or mechanism you use to communicate strongly affect the way your audience effectively gets the information and so you should tailor it accordingly. One of the most important aspects is related to the amount of control you have over the audience, how they get the information, and hence the level of details needed (\citet{Knaflic2015}).

In a live presentation, for instance, you are in full control. You can answer questions your audience may have, you can slow down and go into a particular detail you deem important, or you can speed up over repetitive, obvious, or not-so-important topics. In short words, you are the expert there and so, you can easily provide effective communication, and because of that, you don't need to overcrowd your slides or any other document and divert or lose your audience's attention with unnecessary information. You can for instance just show a plot or graph and a very simple interpretation or bullet points because you are covering vocally the nuances and details about that.

In the case the communication si done through a written document in a non-live situation, you have much less control over your audience, on how they will take the information, on whether they will get the main point. In this situation, you need to be more careful and likely provide a higher level of details to answer or clarify potential questions or doubts your audience may have. In this situation, showing a plot or graph and just a very simple interpretation or bullet points is likely not enough.

It can be a great idea to merge those two formats, when possible, where you can give time to the audience to consume the information on their own for a while and give the topic thought and a moment where you can discuss it in a live situation, not in this order necessarily. So, for example, let's pretend you have to present a proposal for a client. Instead of sending a dense document to explain all the details and just wait for the client's response, you can make a more concise document, easier to go through if you have a live moment with the client. You can for instance present the proposal initially in a live meeting, where you cover in general all the important topics and details, and then send the written document to the client.

\hypertarget{pick-the-correct-format}{%
\subsection{Pick the Correct Format}\label{pick-the-correct-format}}

The second point of this topic on the method of communication is related to the correct format to pick. There are certainly many ways for you to communicate with your client - word, excel, or PowerPoint whether in pdf format or not, dashboard, or even scripts -- but surely one is the most suitable. Again, there is no universal answer for the best format to pick since it may vary according to clients' requests and the type of project you are dealing with. But there is one thing you should always follow, unless strictly necessary, do not share documents in an editable format. You may use Word to write proposals or final reports, Excel for plots or tables, and PowerPoint for live presentations, and that's totally fine, but never share that in the editable format. We always recommend saving in pdf format to share with your client or audience and this is because of two simple reasons. First, the pdf format cannot be modified! You definitely don't want to take the risks of others changing your document which can lead to misunderstanding, putting you in a delicate situation. Second, the pdf format preserves document formatting which means that it retains the intended format if the file is viewed online or printed. In short words, it is very unprofessional to share documents in editable format.

You may be wondering, so in what type of situation would you share an editable format? When would you share a document in Word, PowerPoint, or Excel? In the situation where you are working with a partner for instance. So, for example, a project that you are working on involves multiple partners and one unique report or presentation. In this case, it may be convenient to share the Word or PowerPoint document for each of the partners to include their inputs. After the document is ready, make sure you carefully review the formatting and save it in pdf before sending it out to the client.

There are two other formats that may be common in the sensory data science field, which are dashboards and scripts. If you are developing a graphical user interface for your client, you need to deploy the dashboard at some point to a server for your client to be able to access it. The deployment can be done in two ways: web-based, as a simple client web page, or locally, as a locally installed desktop application. The choice should be based on the client's preference.

The last method of communication that is fairly common in the field is R or any other programming language script. It is very common that the client requests the scripts used for a specific project, the text file containing the set of commands and comments you used for instance to develop an automated analysis reporting dashboard. You can share the repository where the scripts are hosted, or you can zip the scripts and share them with your clients. The details should be discussed with the client's IT team since each company has a particular preference. As the scripts should always be available under the client's requests, you should be careful to not display sensitive or confidential information by reusing codes or throughout the comments.

\hypertarget{storytelling}{%
\section{Storytelling}\label{storytelling}}

There are basically two ways to communicate with our audience, the first is called conventional rhetoric. A PowerPoint full of facts, filled with bullet points and statistics with a presenter with a formal and memorized speech and using the same voice tone, would be the best way to illustrate the conventional rhetoric style. This way to communicate, which drove the businesses of the past, has a more analytical approach, where statistics, charts, metrics would be dumped on the audience and left to them to digest. There is no need to say that this approach is completely outdated, it clearly fails to stimulate the audience's attention or evoke their energy or emotions.
The second way to communicate, which is the last topic we want to cover in this chapter and also happens to be a critical skill for any successful consultant, is through storytelling! Storytelling is something that we all know, from an early age we were introduced to the notion of narrative structure, which means a clear beginning, middle, and end. The ability of one going throughout this structure to tell us a story is what makes a book, play, or movie grab our attention and evoke our emotional responses, is what makes it interesting! In short words, storytelling is one of the most powerful and effective ways to attract people's attention because we were taught to communicate with stories throughout history. This universal language that everyone can understand has the power to truly engage your audience because it translates abstract facts, numbers, and plots into compelling pictures; it inspires, motivates, and drive actions because it taps into people's emotions.

As described in the book \emph{Once Upon an Innovation}, by Jean Storlie and Mimi Sherlock (\citet{Sherlock2020}), the left side of our brain is linked to more logical and analytical thinking, including data processing, number handling, and statistical interpretations. The right side is linked to expression, emotional intelligence, and imagination, and in our context, will be the part of our brain that will capture the big picture, that will turn data and facts into possibilities and innovative ideas. If you as a consultant overwhelms your audience with analytics you will reduce their capacity for big picture thinking, you will shut down their capability to generate novel ideas and solutions. We are not saying that numbers, plots, and facts are not important, but that they should be presented in a story narrative format, in a way that will be able to light up the right side of the brain, and this stimulation of both paths is what trigger unexpected and novel solutions, inspire support and drive changes.

In a real situation, you as a consultant have many pieces of information that you have collected throughout the journey with a client, from the very first communication until the end of a project. You have valuable information about your client company's situation, challenges and issues, needs, expectations, potential solutions and/or failed attempts, and final outcomes. Storytelling is the master of tying all together and articulating it into the context of a story in a creative way to engage and persuade your audience. A good story allows you to successfully connect with your audience, it makes your audience understand, reflect, and act in a way that plots, numbers, and facts altogether simply can't.

You may be wondering. How exactly should I construct a story? What should be covered in each piece of the narrative? We will provide here a summary of the pieces of a good story and the specifics based on our experience and also on the books Storytelling with Data by Cole Nussbaumer (\citet{Knaflic2015}) and Beyond Bullet Points by Cliff Atkinson (\citet{Cliff2018}). Both books dedicate a good part to Storytelling making them a great resource on this topic.

\textbf{The beginning (Context)}

The key piece of any story is the context, the description of the situation, and surrounding details. This first step is the moment to set up the essential information or background on the topic you will be covering to get everyone on common ground. You should initially spend time to make sure your audience clearly understands the context, why this is important or necessary, and why they are there before diving into actions or results. Subsequently, you will raise the challenges or problems and propose some recommended solutions. It is at this very first step that you will first grab your audience's attention. If you fail at this moment, it is very unlikely that you will recover their interest in the subsequent steps.

For live presentations, it is strongly recommended to use the first few minutes to be an icebreaker to make everyone feel more comfortable and create a more friendly environment. In order to do so, you can start introducing yourself in case you haven't met everyone yet, you can have a conversation and talk about the latest news, ask about how they and their families are doing, etc. The second piece of advice is to start the presentation by stating bullets of the main points that will be covered, so your audience will have awareness of what you will be talking about.

\textbf{The middle (Action and Impact)}

Now is when you get to the crux of your story, it is at this moment that you will explain your solutions or actions and highlight the impacts. You will continue it in a way you will convince your audience of the solution you are proposing or make them clearly understand, agree, and be excited about the outcomes and possibilities of a solution you worked on. You should be careful to retain your audience's attention addressing how they can be part and/or benefit from the solution you are referring to. In the case of a live presentation, pose always confidently, show enthusiasm about what you are talking about, and watch out for hidden clues, try to constantly catch your audience's response/feedback through their expressions and body language.

The content to build out your story at this moment is very dependent on the context of the situation, but from a consultant perspective, it will be likely the moment you will further develop the situation or problem covering relevant information, show some data to illustrate the situation, discuss potential solutions to address a particular topic or present the outcomes of your project.

\textbf{The end (Conclusion)}

This is the moment you close your story; it is when you should tie it back to the beginning to somehow recap the problem, highlight the basic idea and conclude the story. You should finish your presentation in an impactful way, re-emphasizing and repeating your main point, what you want to stick deeply in your audience's mind. Once more, the content at the end of the story can be somehow dependent on the context of the situation, but in a consulting world, it would likely include a conclusion of the topic and also next steps and further recommendations.

\hypertarget{reformulate2}{%
\section{Reformulate}\label{reformulate2}}

Something important to keep in mind is the follow-up process after a report is sent or a presentation is delivered. The ability to receive feedback and reformulate is undoubtedly a very important and sometimes challenging skill that consultants seeking success should be aware of. It may be challenging since some consultants can be reluctant to feedback because of a misconception that they are the expert in the field and hence, their approach is the best. So, one of the most important rules, regardless of the expertise and knowledge you have in the field, is to be humble! Consultants must understand the idea that: 1) you need to make your client pleased unless you have a strong reason not to do so like ethical reasons or statistical rules and 2) your point of view can be biased over time and your client's request may indeed improve the clarity of an outcome for instance. Or, you can simply be wrong, miss something and have not taken the best approach. It happens! So, be open to feedback and be prepared to reformulate!

Sometimes the client feedback is something very minor, to adjust the scale of a plot, match the color with the company's palette or change the type of plot. In other cases, the feedback will demand a bit more time. It is common that the way you deemed best to present the outcomes is not that clear from your client's view or the set of data or plots you selected did not convey the message you were expecting or in an extreme situation, your client does not agree or ask you to redo an experiment or procedure. In this case, you will need to dedicate more time to address your client's request.

Regardless of the situation, you should be motivated and be open mind to your client's feedback and afterward carefully work on that to tackle it all at once. You definitely want to avoid a situation where your report or presentation be back and forth with your client. It is recommended that you make all possible changes and prepare a convincing explanation for the things that you strongly do not agree with or have a solid reason not to do so. Ideally, you should get back to your client as soon as possible highlighting the changes that were made and explaining the ones not addressed.

\hypertarget{machine-learning}{%
\chapter{Machine Learning}\label{machine-learning}}

\begin{quote}
Artificial Intelligence (AI) and Machine Learning (ML) in particular have gained a lot of attention in recent years. With the increase of data availability, data storage, and computing power, many techniques that were just \emph{dreams} back then are now easily accessible, and used. And of course, the sensory and consumer science field is not an exception to this rule as we start seeing more and more ML applications\ldots although in our case, we do not have \emph{Big Data} per se, but we do have \emph{diverse data}!
For many of us, AI and ML seems to be a broad and complex topic. This assertion is true, and in fact it would deserve a whole book dedicated just to it. However, our intention in this chapter is to introduce and demistify the concept of ML, by:
1. explaining the differences between supervised and unsupervised ML models,
2. proving that you were already doing it long ago, perhaps whithout knowing,
3. extending it to more advanced techniques,
4. highlighting its main applications in the field.
To do so, some basic code and steps will be provided to the reader to get familiar with such approach. Throughout this chapter, some more specialized resources are provided for those who have the courage and motivation to dig deeper into this topic.
\end{quote}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

Machine Learning is currently a hot topic in the sensory and consumer science field. It is one of the most game-changing technological advancements to support consumer packaged goods companies in the development of new products, playing a considerable role in speeding up the R\&D process (and at the same time reducing the costs). In today's fast-moving and increasingly competitive corporate world, companies that are embracing, adopting and opening their minds to digital transformation and artificial intelligence (AI), moving towards the age of automation, are not one but many steps ahead of their competitors.

Machine Learning (ML) is a branch of AI, which is based on the idea that systems can learn from data, and that has the capability to evolve. Generally speaking, ML refers to various programming techniques that are able to process large amounts of data and extract useful information from it. It refers to data analysis methods that build intelligent algorithms that can automatically improve through the experience gained from the data and can identify patterns or make decisions with minimal human intervention, without being explicitly programmed. ML focuses on using data and algorithms to mimic the way humans learn, gradually improving their accuracy.

Defining the objectives or the situation where ML would bring value is the very first step of the process. Once that is clear, the next step is to collect data or dig into historical data sets to understand what information is available and/or has to be obtained. The data varies according to the situation, but it may refer to product composition or formulation, instrumental measurements (e.g., pH, color, rheology, GC-MS, etc.), sensory attributes (e.g., creaminess, sweetness, bitterness, texture, consistency, etc.), consumer behavior (e.g., consumption frequency, use situation, dietary constraints, etc.), demographics (e.g., age, gender, size of household, etc.) and consumer responses (e.g.~liking, CATA questions, JAR questions, etc.) just to name a few.

First of all, it should be stressed that the size of the data set and its quality are very important as they impact directly the model's robustness. Here are general recommendations (to be adapted to each situation, data type, and objectives):

\begin{itemize}
\tightlist
\item
  The higher the number of statistical units the better, 12-15 being the minimum recommended when statistical units correspond to samples.
\item
  The number of measurements (instrumental, sensory and/or consumer measurements) and the number of consumers evaluating the products are also very relevant to the model's quality. In practice, a minimum of 100 participants is usually recommended for consumer tests, which is deemed sufficient to apply ML (although here again, the more the better).
  For data quality, the variability of the samples is one of the most important aspects (besides the standardization of data collection). The larger the variability between samples, the broader the space the model covers. Additionally, it is strongly recommended to capture the consumers' individual differences, not only through demographic information, but also through perception (including with rapid sensory description methods, Just About Right (JAR) or Ideal Profile Method (IPM)). Eventually, within-subject design (i.e.~sequential monadic design) provide better quality models as they allow accounting for individual response patterns.
\end{itemize}

\hypertarget{introduction-of-the-data}{%
\section{Introduction of the Data}\label{introduction-of-the-data}}

For this section, we use the wine data set from the \texttt{\{rattle\}} package (\url{https://rdrr.io/cran/rattle.data/man/wine.html}). This data consists of the results of a chemical analysis of wines grown in a specific area of Italy. In total, the results of 13 chemical analyses (e.g., alcohol, malic acid, color intensity, phenols, etc.) are provided for 178 samples that represent three types of wines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(rattle)}

\NormalTok{wine }\OtherTok{\textless{}{-}}\NormalTok{ rattle}\SpecialCharTok{::}\NormalTok{wine }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{machine-learning-methods}{%
\section{Machine Learning Methods}\label{machine-learning-methods}}

The notion of Machine Learning is vast, as it covers a large variety of analyses. In fact, ML algorithms are often classified based on the goals of their analysis. Three main groups are often considered:

\begin{itemize}
\tightlist
\item
  Unsupervised Learning:
\end{itemize}

Unsupervised ML aims at finding structure within the data. Input are unlabeled data, meaning that no \emph{output} values are yet known. In this case, the algorithms operate independently from any information about the data to find patterns and trends. For instance, this is achieved by learning from the data distribution the features that distinguish between statistical entities using similarity and dissimilarity measurements.
Such ability to discover \emph{unknown} patterns in the data makes such algorithms ideal for exploratory analysis. In Sensory and Consumer Science, the best known Unsupervised ML techniques are Principal Component Analysis (PCA) for dimensionality reduction and hierarchical cluster analysis (e.g.~for consumer segmentation).

\begin{itemize}
\tightlist
\item
  Supervised Learning:
\end{itemize}

Supervised ML is arguably the most popular type of ML: When people talk about ML, they often refer to Supervised techniques. Supervised ML takes labeled data as input, meaning that the statistical entities are defined by one or more output variables. The aim of the algorithm is then to a find a mapping function that connects the input variables with those output variables. Ultimately, the ML model aims to explain output variables using the input variables.
A common situation requiring Supervised ML in Sensory and Consumer Science consists in predicting consumer responses (e.g.~liking) using sensory descriptions, analytic data, demographics, or any other information. ML models provide insights on how to improve product performance, and allow predicting consumer responses of new prototypes or products. Another common situation is to use Supervised ML to predict the sensory profile of products using formulation data (i.e., ingredients and process parameters).

\begin{itemize}
\tightlist
\item
  Semi-supervised Learning:
\end{itemize}

Semi-Supervised ML is not an ML approach per se. Instead, it is a combination of both Unsupervised and Supervised approaches. It first aims to create an output variable using Unsupervised techniques, and then to explain or use this output variable using other information through Supervised ML. A good example of Semi-Supervised approach consists in defining clusters of consumers based on liking (unsupervised), and to characterize these clusters using demographic data using decision trees for instance (supervised). External Preference Mapping is another example since it first reduces dimensionality of the sensory data through PCA (unsupervised), and then uses these dimensions to explain the consumers' liking scores using regressions (supervised).

\begin{quote}
A forth type of Machine Learning is called \emph{Reinforcement Learning} that relies on feedback provided to the machine. It is a technique that enables an agent to learn through trial and error from its own actions and experiences. Reinforcement Learning is commonly used in some tech applications (e.g.~gaming and robotics), specially when large data sets are available. Such approach has little reach in sensory and consumer science at the moment. Therefore, we are not going to develop it further here.
\end{quote}

\hypertarget{unsupervised-machine-learning}{%
\section{Unsupervised Machine learning}\label{unsupervised-machine-learning}}

In sensory and consumer science, unsupervised learning models are mainly used for Dimensionality Reduction and for Clustering.

\hypertarget{dimensionality-reduction}{%
\subsection{Dimensionality Reduction}\label{dimensionality-reduction}}

Dimensionality reduction is a technique used to transform a high dimensional space into a lower dimensional space that still retains as much information as possible. In practice, the original high-dimensional space involves many variables that are correlated with each other, but that could be summarized by \emph{latent} variables or \emph{principal components}, which are orthogonal to each other.\footnote{When all the \emph{principal components} are considered, none of the information present in the raw data is lost, and their representation is simply shifted from an unstructured high-dimensional space to a structured low\emph{er}-dimensional space.}

Most frequently, dimensionality reduction is performed for the following reasons:

\begin{itemize}
\tightlist
\item
  Summarizing Data (and removing redundant features);
\item
  2D or 3D visualization of the data (most important information);
\item
  Finding latent variables and untangling initial variables;
\item
  Pre-processing data to then reduce training time and computational resources;
\item
  Improving ML algorithms accuracy by removing the lower dimensions (the one containing less information) often considered as \emph{noise};
\item
  Avoiding problems of over-fitting.
\end{itemize}

Some of these approaches were presented earlier in this book, in particular in Section \ref{data-analysis}. However, there are numerous dimensionality reduction methods that can be used depending on the data at hand. The most common and well known methods used in the sensory and consumer science are the ones that apply linear transformations, including Principal Components Analysis (PCA), Factor Analysis (FA), and derivatives such as (Multiple) Correspondence Analysis, Multiple Factor Analysis, etc.

Let's apply this technique to the \texttt{wine} data. To get familiar with the data, we can first visualize the information on a 2D plot, and then reduce the data set to the first 2 dimensions only.

Since the different variables represent analytical measures that are defined using different scales, a standardized PCA is performed. This is the default option in \texttt{PCA()} from \texttt{\{FactoMineR\}}(\texttt{scale.unit=TRUE}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(FactoMineR)}
\NormalTok{res\_pca }\OtherTok{\textless{}{-}} \FunctionTok{PCA}\NormalTok{(wine, }\AttributeTok{quali.sup=}\DecValTok{1}\NormalTok{, }\AttributeTok{scale.unit=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{graph=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The results of the PCA can be visualized using \texttt{\{factoextra\}}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{ (factoextra)}
\CommentTok{\# fviz\_eig(res\_pca, addlabels=TRUE, ylim=c(0,50))}
\FunctionTok{fviz\_pca\_biplot}\NormalTok{(res\_pca, }\AttributeTok{repel=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{label=}\StringTok{"var"}\NormalTok{, }
                \AttributeTok{col.var=}\StringTok{"red"}\NormalTok{, }\AttributeTok{col.ind=}\StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-301-1.pdf}

The first plane of the PCA suggests that there are 3 distinct groups of wines. Let's define them mathematically using cluster analysis. For this process, we propose to reduce the full data to its first two components only. This approach is used here to illustrate how PCA can be used as a pre-processing step. Additionally, such pre-processing can help detecting clearer patterns in the data, as we will see in the next section with clustering.

\begin{quote}
Note that such use of PCA as a pre-processing step was already done earlier in Section \ref{prefmap} when the sensory space was reduced to its first 2 dimensions before performing the external preference mapping.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wine\_reduced }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(res\_pca}\SpecialCharTok{$}\NormalTok{ind}\SpecialCharTok{$}\NormalTok{coord[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

Clustering is a technique used when dealing with high-dimensional data to discover groups of observations that are similar to each other (or different from each other). In other words, it is a method that groups unlabeled data based on their similarities and differences in a way that objects with strong similarities are grouped together, and are separated from objects to whom they have little to no similarities.

Again, a very common application in S\&C Science is to segment consumers based on a variety of factors such as shopping or usage behavior, attitudes, interests, preferences. As consumers being associated in the same market segment tend to respond similarly, segmentation is a key strategy for companies to better understand their consumers, and tailor effectively their products or marketing approaches for the different target groups. Similarly, it is also used to classify products in homogeneous groups based on their analytical, sensory and/or consumer description in order to analyze the product offer and identify different segments on the market.

There are any clustering approaches and algorithms. They can be categorized into a different types including \emph{exclusive} (e.g.~k-means), \emph{hierarchical} (see Section \ref{hac} for an example) and \emph{probabilistic} (e.g.~Gaussian Mixture Model). The first two are most widely used and well known in the sensory field.

Although agglomerative hierarchical clustering (HAC) is more common in sensory and consumer research, an example illustrating such approach was already provided in Section \ref{hac}. For that reason, here we propose to present another approach using \emph{k-means}.
K-means clustering is a popular unsupervised machine learning algorithm for partitioning a given data set in a way that the total intra-cluster variation is minimized. Both approaches (HAC and k-means) however differ in their ways of forming clusters, For instance, the algorithm of k-means require the user to pre-specify the number of clusters to be created, whereas HAC produces a tree (called dendrogram) which helps visualizing the data hierarchical structure and deciding on the optimal number of clusters. Detailed information about clustering methods and analysis can be found in the book \emph{Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning} by Alboukadel Kassambara (\citet{Kassambara2017}).

In order to cluster our wines, let's start with defining the optimal number of clusters (k) to consider. This can be done using \texttt{fviz\_nbclust()} from \texttt{\{factoextra\}}. This function creates a graph which represents the variance within the clusters. On this representation, the bend (also called \emph{elbow}) indicates the optimal number of clusters, any additional cluster beyond that point has less value.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_nbclust}\NormalTok{(wine\_reduced, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/find_opt_cluster_number-1.pdf}

Here, the optimal solution consists in defining 3 clusters.

Next, the k-means algorithm starts with randomly selecting k (here 3) centroids. In order to be able to reproduce our results (despite the randomness), we propose to initially set a seed (through \texttt{set.seed\ ()})\footnote{\texttt{set.seed()} fixes random selections so that they can easily be reproduced.}. Otherwise, it is recommended to set within \texttt{kmeans()} a number of random sets, i.e.~the number of times (here 20) R will try different random starting assignments. Increasing this number yields more stable results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{wine\_kmeans }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(wine\_reduced, }\AttributeTok{centers=}\DecValTok{3}\NormalTok{, }\AttributeTok{nstart=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, the results can be visualized using \texttt{fviz\_cluster()} from \texttt{\{factoextra\}}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{data=}\NormalTok{wine\_reduced, }\AttributeTok{cluster=}\NormalTok{wine\_kmeans}\SpecialCharTok{$}\NormalTok{cluster),}
             \AttributeTok{ellipse.type=}\StringTok{"norm"}\NormalTok{, }\AttributeTok{geom=}\StringTok{"point"}\NormalTok{, }\AttributeTok{stand=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-303-1.pdf}

In the resulting plot, the 3 clusters are clearly separated, as can be seen by their little to no overlap.

\begin{quote}
An interesting suggestion is to run the same analysis on the full data (here, we limited it to the first 2 dimensions of the PCA) and to compare the results.
\end{quote}

When applying such clustering techniques, one may sometimes encounter very atypical variables that could be deemed noisy. This may occur in when analyzing consumer hedonic data and finding few consumers who have an atypical response pattern. This may create enough `noise' in the data to blur the main data structure and affect the outcome of the k-means algorithm. To solve this problem, \citet{vigneau2016} have developed strategies to segment consumers while setting aside atypical or irrelevant consumers. This can be achieved by using either a \emph{noise cluster} where to dump `irrelevant' consumers or a sparse latent variable (Sparse LV) strategy. Both strategies have been implemented in the \texttt{\{ClustVarLV\}} package, and can be selected using ``kplusone'' or ``sparselv'' parameters in the \texttt{CLV\_kmeans()} function.

It should be noted that, in sensory and consumer science, it is increasingly more frequent to aim to cluster panelists, based not only on one variable (i.e.~their liking for a series of products), but on sets of several variables (i.e.~their description of products for a series of attributes, or their responses to a full online questionnaire) or even on distance matrices (as obtained from a free sorting task, for example). Segmentation strategies would also apply to such data.

For instance, Cariou and Wilderjans have developed an approach for clustering around latent variables for three-way data (CLV3W). This approach that is implemented in \texttt{\{ClustVarLV\}} could be used to detect panel disagreement in three-way conventional sensory profiling data \citep{wilderjans2016} or to segment consumers based on multi-attribute product evaluation, by removing the non-negativity constraint to the \texttt{CLV3W()} procedure \citep{cariou2018}. As for free sorting and projective mapping data, the CLUSTATIS partitioning algorithm could be applied using \texttt{\{ClustBlock\}} \citep[\citet{llobell2020}]{llobell2019}.

\hypertarget{supervised-learning}{%
\section{Supervised learning}\label{supervised-learning}}

There are many ways to carry out Supervised ML, which again would require an entire book dedicated just to it. In this section, we will introduce you to the basics, which should give you a nice kick-start for your own analysis. For those who want to learn more on this topic, we recommend reading ``Hands-On Machine Learning with R'', by Bradley Boehmke and Brandon Greewell (\url{https://bradleyboehmke.github.io/HOML}){[}\url{https://bradleyboehmke.github.io/HOML}{]}, and to ``Tidy Modeling with R'' by Max Kuhn and Sylvia Silge (\url{https://www.tmwr.org/}){[}\url{https://www.tmwr.org/}{]} for more in-depth information.

\hypertarget{workflow}{%
\subsection{Workflow}\label{workflow}}

In sensory and consumer science, supervised learning is commonly carried out using a regression type of analysis, where for instance consumer ratings are used as output (target), and product information (i.e.~sensory profiles and/or analytical measurement) are used as input. The goal of the analysis is then to explain (and sometime predict) the (say) liking scores using the sensory information about the products.

To do so, models are initially trained using a subset of the data (called \emph{training set}). Once obtained, the model is then tested and validated on another part of the data (called \emph{test set} and \emph{validation set})\footnote{It is common practice to split the data so that the model is built by using 60\% of the data, the remaining 40\% being used for testing and validation. It is however important to use separate data for these steps to avoid any over-fitting.}. Once this process is done, the model can be continuously improved, discovering new patterns and relationships as it trains itself using new data sets.

\hypertarget{regression}{%
\subsection{Regression}\label{regression}}

Regression methods approximate the target variable\footnote{This is a bit of simplification since in some cases, it is some transformations of combination of predictors that approximate the target variable, as in logistic regression for example.} with (usually linear) combination of predictor variables. There are many regression algorithms varying by type of data they can handle, type of target variable, and additional aspects such as the ability to perform dimensionality reduction. The most relevant methods for sensory and consumer science will be presented here.

\begin{itemize}
\tightlist
\item
  \textbf{Linear regression}:
\end{itemize}

The simplest and most popular variant is linear regression in which a continuous target variable is approximated as linear combination of predictors in a way that the sum of squares of the errors (SSE) is minimized. It can be for example used to predict consumer liking of a product based on its sensory profile, but the user has to keep in mind that linear regression can in some cases return predicted values outside the reasonable range of target values. This can be addressed by capping the predictions to a desired range. Functions in R to apply linear regression are: \texttt{lm()} and \texttt{glm()} or \texttt{parsnip::linear\_reg()\ \%\textgreater{}\%\ parsnip::set\_engine("lm")} when using the \texttt{\{tidymodels\}} workflow.

\begin{itemize}
\tightlist
\item
  \textbf{Logistic regression}:
\end{itemize}

Logistic regression is an algorithm which - by use of logistic transformation - allows to apply the same approach as linear regression to cases with binary target variables. It can be used in R with \texttt{glm(family\ =\ "binomial")} or \texttt{parsnip::logistic\_reg()\ \%\textgreater{}\%\ parsnip::set\_engine("glm")} when using the \texttt{\{tidymodels\}} workflow.

\begin{itemize}
\tightlist
\item
  \textbf{Penalized regression}:
\end{itemize}

Often, the data used for modeling contain a lot of (highly correlated) predictor variables. In such cases of multicolinearity, linear and/or logistic regression may become unstable and produce unreasonable results. This can be addressed through the use of so-called penalized regression. Instead of minimizing pure error term, the algorithm minimizes both the error and the regression coefficients at the same time. This leads to more stable predictions.

There are three variations of penalized regression and all of them can be accessed via \texttt{glmnet::glmnet()} (\(\beta\) is set of regression coefficients and \(\lambda\) is a parameter to be set by user or determined from cross-validation):

\begin{itemize}
\tightlist
\item
  Ridge regression (L2 penalty) minimizes \(SSE + \lambda \sum|\beta|^2\) and drives the coefficients to smaller values;
\item
  Lasso regression (L1 penalty) minimizes \(SSE + \lambda \sum|\beta|\) and forces some of the coefficients to vanish, which allows some variable selection
\item
  Elastic-net regression is a combination of the two previous variants \(SSE + \lambda_1 \sum|\beta| + \lambda_2 \sum|\beta|^2\).
\end{itemize}

Penalized regression can be also ran in the \texttt{\{tidymodels\}} workflow using \texttt{parsnip::linear\_reg()\ \%\textgreater{}\%\ parsnip::set\_engine("glmnet")}.

\begin{itemize}
\tightlist
\item
  \textbf{MARS}:
\end{itemize}

One limitation of all above-mentioned methods is that they assume linear relationship between the predictor and the target variables. Multivariate adaptive regression spline (MARS) addresses this \emph{issue} by modeling non-linear relationship with piece wise linear function. This gives a nice balance between simplicity and ability to fit complex data, for example \(\Lambda\)-shaped once where there is a maximal point from which function decreases in both directions. In R this model can be accessed via \texttt{earth::earth()} function.

\begin{itemize}
\tightlist
\item
  \textbf{PLS}:
\end{itemize}

In case of single and multiple target variables, partial least squares (PLS) regression can be applied. Similarly to PCA, PLS looks for components that maximizes the explained variance of the predictors, while simultaneously maximizing their correlation to the target variables. PLS can be applied with \texttt{lm()} by specifying multiple targets or in the \texttt{\{tidymodels\}} workflow with \texttt{plsmod::pls()\ \%\textgreater{}\%\ parsnip::set\_engine("mixOmics")}.

\hypertarget{other-common-supervised-ml-algorithms}{%
\subsection{Other common Supervised ML algorithms}\label{other-common-supervised-ml-algorithms}}

Additional Supervised ML techniques include:

\begin{itemize}
\tightlist
\item
  \textbf{K-nearest neighbors}
\end{itemize}

A very simple, yet useful and robust algorithm that works for both numeric and nominal target variables is K-nearest neighbors. The idea is that for every new observation to predict, the algorithms finds K closest points in the training set and use either their mean value (for numeric targets) or the most frequent value (for nominal targets) as prediction. This algorithm can be used with \texttt{kknn::kknn()} function or in the \texttt{\{tidymodels\}} workflow with \texttt{parsnip::nearest\_neighbor()\ \%\textgreater{}\%\ parsnip::set\_engine("kknn")}.

\begin{itemize}
\tightlist
\item
  \textbf{Decision trees}
\end{itemize}

Decision tree algorithms model the data by splitting the training set in smaller subsets in a way that each split is done by a predictor variable so that it maximizes the difference in target variable between the subsets. One important advantage of decision trees is that they can model complex relationships and interactions between predictors. To use decision tree in R, \texttt{rpart::rpart()} or in the \texttt{\{tidymodels\}} workflow \texttt{parsnip::decision\_tree()\ \%\textgreater{}\%\ parsnip::set\_engine("rpart")} can be used.

\begin{itemize}
\tightlist
\item
  \textbf{Black boxes}
\end{itemize}

The black boxes algorithm includes models for which the structure is too complex to directly interpret relationship between predictor variables and a value predicted by the model. The advantage of such models is their ability to model more complicated data than in case of interpretable models, but they have a greater risk of overfitting. Also, the lack of clear interpretation may not be acceptable in some business specific use cases. The later problem can be addressed by use of explanation algorithms that will be discussed in a later part of this chapter.

\begin{itemize}
\tightlist
\item
  \textbf{Random forests}
\end{itemize}

A random forest is a set of decision trees, each one trained on random subset of observations and/or predictors. The final prediction is then obtained by averaging the individual trees' predictions. By increasing the number of trees, we also increase the precision of the results. The random forest algorithm hence minimizes some of the limitations of a decision tree algorithm, by for instance reducing the risks of overfitting, and by increasing its precision.

\hypertarget{practical-guide-to-supervised-machine-learning}{%
\section{Practical Guide to Supervised Machine Learning}\label{practical-guide-to-supervised-machine-learning}}

Now that we have a general idea of the purpose of Supervised ML approach, let's build a simple machine learning model in the context of a sensory and consumer study. But before doing that, let's introduce the \texttt{\{tidymodels\}} framework

\hypertarget{introduction-to-the-tidymodels-framework}{%
\subsection{\texorpdfstring{Introduction to the \texttt{\{tidymodels\}} framework}{Introduction to the \{tidymodels\} framework}}\label{introduction-to-the-tidymodels-framework}}

R contains many fantastic systems for building machine learning models. For various reasons that will be explained here, we propose to use the \texttt{\{tidymodels\}} framework (\url{https://www.tidymodels.org/}) for our analysis.

Similarly to the \texttt{\{tidyverse\}}, \texttt{\{tidymodels\}} is a collection of packages dedicated to modeling. It contains packages such as \texttt{\{rsample\}} (general resampling infrastructure), \texttt{\{yardstick\}} (performance metrics), \texttt{\{recipes\}} (pre-processing and feature engineering steps for modeling), \texttt{\{workflows\}} (modeling workflow), \texttt{\{broom\}} (tidy statistical objects) and \texttt{\{parsnip\}} (fitting models) just to name a few. Yet, the similarity between \texttt{\{tidymdels\}} and \texttt{\{tidyverse\}} does not end there since \texttt{\{tidymodels\}} is built (and uses) on the \texttt{\{tidyverse\}}, hence being the perfect extension for modeling data.

Besides modeling data, \texttt{\{tidymodels\}} aims in \emph{tidying} the process of modeling data. Such process is done at different levels:

\begin{itemize}
\tightlist
\item
  Tidying the entire modeling workflow by integrating the different steps (including data preparation, model fitting, and data prediction) into simple functions (\texttt{\{parnsip\}}).
\item
  Tidying (by standardizing) the inputs and outputs for the different Machine Learning algorithms\footnote{To avoid re-inventing the wheel and to be more flexible, \texttt{\{tidymodels\}} allows calling ML algorithm from various packages in a standardized way, even when those packages often require the data to be structured in a different way, use different names for similar parameters, etc.}
\item
  Tidying the models so that the outputs can be easily extracted and used.
\item
  Providing all the relevant functions required for modelling in one unique collection of packages.
\end{itemize}

Regardless of the algorithm used, the typical modeling approach used by \texttt{\{tidymodels\}} is as following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split your data into training and test set (including sets for Cross-Validation)
\item
  Build a recipe by informing the model and any pre-processing step required on the data
\item
  Define the model (and its parameter) to consider
\item
  Create a workflow by combining the previous step together
\item
  Run your model
\item
  Evaluate your model
\item
  Predict new values
\end{enumerate}

For more information, we refer the readers to ``Tidy Modeling with R'' by Max Kuhn and Julia Silge (\url{https://www.tmwr.org/}){[}\url{https://www.tmwr.org/}{]}.

Let's load the \texttt{\{tidymodels\}} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sampling-the-data}{%
\subsection{Sampling the data}\label{sampling-the-data}}

As mentioned earlier, an important step consists in splitting the data into a training and testing set. To do so, the function \texttt{initial\_split()} is used. This function takes as input the original data and returns the information on how to make the different partitions. In practice, such partition could be obtain completely randomly by simply specifying the proportion of data in each partition (here \texttt{prop=0.7} meaning that 70\% of the data is in the training set, the rest being in the test set). However, we can provide constraints so that the structure of the original data is respected. In our case, \texttt{Type} contains 3 levels which may not be perfectly balanced. By specifying \texttt{strata=Type}, we ensure that the different splits respect the original data in terms of proportions for \texttt{Type}.

After the \texttt{initial\_split()}, the \texttt{training()} and \texttt{testing()} functions are used to obtain the training and testing subsets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wine\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{data=}\NormalTok{wine, }\AttributeTok{strata=}\StringTok{"Type"}\NormalTok{, }\AttributeTok{prop=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{wine\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(wine\_split)}
\NormalTok{wine\_testing }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(wine\_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation}{%
\subsection{Cross Validation}\label{cross-validation}}

Cross-validation (CV) is an important step for checking the model quality. To allow performing CV, additional sets of data are required. These sets of data can be obtained through resampling method before building the model. In practice, for each new set of data, a subset is used for building the model, the other subset being then used to measure the performance of such model (similar to the training and testing set defined earlier). However, in this case, the resampling is only performed on the training set defined earlier.

To generate such sets of data, the \texttt{vfold\_cv} function is used. Here we start with a 5-fold cross-validation first. Again, \texttt{strata=Type} to a conduct stratified sampling to ensure that each resample is created within the stratification variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wine\_cv }\OtherTok{\textless{}{-}}\NormalTok{ wine\_train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vfold\_cv}\NormalTok{(}\AttributeTok{v=}\DecValTok{5}\NormalTok{, }\AttributeTok{strata=}\NormalTok{Type)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-preprocessing-recipes}{%
\subsection{\texorpdfstring{Data Preprocessing \texttt{\{recipes\}}}{Data Preprocessing \{recipes\}}}\label{data-preprocessing-recipes}}

The \texttt{\{recipes\}} package contains a rich set of data manipulation tools which can be used to preprocess the data and to define roles for each variable (e.g.~outcome and predictor). To add a recipe, the function \texttt{recipe()} is used. This function has two arguments: a formula and the data (here \texttt{wine\_train}). Any variable on the left-hand side of the tilde (\texttt{\textasciitilde{}}) is considered the model outcome. In our example, we want to use a machine learning model to predict the type of the wine, therefore \texttt{Type} would be the target on the left hand side of the \texttt{\textasciitilde{}}. On the right-hand side of the tilde are the predictors. One can write out all the variables, but an easier option is to use the dot (\texttt{.}) to indicate all other variables as predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_recipe }\OtherTok{\textless{}{-}}\NormalTok{ wine\_train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{recipe}\NormalTok{(Type }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

In this instance, we do not need to preprocess further any of the variables present in the data. Yet if it was the case, we could use the various \texttt{step\_*()} functions, which then perform any transformation required on the declared variables including:

\begin{itemize}
\tightlist
\item
  \texttt{step\_log()} for a log transformation;
\item
  \texttt{step\_dummy()} to transform categorical data into dummy variables (useful to combine with functions such \texttt{all\_nominal\_predictors()}, \texttt{starts\_with()}, \texttt{matches()}, etc.);
\item
  \texttt{step\_interact()} creates interaction variables;
\item
  \texttt{step\_num2factor()} converts numeric variables to factor;
\item
  \texttt{step\_scale()} scales numeric variables;
\item
  \texttt{step\_pca()} converts numeric data into 1 or more principal components.
\item
  etc.
\end{itemize}

\begin{quote}
It should be noted that, by default, any of the pre-processing performed here on the trained data set is also applied adequately on the test set. For instance, with \texttt{step\_scale()}, the mean and standard deviation are computed on the training data, and are then applied on the test data (the means and standard deviation are not re-computed from the test set).
\end{quote}

\hypertarget{model-definition}{%
\subsection{Model definition}\label{model-definition}}

Once the model formula is defined, and the instructions for the data pre-processing is set, we need to decide which type of ML algorithm should be used. Let's consider the random forest classifier for the wine data using \texttt{rand\_forest()} (the algorithm proposed by the \texttt{\{ranger\}} package is used here). This function has 3 hyper-parameters (\texttt{mtry,\ trees,\ min\_n}) which can be tuned to achieve the best possible results.

Model tuning is the process of finding the optimal values for those parameters. In order to find the best hyper-parameter combinations, we need to define a search range for each of them. When we choose the family of the model we want to use (\texttt{rand\_forest} in this example), we have to let the machine knows that a given parameter (\texttt{mtry,\ trees,\ min\_n}) is not defined explicitly and will be tuned instead. To achieve such a result, we must use the function \texttt{tune()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_spec }\OtherTok{\textless{}{-}} \FunctionTok{rand\_forest}\NormalTok{(}
  \AttributeTok{mtry =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{trees =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{min\_n =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\AttributeTok{engine =} \StringTok{"ranger"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{set-the-whole-process-into-a-workflow}{%
\subsection{Set the whole Process into a workflow}\label{set-the-whole-process-into-a-workflow}}

Finally, we combine the model and the recipe into a single \texttt{workflow()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(model\_recipe) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_model}\NormalTok{(rf\_spec)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning-the-parameters}{%
\subsection{Tuning the parameters}\label{tuning-the-parameters}}

In the previous section, placeholders for tuning hyper-parameters were created. It is time to define the scope of the search, and to choose the method for searching the parameter space. To do so, \texttt{grid\_regular()} can be used:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params\_grid }\OtherTok{\textless{}{-}}\NormalTok{ rf\_spec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{parameters}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{update}\NormalTok{(}\AttributeTok{mtry =} \FunctionTok{mtry}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)),}
         \AttributeTok{trees =} \FunctionTok{trees}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{200}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{grid\_regular}\NormalTok{(}\AttributeTok{levels=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now that the hyper-parameter search range is defined, let's look for the best combination using the \texttt{tune\_grid()} function. The cross-validation set is used for this purpose, so that the data used for training the model has not been used yet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tuning }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(rf\_wf, }\AttributeTok{resamples=}\NormalTok{wine\_cv, }\AttributeTok{grid=}\NormalTok{params\_grid)}
\end{Highlighting}
\end{Shaded}

The \texttt{autoplot()} function is called to take a quick look at the \texttt{tuning} object.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(tuning)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-304-1.pdf}

Ultimately, the best combination of parameters is obtained using the \texttt{select\_best()} function. Such paramaters is defined based on the quality of the model, which can be estimated through a various metrics. Here we decided to use \texttt{roc\_auc} (Area Under the Receiver Operating Characteristic Curve), as it provides a reliable estimate of the quality of the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params\_best }\OtherTok{\textless{}{-}} \FunctionTok{select\_best}\NormalTok{(tuning, }\StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-training}{%
\subsection{Model training}\label{model-training}}

The best parameters can be applied to our model, and the final model can be trained using the entire training set. This is done using the \texttt{fit()} function that we apply to our workflow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_model }\OtherTok{\textless{}{-}}\NormalTok{ rf\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{finalize\_workflow}\NormalTok{(params\_best) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(wine\_train)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-evaluation}{%
\subsection{Model evaluation}\label{model-evaluation}}

A very important part in building machine learning models is to assess the quality of the model. A first approach consists in applying the model thus obtained on the testing data set (\texttt{wine\_testing}), which the model has not seen yet.

To do so, the \texttt{predict()} function is used. The \texttt{predict()} function of \texttt{\{tidymodels\}} allows adding in an easy way the predictions obtained from models to the original data. This procedure allows comparing the predictions with the actual data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_vs\_pred }\OtherTok{\textless{}{-}}\NormalTok{ wine\_testing }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(}\FunctionTok{predict}\NormalTok{(final\_model, .))}
\end{Highlighting}
\end{Shaded}

Here, \texttt{obs\_vs\_pred} is a data frame which contains both the actual wine type (\texttt{Type}) and the predicted wine type (\texttt{.pred\_class}). Comparing these two variables allow judging the quality of the model. Such comparison can be done through a confusion matrix. A confusion matrix is a table where each row represents instances in the actual class, while each column represents the instances in a predicted class. From the \texttt{autoplot()} function, it appears that the predictions were almost perfect (only two wines was wrongly classified).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm }\OtherTok{\textless{}{-}} \FunctionTok{conf\_mat}\NormalTok{(obs\_vs\_pred, Type, .pred\_class)}
\FunctionTok{autoplot}\NormalTok{(cm, }\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/cm-1.pdf}

There are several ways to look into the model quality, being the approach and model metrics highly dependent on the situation/type of model used. In our case, that we have a multiclass classification, we can besides the confusion matrix, check the model accuracy. Classification accuracy is a metric that summarized the fraction of predictions our model got right (somehow it brings similar information we get from the confusion matrix). We have to first organize the data to have have the model predictions. Then we directly calculate the accuracy and also kappa accuracy, using the functions \texttt{accuracy\ ()} and \texttt{kap\ ()}. Kappa (kap()) is a similar measure to accuracy(), but is normalized by the accuracy that would be expected by chance alone. It can be very useful when one or more classes have large frequency distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_vs\_pred\_prob }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(wine\_testing }\SpecialCharTok{\%\textgreater{}\%} 
                                \FunctionTok{select}\NormalTok{(Type), }
                              \FunctionTok{predict}\NormalTok{(final\_model, }
\NormalTok{                                      wine\_testing, }
                                      \AttributeTok{type =} \StringTok{"prob"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Type =} \FunctionTok{as.factor}\NormalTok{(Type))}

\FunctionTok{accuracy}\NormalTok{(obs\_vs\_pred, }\AttributeTok{truth =} \StringTok{"Type"}\NormalTok{, }\AttributeTok{estimate =} \StringTok{".pred\_class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   <chr>    <chr>          <dbl>
## 1 accuracy multiclass     0.964
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kap}\NormalTok{(obs\_vs\_pred, }\AttributeTok{truth =} \StringTok{"Type"}\NormalTok{, }\AttributeTok{estimate =} \StringTok{".pred\_class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 kap     multiclass     0.945
\end{verbatim}

The accuracy and Kappa accuracy of our model is extremely high (\textgreater{} 0.94), emphasizing its great model performance.

A simple and easy way to have a higher level understanding on what variables play the most important role in our model (wine classification) is through the called Feature Importance plot. To create this plot, we need to first create an object known as \texttt{explainer} from the tidymodels workflow, using the function \texttt{explain\_tidymodels} from the package \texttt{\{DALEXtra\}}. This function will take as argument: the model to be explained, the data to be used to calculate the explanations which should be passed without a target column (\texttt{wine\_train} removing the columns \texttt{Type}) and the numeric vector with outputs/scores (y). The \texttt{explainer} object can be then used to create the Feature Importance plot using the function \texttt{variable\_importance}.

The interpretation is very straightforward, in the way that variables are conveniently ordered according to their importance. The higher the cross entropy loss after permutations, the more important the variable is to in our case, decide to which group each wine belong to. So, for example, if color is permuted (spoil the variable), it turns out that the model will be more than 2.5 times worse than the one with the correct color variable. In summary, cross entropy loss after permutations measures how much the permutation of a variable would impact the model performance. The higher the impact, the most important the variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DALEXtra)}
\FunctionTok{library}\NormalTok{(modelStudio)}

\NormalTok{data\_to\_explain }\OtherTok{\textless{}{-}}\NormalTok{ wine\_train}

\NormalTok{explainer }\OtherTok{\textless{}{-}} \FunctionTok{explain\_tidymodels}\NormalTok{(}\AttributeTok{model =}\NormalTok{ final\_model, }
                                \AttributeTok{data =}\NormalTok{ data\_to\_explain }\SpecialCharTok{\%\textgreater{}\%} 
                                  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Type),}
                                \AttributeTok{y =}\NormalTok{ data\_to\_explain}\SpecialCharTok{$}\NormalTok{Type)}

\NormalTok{var\_imp }\OtherTok{\textless{}{-}} \FunctionTok{variable\_importance}\NormalTok{(explainer, }
                               \AttributeTok{loss\_function =}\NormalTok{ loss\_cross\_entropy, }
                               \AttributeTok{type =} \StringTok{"ratio"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(var\_imp)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/fi-1.pdf}

Building a machine learning model may seem complicated at first since there are many steps and important things to watch out for. Although we presented here a simple example, for one specific situation of classification, you will realize that this way of working (typical modeling approach showed), is highly applicable to other situations. Again, the main idea of this chapter is to open your mind, provide you with the basics, and ultimately motivate you to seek and learn more about machine learning!

\hypertarget{text-analysis}{%
\chapter{Text Analysis}\label{text-analysis}}

\begin{quote}
In the previous chapters, most transformations and analyses were performed on \emph{simple} data, i.e.~data that represent something very specific, understandable, predictable, and stand-alone. For numerical variables (e.g.~sensory attributes), one data entry is simply a number often defined within a range. For categorical variables or factors, each data entry is a pre-defined entry (e.g.~product names, or a category for a given variable) chosen from a list of possible options. But there are situations where the data is intrinsically more \emph{complex} and less structured.
A good illustration of such \emph{complex} situation is text analysis. Before collecting the data, we do not know explicitely what kind of information we will get (with open-ended questions, respondents are free to say/write whatever they want!). In that case, each data entry (from words, to sentences, to paragraphs\ldots) is more \emph{messy} as it may contain relevant and less informative elements. The goal of the analysis is then to extract the relevant information from the data and to summarize it automatically. In this section, we will show you how such data can be processed and how infromation can be extracted.
\end{quote}

\hypertarget{introduction-to-natural-language-processing}{%
\section{Introduction to Natural Language Processing}\label{introduction-to-natural-language-processing}}

Humans exchange information through the use of languages. There is of course a very large number of different languages, each of them having their own specificity. The science that studies languages per se is called \emph{linguistics}: It focuses on areas such as phonetics, phonology, morphology, syntax, semantics, and pragmatics.

Natural Language Processing (NLP) is a sub-field of linguistics, computer science, and artificial intelligence. It connects computers to human language by processing, analyzing, and modeling large amounts of natural language data. One of the main goals of NLP is to \emph{understand} the contents of documents, and to extract accurately information and insights from those documents. In Sensory and Consumer Research, we often refer to NLP when we talk about \emph{Text Analysis} .

Since the fields of linguistics and NLP are widely studied, a lot of documentations is already available online. The objective of this chapter is to provide sufficient information for you to be familiar with textual data, and to give you the keys to run the most useful analyses in Sensory and Consumer Research.

For those who would like to dive deeper into NLP, we recommend reading (\citet{Silge2017}, \citet{Becue-Bertaut2019}), and (\citet{Hvitfeldt2021}) for more advanced techniques.

\hypertarget{application-of-text-analysis-in-sensory-and-consumer-science}{%
\section{Application of Text Analysis in Sensory and Consumer Science}\label{application-of-text-analysis-in-sensory-and-consumer-science}}

\hypertarget{text-analysis-as-way-to-describe-products}{%
\subsection{Text analysis as way to describe products}\label{text-analysis-as-way-to-describe-products}}

In recent years, open-ended comments have gained interest as it is the fastest, safest, most unbiased way to collect spontaneous data from participants (\citet{Piqueras2015}).

Traditionally, most SCS questionnaires relied primarily on closed questions, to which open-ended questions were added to uncover the consumers' reasons for liking or disliking products. In practice, these open-ended questions were positioned right after liking questions, and aimed at providing some understanding about why a product may or may not be liked, and to give the participants a chance to reduce their frustration by explaining their responses to certain questions. As a result of such practices, these questions were usually not deeply analyzed.

With the development of the so-called \emph{rapid} and consumer-oriented descriptive methods, the benefits of open-ended questions became more apparent as they provide a new way to uncover sensory perception. In practice, respondents are asked to give any terms that describe their sensory perception in addition to their quantitative evaluation of the products by the means of intensity rating or ranking (e.g.~Free Choice Profile, Flash Profile), or similarities and dissimilarities assessment (e.g.~Free Sorting Task, and Ultra Flash Profile as an extension of Napping). Since the textual responses are now an integral part of the method, its analysis can no longer be ignored.

The importance of open-ended questions increased further as it has been shown that respondents can reliably describe in their own words their full experience (perception, emotion, or any other sort of association) with products. Recently, Mahieu et al.~{[}REF REF REF{]} showed the benefits of using open-ended questions over CATA\footnote{CATA can be seen as a simplified version of open-comments in the sense that respondents also associate products to words, however they lose the freedom of using their own as they need to select them from a pre-defined list.}. In this study, consumers were asked to describe with their own words both the products they evaluated and what their ideal product would be like. Similarly, Luc et al.~{[}REF REF REF{]} proposed an alternative to Just About Right (JAR) scale method - called free-JAR - and in which consumers describe the samples using their own words, by still following a JAR terminology (too little, too much, or JAR, etc.).

The inclusion of open-ended questions as one of the primary elements of sensory and consumer tasks blurs the line with other fields, including psychology and sociology where these qualitative methods originated. More recently, advances in the technology (web-scraping, social listening, etc.) opened new doors that brought SCS closer to other fields such as marketing for instance. The amount of data that are collected with such techniques can be considerably larger, but the aim of the analysis stays the same: extracting information from text/comments.

\hypertarget{objectives-of-text-analysis}{%
\subsection{Objectives of Text Analysis}\label{objectives-of-text-analysis}}

Open-ended comments, and more generally textual responses in questionnaires, are by definition qualitative. This means that the primary analysis should be qualitative. It could simply consist in reading all these comments and eventually summarizing the information gathered. But as the number of comments increases, such an approach quickly becomes too time and energy consuming for the analysts. How can we transform such qualitative data into quantitative measures? How can we digest and summarize the information contained in these comments without losing the overall meaning of the messages (context)?

One easy solution is to simply count how often a certain word is being used in a given context (e.g.~how often the word \texttt{sweet} is being associated to each product evaluated). However, if such a solution is a reasonable one to start with, we will show some alternatives that allow going deeper into the understanding of textual inputs. This is the objective of the textual analysis and NLP that we are going to tackle in the next sections.

\hypertarget{classical-text-analysis-workflow}{%
\subsection{\texorpdfstring{Classical \emph{text analysis} workflow}{Classical text analysis workflow}}\label{classical-text-analysis-workflow}}

In SCS, the generic notion of \emph{text analysis} often includes any step or procedure that allows going from the raw data (e.g.~consumer comments, text scrapped from website or social media, etc.) to results and insights. However, such process requires many separate steps, often defined as following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Tokenization} is the step that splits the raw data into statistical units of interest, also called token\footnote{A token can be a single word, a group of \emph{n}-words (also know as \emph{n-grams}), a sentence, or an entire document.}.
\item
  Non-informal words or \textbf{stopwords} (e.g.~\emph{and}, \emph{I}, \emph{you}, etc.) are then removed from the data to facilitate the extraction of the information.
\item
  \textbf{Stemming} consists in reducing words to their root form, hence grouping the different variants of the same word (e.g.~singular/plural, infinitive or conjugated verbs, etc.)
\item
  An extra (optional) step called \textbf{lemmatization} consists in grouping words that have similar meanings under one umbrella. The advantage of such procedure is that it simplifies further the analysis and its interpretation. However, it can be time consuming and more importantly, it relies on the analyst own judgement: two different analysts performing the same task on the same data will obtain different end results.
\item
  The final data is then \textbf{analyzed} and summarized (often through counts) to extract information or patterns.
\end{enumerate}

\hypertarget{warnings}{%
\subsection{Warnings}\label{warnings}}

Languages are complex, as many aspects can influence the meaning of a message. For instance, in spoken languages, the intonation is as important as the message itself. In written languages, non-word items (e.g.~punctuation, emojis) may also completely change the meaning of a sentence (e.g.irony). Worst, some words have different meanings depending on their use (e.g.~\emph{like}), and the context of the message provides its meaning. Unfortunately, the full \emph{context} is only available when analyzed manually (e.g.~when the analyst reads all the comments), meaning that automating analyses do not always allow capturing it properly. In practice however, reading all the comments is not a realistic solution. This is why we suggest to automate the analysis to extract as much information as possible, before going back to the raw text to ensure that the conclusions drawn match the data.

\hypertarget{illustration-using-sorting-task-data}{%
\section{Illustration using Sorting Task Data}\label{illustration-using-sorting-task-data}}

Let's start with loading the usual packages of need:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

The data set used for illustration was kindly shared by Dr.~Jacob Lahne. It is part of a study that aimed in developing a CATA lexicon for Virginia Hard (Alcoholic) Ciders (REF REF REF.). The data can be found in \emph{cider\_text\_data.xlsx}.

Let's also import the data to our R session:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"cider\_text\_data.xlsx"}\NormalTok{) }
\NormalTok{cider\_og }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(file\_path) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sample =} \FunctionTok{as.character}\NormalTok{(sample))}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-pre-processing}{%
\subsection{Data Pre-processing}\label{data-pre-processing}}

Before starting, it is important to mention that there is a large variety of R-based solutions and R packages that handle textual data, including:

\begin{itemize}
\tightlist
\item
  The IRaMuTeQ project (REF REF Reinert 1983) is a free software dedicated to text analysis and developed in R and Python. It includes Reinert textual clustering method (for more information, see \url{http://www.iramuteq.org/})
\item
  \texttt{\{tm\}} package for text mining
\item
  \texttt{\{tokenizers\}} to transform strings into tokens
\item
  \texttt{\{SnowballC\}} for text stemming
\item
  \texttt{\{SpacyR\}} for Natural Language Processing
\item
  \texttt{\{Xplortext\}} for deep understanding and analysis of textual data.
\end{itemize}

However, to ensure a continuity with the rest of the book, we will emphasize the use of the \texttt{\{stringr\}} package for handling strings (here text) combined with the \texttt{\{tidytext\}} package. Note that \texttt{\{stringr\}} is part of the \texttt{\{tidyverse\}} and both packages fit very well within the \texttt{\{tidyverse\}} philosophy.

Let's load this additional package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction-to-working-with-strings-stringr}{%
\subsection{\texorpdfstring{Introduction to working with strings (\texttt{\{stringr\}})}{Introduction to working with strings (\{stringr\})}}\label{introduction-to-working-with-strings-stringr}}

The \texttt{\{stringr\}} package brings a large set of tools that allow working with strings. Most functions included in \texttt{\{stringr\}} start with \texttt{str\_*()}. Here are some of the most convenient functions:

\begin{itemize}
\tightlist
\item
  \texttt{str\_length()} to get the length of the string;
\item
  \texttt{str\_c()} to combine multiple strings into one;
\item
  \texttt{str\_detect()} to search for a pattern in a string, and \texttt{str\_which()} find the position of a pattern within the string;
\item
  \texttt{str\_extract()} and \texttt{str\_extract\_all()} to extract the first (or all) matching pattern from a string;
\item
  \texttt{str\_remove()} and \texttt{str\_remove\_all()} to remove the first (or all) matching pattern from a string;
\item
  \texttt{str\_replace()}, \texttt{str\_replace\_all()}, to replace the first (or all) matching pattern with another one.
\end{itemize}

It also includes \emph{formatting} options that can be applied to strings, including:

\begin{itemize}
\tightlist
\item
  \texttt{str\_to\_upper()} and \texttt{str\_to\_lower()} to convert strings to uppercase or lowercase;
\item
  \texttt{str\_trim()} and \texttt{str\_squish()} to remove white spaces;
\item
  \texttt{str\_order} to order the element of a character vector.
\end{itemize}

Examples of application of some of these functions is shown in the next sections.

\hypertarget{tokenization}{%
\subsection{Tokenization}\label{tokenization}}

The analysis of textual data starts with defining the statistical unit of interest, also known as \emph{token}. This can either be a single word, a group of words, a sentence, a paragraph, a whole document etc. The procedure to transform the document into tokens is called \emph{tokenization}.

By looking at our data (\texttt{cider\_og}), we can notice that for each sample evaluated, respondents are providing a set of responses, ranging from a single word (e.g.~\texttt{yeasty}) to a group of words (\texttt{like\ it\ will\ taste\ dry\ and\ acidic}). Fortunately, the data is also well structured since the responses seem to be separated by a \texttt{;} or \texttt{,}.

Let's transform this text into tokens using \texttt{unnest\_tokens()} from the \texttt{\{tidytext\}} package. The function \texttt{unnest\_tokens()} proposes different options for the tokenization including by \emph{words}, \emph{ngrams}, or \emph{sentences} for instance. However, let's take advantage of the data structure and use a specific character to separate the tokens (here \texttt{;}, \texttt{,} etc.). The \texttt{regex} parameter allows us to specify the patterns to consider:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider\_og }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(tokens, comments, }\AttributeTok{token=}\StringTok{"regex"}\NormalTok{, }
                \AttributeTok{pattern=}\StringTok{"[;|,|:|.|/]"}\NormalTok{, }\AttributeTok{to\_lower=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The original comments from consumers are now split into tokens, increasing the size of the file from 168 individual comments to 947 rows of tokens.

This procedure already provides some interesting information as we could easily count word usage and answer questions such as ``how often the word \emph{apple} is used to describe each samples?'' for instance. However, a deeper look at the data shows some inconsistencies since some words starts with a space, or have capital letters (remember that R is case-sensitive!). Further pre-processing is thus needed.

\hypertarget{simple-transformations}{%
\subsection{Simple Transformations}\label{simple-transformations}}

To further prepare the data, let's standardize the text by removing all the white spaces (\emph{irrelevant} spaces in the text, e.g.~at the start/end, double spaces, etc.), transforming everything to lower case (note that this could have been done earlier through the parameter \texttt{to\_lower=TRUE} from \texttt{unnest\_tokens()}), removing some special letters, replacing some misplaced characters etc.\footnote{This process is done in iterations: the more you clean your document, the more you find some small things to fix\ldots until you're set!}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_to\_lower}\NormalTok{(tokens)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_trim}\NormalTok{(tokens)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_squish}\NormalTok{(tokens)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_remove\_all}\NormalTok{(tokens, }\AttributeTok{pattern=}\StringTok{"[(|)|?|!]"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_remove\_all}\NormalTok{(tokens, }\AttributeTok{pattern=}\StringTok{"[ó|ò]"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tokens =} \FunctionTok{str\_replace\_all}\NormalTok{(tokens, }\AttributeTok{pattern=}\StringTok{"õ"}\NormalTok{, }\AttributeTok{replacement=}\StringTok{"\textquotesingle{}"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To ensure that the cleaning job is done (for now), let's produce the list of tokens generated here (and its corresponding frequency)\footnote{Although not present in the text, we will use the next 3 lines of code multiple times to count the number of words present in the data.}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(tokens) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 476 x 2
##   tokens     n
##   <chr>  <int>
## 1 sweet     55
## 2 fruity    33
## 3 sour      32
## 4 tart      28
## # ... with 472 more rows
\end{verbatim}

The most used words to describe the ciders are \texttt{sweet} (55 occurrences), \texttt{fruity} (33 occurrences), and \texttt{sour} (32 occurrences).

A closer look at this list highlights a few things that still need to get tackled:

\begin{itemize}
\tightlist
\item
  The same concept can be described in different ways: \texttt{spicy}, \texttt{spices}, and \texttt{spiced} may all refer to the same concept, yet they are written differently and hence are considered as different tokens. This will be handled in a later stage.
\item
  Multiple concepts are still joined (and hence considered separately: \texttt{sour\ and\ sweet} is currently neither associated to \texttt{sour}, nor to \texttt{sweet}, and we may want to disentangle them.
\item
  There could be some typos: Is \texttt{sweat} a typo and should read \texttt{sweet}? Or did that respondent really perceived the cider as \texttt{sweat}?
\item
  Although most tokens are made of one (or few) words, some others are defined as a whole sentence (e.g.~\texttt{this\ has\ a\ very\ lovely\ floral\ and\ fruity\ smell}).
\end{itemize}

Let's handle each of these different points\ldots{}

\hypertarget{splitting-further-the-tokens}{%
\subsection{Splitting further the tokens}\label{splitting-further-the-tokens}}

For an even deeper cleaning, let's go one step further and split the remaining tokens into single words by using the space as separator. Then, we can number each token for each assessor using \texttt{row\_number()} to ensure that we can still recover which words belong to the same token, as defined previously. This information will be specially relevant later when looking at \emph{bigrams}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{relocate}\NormalTok{(subject, }\AttributeTok{.before=}\NormalTok{sample) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(subject, sample) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{num =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(tokens, tokens, }\AttributeTok{token=}\StringTok{"regex"}\NormalTok{, }\AttributeTok{pattern=}\StringTok{" |{-}"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(cider)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   subject sample rating tokens      num
##   <chr>   <chr>   <dbl> <chr>     <int>
## 1 J1      182         8 hard          1
## 2 J1      182         8 cider         1
## 3 J1      182         8 smell         1
## 4 J1      182         8 fermented     2
## # ... with 2 more rows
\end{verbatim}

For \texttt{J1} and \texttt{182} for instance, the first token is now separated into three words: \texttt{hard}, \texttt{cider}, and \texttt{smell}.

A quick count of words show that \texttt{sweet} appears now 96 times, and \texttt{apple} 82 times. Interestingly, terms such as \texttt{a}, \texttt{like}, \texttt{the}, \texttt{of}, \texttt{and} etc. also appear fairly frequently.

\hypertarget{stopwords}{%
\subsection{Stopwords}\label{stopwords}}

\emph{Stop words} refer to \emph{common} words that do not carry much (if at all) information. In general, stop words include words (in English) such as \emph{I}, \emph{you}, \emph{or}, \emph{of}, \emph{and}, \emph{is}, \emph{has}, etc. It is thus common practice to remove such stop words before any analysis as they would \emph{pollute} the results with unnecessary information.

Building lists of stop words can be tedious. Fortunately, it is possible to find some pre-defined lists, and to eventually adjust them to our own needs by adding and/or removing words. In particular, the package \texttt{\{stopwords\}} contains a comprehensive collection of stop word lists:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stopwords)}
\FunctionTok{length}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source=}\StringTok{"snowball"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 175
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source=}\StringTok{"stopwords{-}iso"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1298
\end{verbatim}

The English Snowball list contains 175 words, whereas the English list from the Stopwords ISO collection contains 1298 words.

A deeper look at these lists (and particularly to the \emph{Stopwords ISO} list) shows that certain words including \emph{like}, \emph{not} and \emph{don't} (just to name a few) are considered as stop words. If we would use this list blindly, we would remove these words from our comments. Although using such list on our current example would have a limited impact on the analysis (most comments are just few descriptive words), it would have a more critical impact on other studies in which consumers give their opinion on samples. Indeed, the analysis of the two following comments \emph{I like Sample A} and \emph{I don't like Sample B} would be lost although they provide some relevant information.

It is therefore important to remember that although a lot of stop words are relevant in all cases, some of them are topic specific and should (or should not) be used in certain contexts. Hence, inspecting and adapting these lists before use is strongly recommended.

Since we have a relatively small text size, let's use the \emph{SnowBall Stopword} list as a start, and look at the terms that our list and this stopword list share:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_list }\OtherTok{\textless{}{-}} \FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source=}\StringTok{"snowball"}\NormalTok{)}
\NormalTok{word\_list }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(tokens) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(tokens)}

\FunctionTok{intersect}\NormalTok{(stopword\_list, word\_list)}
\end{Highlighting}
\end{Shaded}

As we can see, some words such as \emph{off}, \emph{not}, \emph{no}, \emph{too}, and \emph{very} would automatically be removed. However, such qualifiers are useful in the interpretation of sensory perception, so we would prefer to keep them. We can thus remove them from \texttt{stopword\_list}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_list }\OtherTok{\textless{}{-}}\NormalTok{ stopword\_list[}\SpecialCharTok{!}\NormalTok{stopword\_list }\SpecialCharTok{\%in\%} 
                                 \FunctionTok{c}\NormalTok{(}\StringTok{"off"}\NormalTok{,}\StringTok{"no"}\NormalTok{,}\StringTok{"not"}\NormalTok{,}\StringTok{"too"}\NormalTok{,}\StringTok{"very"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

Conversely, we can look at the words from our data that we would not consider relevant and add them to the list. To do so, let's look at the list of words in our data that is not present in \texttt{stopword\_list}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word\_list[}\SpecialCharTok{!}\NormalTok{word\_list }\SpecialCharTok{\%in\%}\NormalTok{ stopword\_list]}
\end{Highlighting}
\end{Shaded}

Words such as \emph{like}, \emph{sample}, \emph{just}, \emph{think}, or \emph{though} do not seem to bring any relevant information here. Hence, let's add them (together with others) to our customized list of stop words\footnote{As an exercise, you could go deeper into the list and decide by yourself whether you would want to remove more words.}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_list }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stopword\_list, }
                   \FunctionTok{c}\NormalTok{(}\StringTok{"accompany"}\NormalTok{,}\StringTok{"amount"}\NormalTok{,}\StringTok{"anything"}\NormalTok{,}\StringTok{"considering"}\NormalTok{,}
                     \StringTok{"despite"}\NormalTok{,}\StringTok{"expected"}\NormalTok{,}\StringTok{"just"}\NormalTok{,}\StringTok{"like"}\NormalTok{,}\StringTok{"neither"}\NormalTok{,}
                     \StringTok{"one"}\NormalTok{,}\StringTok{"order"}\NormalTok{,}\StringTok{"others"}\NormalTok{,}\StringTok{"products"}\NormalTok{,}\StringTok{"sample"}\NormalTok{,}
                     \StringTok{"seems"}\NormalTok{,}\StringTok{"something"}\NormalTok{,}\StringTok{"thank"}\NormalTok{,}\StringTok{"think"}\NormalTok{,}\StringTok{"though"}\NormalTok{,}
                     \StringTok{"time"}\NormalTok{,}\StringTok{"way"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

A final look at the list of stop words (here ordered alphabetically) ensures that it fits our need:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_list[}\FunctionTok{order}\NormalTok{(stopword\_list)]}
\end{Highlighting}
\end{Shaded}

Finally, the data is being cleaned by removing all the words stored in \texttt{stopword\_list}. This can easily be done either using \texttt{filter()} (we keep tokens that are not contained in \texttt{stopword\_list}), or by using \texttt{anti\_join()}\footnote{Note that if we were using the original list of stopwords, \texttt{anti\_join()} can directly be associated to \texttt{get\_stopwords(source="snowball")}.}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{tokens =}\NormalTok{ stopword\_list), }\AttributeTok{by=}\StringTok{"tokens"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{stemming-and-lemmatization}{%
\subsection{Stemming and Lemmatization}\label{stemming-and-lemmatization}}

After removing the stop words, the data contains a total of 328 different words. However a closer look at this list shows that it is still not optimal, as for instance \texttt{apple} (82 occurrences) and \texttt{apples} (24 occurrences) are considered as two separate words although they refer to the same \emph{concept}.

To further \emph{clean} the data, two similar approaches can be considered: \textbf{stemming} and \textbf{lemmatization}.

The procedure of stemming consists in performing a step-by-step algorithm that reduces each word to its base word (or \emph{stem}). The most used algorithm is the one introduced by REF (Porter, 1980) which is available in the \texttt{\{SnowballC\}} package through the \texttt{wordStem()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(SnowballC)}

\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(tokens))}
\end{Highlighting}
\end{Shaded}

The stemming reduced further the list to 303 words. Now, \texttt{apple} and \texttt{apples} have been combined into \texttt{appl} (106 occurrences). However, due to the way the algorithm works, the final tokens are no longer English\footnote{Different algorithms for different languages exist, so we are not limited to stemming English words.} words.

Alternatively, we can \emph{lemmatize} words. Lemmatization is similar to stemming except that it does not cut words to their stems: Instead it uses knowledge about the language's structure to reduce words down to their dictionary form (also called \emph{lemma}). This approach is implemented in the \texttt{\{spacyr\}} package\footnote{spaCy is a library written in Python: for the \texttt{\{spacyr\}} package to work, you'll need to go through a series of steps that are described here: (\url{https://cran.r-project.org/web/packages/spacyr/readme/README.html}){[}\url{https://cran.r-project.org/web/packages/spacyr/readme/README.html}{]}} and the \texttt{spacy\_parse()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spacyr)}

\FunctionTok{spacy\_initialize}\NormalTok{(}\AttributeTok{entity=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{lemma }\OtherTok{\textless{}{-}} \FunctionTok{spacy\_parse}\NormalTok{(cider}\SpecialCharTok{$}\NormalTok{tokens) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{tokens=}\NormalTok{token, lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unique}\NormalTok{()}

\NormalTok{cider }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(cider, lemma, }\AttributeTok{by=}\StringTok{"tokens"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As can be seen, as opposed to stems, lemmas consist in \emph{regular} words. Here, the grouping provides similar number of terms (approx 300) in both cases:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(stem)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 301 x 2
##   stem          n
##   <chr>     <int>
## 1 acid         23
## 2 acrid         1
## 3 aftertast    12
## 4 alcohol      13
## # ... with 297 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(lemma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 303 x 2
##   lemma       n
##   <chr>   <int>
## 1 acid        3
## 2 acidic     18
## 3 acidity     2
## 4 acrid       1
## # ... with 299 more rows
\end{verbatim}

In the case of lemmatization, \texttt{acid}, \texttt{acidity}, and \texttt{acidic} are still considered as separate words whereas they are all grouped under \texttt{acid} with the stemming procedure. This particular example shows the advantage and disadvantage of each method, as it may (or may not) group words that are (or are not) meant to be grouped. Hence, the use of lemmatization/stemming procedures should be thought carefully. Depending on their objective, researchers may be interested in the different meanings conveyed by such words as \texttt{acid}, \texttt{acidity}, and \texttt{acidic} and decide to keep them separated, or decide to group them for a more holistic view of the main sensory attributes that could be derived from this text.

It should also be said that neither the lemmatization nor the stemming procedure will combine words that are different but bear similar meanings. For instance, the words \texttt{moldy} and \texttt{rotten} have been used, and some researchers may decide to group them if they consider them equivalent. This type of grouping should be done manually on a case-by-case using \texttt{str\_replace()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(lemma }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"moldy"}\NormalTok{,}\StringTok{"rotten"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   lemma      n
##   <chr>  <int>
## 1 moldy      2
## 2 rotten     5
\end{verbatim}

As can be seen here, originally, \texttt{moldy} was stated twice whereas \texttt{rotten} was stated 5 times. After re-placing \texttt{moldy} by \texttt{rotten}, the newer version contains 7 occurrences of \texttt{rotten} and none of \texttt{modly}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma =} \FunctionTok{str\_replace}\NormalTok{(lemma, }\StringTok{"moldy"}\NormalTok{, }\StringTok{"rotten"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(lemma }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"moldy"}\NormalTok{,}\StringTok{"rotten"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   lemma      n
##   <chr>  <int>
## 1 rotten     7
\end{verbatim}

Doing such transformation can quickly be tedious to do directly in R. As an alternative solution, we propose to export the list of words in Excel, create a new column with the new grouping names, and merge the newly acquired names to the previous file. This is the approach we used to create the file entitled \emph{Example of word grouping.xlsx}. In this example, one can notice that we limited the grouping to a strict minimum for most words except \texttt{bubble} that we also combined to \texttt{bubbly}, \texttt{carbonate}, \texttt{champagne}, \texttt{moscato}, \texttt{fizzy}, and \texttt{sparkle}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_list }\OtherTok{\textless{}{-}} \FunctionTok{read\_xlsx}\NormalTok{(}\StringTok{"data/Example of word grouping.xlsx"}\NormalTok{)}
\NormalTok{cider }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{full\_join}\NormalTok{(new\_list, }\AttributeTok{by=}\StringTok{"lemma"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{new name}\StringTok{\textasciigrave{}}\NormalTok{), lemma, }\StringTok{\textasciigrave{}}\AttributeTok{new name}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\StringTok{\textasciigrave{}}\AttributeTok{new name}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This last \emph{cleaning} approach reduces further the number of words to 278.

\hypertarget{text-analysis-1}{%
\section{Text Analysis}\label{text-analysis-1}}

Now that the text has been sufficiently cleaned, some analyses can be run to compare the samples in the way they have been described by the respondents. To do so, let's start with simple analyses.

\hypertarget{raw-frequencies-and-visualization}{%
\subsection{Raw Frequencies and Visualization}\label{raw-frequencies-and-visualization}}

In the previous sections, we have already shown how to count the number of occurrences of each word. We can reproduce this and show the top 10 most used words to describe our ciders:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n}\SpecialCharTok{\textgreater{}=}\DecValTok{10}\NormalTok{, }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lemma)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(lemma, n), }\AttributeTok{y=}\NormalTok{n))}\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{colour=}\StringTok{"grey80"}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"List of words mentioned at least 10 times"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-329-1.pdf}

As seen previously, the most mentioned words are \texttt{apple}, \texttt{sweet}, \texttt{fruity}, and \texttt{sour}.

Let's now assess the number of time each word has been used to characterize each product.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lemma), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(sample)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sample, lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{lemma, }\AttributeTok{values\_from=}\NormalTok{n, }\AttributeTok{values\_fill=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 276
##   sample acidic aftertaste alcohol appeal apple aroma
##   <chr>   <int>      <int>   <int>  <int> <int> <int>
## 1 182         6          2       3      1    18     4
## 2 239         5          2       2      2    19     5
## 3 365         3          3       0      0    25     3
## 4 401         4          2       2      0     9     5
## # ... with 2 more rows, and 269 more variables:
## #   artificial <int>, astringent <int>, bad <int>,
## #   banana <int>, barn <int>, begin <int>,
## #   bitter <int>, blackberry <int>, bland <int>,
## #   bold <int>, bubble <int>, candy <int>,
## #   cider <int>, clean <int>, crisp <int>,
## #   decent <int>, different <int>, dog <int>, ...
\end{verbatim}

A first look at the contingency table shows that \texttt{apple} has been used 25 times to characterize sample \texttt{365} while it has only been used 9 times to characterize sample \texttt{401}.

Since the list of terms is quite large, we can visualize these frequencies in different ways: First, we could re-adapt the histogram produced previously overall but per product. This could give a good overview of which words characterize each sample (results not shown here):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prod\_term }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lemma), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(sample)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sample, lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{split}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{sample) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{map}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(data)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{filter}\NormalTok{(n}\SpecialCharTok{\textgreater{}=}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(lemma, n), }\AttributeTok{y=}\NormalTok{n))}\SpecialCharTok{+}
      \FunctionTok{geom\_col}\NormalTok{()}\SpecialCharTok{+}
      \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
      \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
      \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
      \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{colour=}\StringTok{"grey80"}\NormalTok{))}\SpecialCharTok{+}
      \FunctionTok{coord\_flip}\NormalTok{()}\SpecialCharTok{+}
      \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"List of words mentioned at least 5 times for "}\NormalTok{, }
\NormalTok{                     data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(sample) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()))}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

Another approach consists in visualizing the association between the samples and the words in a multiple way using Correspondence Analysis (CA). Since the CA can be sensitive to low frequencies (Add REF), we suggest to only keep terms that were at least mentioned 5 times across all samples, resulting in a shorter frequency table. We then use the \texttt{CA()} function from \texttt{\{FactoMineR\}} to build the CA map:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider\_ct }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lemma), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(sample)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sample, lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}=} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{lemma, }\AttributeTok{values\_from=}\NormalTok{n, }\AttributeTok{values\_fill=}\DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\AttributeTok{var=}\StringTok{"sample"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(FactoMineR)}
\NormalTok{cider\_CA }\OtherTok{\textless{}{-}} \FunctionTok{CA}\NormalTok{(cider\_ct)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-332-1.pdf}

As can be seen, sample \texttt{731} is more strongly associated to alcoholic terms such as \texttt{alcohol} or \texttt{wine}, and colors (\texttt{red}, \texttt{green}). Samples \texttt{239} and \texttt{401} are more associated to \texttt{sour} and \texttt{bitter} (and \texttt{pear} for \texttt{239}), whereas samples \texttt{519} and \texttt{182} are more frequently described by terms such as \texttt{fruity}, and \texttt{sweet} (\texttt{floral} is also used to characterize \texttt{182}).

An alternative for visualizing these frequencies is through wordclouds, which can easily be done using the \texttt{\{ggwordcloud\}} package. This package has the advantage to build such representation in a \texttt{\{ggplot2\}} format. Such wordclouds (here one per product) can be obtained using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider\_wc }\OtherTok{\textless{}{-}}\NormalTok{ cider }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lemma), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(sample)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sample, lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}=} \DecValTok{5}\NormalTok{)}

\FunctionTok{library}\NormalTok{(ggwordcloud)}
\FunctionTok{ggplot}\NormalTok{(cider\_wc, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{sample, }\AttributeTok{colour=}\NormalTok{sample, }\AttributeTok{label=}\NormalTok{lemma, }\AttributeTok{size=}\NormalTok{n))}\SpecialCharTok{+}
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{(}\AttributeTok{eccentricity =} \FloatTok{2.5}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-333-1.pdf}

In these wordclouds, we notice that \texttt{apple} and \texttt{sweet} appear in larger fonts for (almost) all the samples, which can make the comparison quite difficult between samples. Fortunately, the \texttt{geom\_text\_wordcloud()} function provides an interesting parameter in its aesthetics called \texttt{angle\_group} which allows controlling the position of the words. To illustrate this, let's apply the following rule: for a given sample, if the proportion of association of a word is larger than 1/6 (as we have 6 samples), the word will be printed in the upper part of its wordcloud, and in the lower part otherwise. To facilitate the readability, the color code used follow the same rule:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider\_wc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(lemma) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{colour=}\NormalTok{ prop}\SpecialCharTok{\textless{}}\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{label=}\NormalTok{lemma, }\AttributeTok{size=}\NormalTok{n, }
             \AttributeTok{angle\_group =}\NormalTok{ prop }\SpecialCharTok{\textless{}} \DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{(}\AttributeTok{eccentricity =} \FloatTok{2.5}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sample)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-334-1.pdf}

As can be seen, the term \texttt{apple} is more frequently (i.e.~more than 1/6) used to characterize samples \texttt{182}, \texttt{239}, \texttt{365}, and \texttt{731}. The term \texttt{sweet} is more frequently used to characterize samples \texttt{182} and \texttt{519}. Such conclusions would have been more difficult to reach based on the previous \emph{unstructured} wordcloud.

\hypertarget{bigrams-n-grams}{%
\subsection{\texorpdfstring{Bigrams, \emph{n}-grams}{Bigrams, n-grams}}\label{bigrams-n-grams}}

In the previous set of analyses, we defined each word as a token. This procedure disconnects words from each others, hence discarding the context around each word. Although this approach is common, it can lead to misinterpretation since a product that would often be associated to (say) \emph{not sweet} would in the end be characterized as \emph{not} and \emph{sweet}. A comparison of samples based on the sole word \emph{sweet} could suggest that the previous product is often characterized as sweet whereas it should be the opposite.

To avoid this misinterpretation, two solutions exist:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace \emph{not sweet} by \emph{not\_sweet}, so that it is considered as one token rather than two;
\item
  Look at groups of words, i.e.~at words within their surroundings.
\end{enumerate}

The latter option leads us to introduce the notion of bi-grams (groups of 2 following words), tri-grams (groups of 3 following words), or more generally \emph{n}-grams (groups of \emph{n} following words). More precisely, we are applying the same frequency count as before except that we are no longer considering one word as a token, but as a sequence of 2, 3, or more generally \emph{n} words as a token. Such grouping can be obtained by the \texttt{unnest\_tokens()} from \texttt{\{tidytext\}} in which \texttt{token=\textquotesingle{}ngrams\textquotesingle{}}, with \texttt{n} defining the number of words to consider.

For simplicity, let's apply this to the original data, although it could be applied to the cleaned version (here we consider bi-grams).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider\_2grams }\OtherTok{\textless{}{-}}\NormalTok{ cider\_og }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(bigrams, comments, }\AttributeTok{token=}\StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n=}\DecValTok{2}\NormalTok{)}

\NormalTok{cider\_2grams }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(bigrams) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,230 x 2
##   bigrams          n
##   <chr>        <int>
## 1 sweet fruity    11
## 2 a little         9
## 3 slight apple     9
## 4 smells like      9
## # ... with 1,226 more rows
\end{verbatim}

In our example, \texttt{sweet\ fruity} is the strongest 2-words association. Other relevant associations are \texttt{green\ apple}, \texttt{sweet\ apple}, or \texttt{very\ sweet}.
Of course, such bi-grams can also be obtained per product:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cider\_2grams }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sample) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(bigrams) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(sample }\SpecialCharTok{==} \StringTok{"182"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 255 x 3
##   sample bigrams        n
##   <chr>  <chr>      <int>
## 1 182    hint of        3
## 2 182    not sweet      3
## 3 182    not very       3
## 4 182    red apples     3
## # ... with 251 more rows
\end{verbatim}

For sample \texttt{182}, \texttt{not\ sweet} appears 3 times which can be surprising since it was one of the sample the most associated to \texttt{sweet} with 22 occurrences.

\hypertarget{word-embedding}{%
\subsection{Word Embedding}\label{word-embedding}}

The previous section introduces the concept of context, as words are associated to their direct neighbors. Another approach called \emph{word embedding} goes one step further by looking at connections between words within a certain window: for instance, how often are \emph{not} and \emph{sweet} present together within a window of 3, 5, or 7 words? Such an approach is not presented here as it is more relevant for longer text documents.

In the previous sections, we already introduced the notion of \emph{term frequency} (\emph{tf}), which corresponds to the number of times a word is being used in a document. When a collection of documents are analyzed and compared, it is also interesting to look at the \emph{inverse document frequency} (\emph{idf}), which consists in highlighting words that discriminate between documents by reducing the weight of common words and by increasing the weight of words that are specific to certain documents only. In practice, both concepts are associated (by multiplication) to compute a term's \emph{tf-idf}, which measures the frequency of a term adjusted for its rarity in use.

\hypertarget{sentiment-analysis}{%
\subsection{Sentiment Analysis}\label{sentiment-analysis}}

Textual analysis as we presented here is purely descriptive. In other words, the items that we analyze have no particular valence (i.e.~they are neither negative, nor positive). When text data are more spontaneous (e.g.~social media such as tweets, or consumers' responses to open-ended questions), they can be the charged with positive or negative connotations. A good way to measure the overall valence of a message is through \emph{Sentiment Analysis}.

To perform \emph{Sentiment Analysis}, we start by deconstructing the message into words (tokenization approach considered previously). Then, in a similar approach to the stop words, we can combine our list of words with a pre-defined list that defines which words should be considered as positive or negative (the rest being neutral). Ultimately, all the scores associated to each message can be summed, hence providing the overall valence score of a message.

To get examples of \emph{sentiment} list, the \texttt{get\_sentiments()} function from the \texttt{\{tidytext\}} package can be used. This function proposes 4 potential lists: \texttt{"bing"}, \texttt{"afinn"}, \texttt{"loughran"}, and \texttt{"nrc"} (REFERENCES). Of course, such lists can be modified and adapted to your own needs in case they do not fit perfectly.

\hypertarget{to-go-further-1}{%
\section{To go further\ldots{}}\label{to-go-further-1}}

Text Mining and Natural Language Processing is a topic that has been (and is still being) studied for a very long time. Recently, it has made a lot of progress thanks to the advances in technology, and has gain even more interest with the abundance of text through social media, websites, blogs, etc. It is hence no surprise that a lot of machine learning models use text data (topic modelling, classification of emails to spam, etc.). Even current handy additions to simplify our life are based on text analysis (e.g.~suggestions in emails, translation, etc.)

In case you would want to go further on this topic, we strongly recommend the following books:

\begin{itemize}
\tightlist
\item
  Text Mining with R
\item
  Supervised Machine Learning for Text Analysis in R
\item
  Textual Data Science with R
\item
  R for Data Science (through the introduction to web-scrapping etc.)
\end{itemize}

\hypertarget{dashboards}{%
\chapter{Dashboards}\label{dashboards}}

\begin{quote}
Since preparing the data and analyzing them is only a part of the story, we tackled in Section \ref{auto-report} how to generate a report from your analysis (and briefly how to analyze your data within your report) while Section \ref{value-delivery} discussed points to consider to be as impactful in your communication as possible. For the latter, many formats on how to deliver and present your results were suggested, including the use of interactivity through dashboards. Since it is possible to build dashboard in R, we had to include a section that would introduce you to such solution. So embrace it, integrate it to your toolbox, and soon it will be your turn to shine during presentations!
\end{quote}

\hypertarget{objectives}{%
\section{Objectives}\label{objectives}}

We have certainly been all through situations in which we spent a lot of time analyzing data for our study, built our report and our story, spent time in perfecting our presentation. But when come the day of the presentation to your manager and colleagues, we get questions such as: \emph{What would happen if we split the results between users/non users, or between gender for instance?} In case you haven't been prepared for this question, and didn't run the analysis up-front, you probably answered something like: \emph{Let me re-run some analyses and I'll update you on that!}

Now imagine that you are in a similar situation, except that when such question arises, you have a tool that can answer live their questions using the data and the same analysis. Wouldn't that bring your discussion to another level?
Even better: Imagine you can share this tool with your colleagues for them to use it and answer their own questions, or use it for their own projects, even if they are not good at coding (nor even slightly familiar with R)?

Simply said, this is one of the roles of dashboards, as it brings interactivity to results (tables, graphs) by re-running and updating them when data, options, and/or parameters are being altered.

The goal of this section is to build such dashboard using R and the \texttt{\{shiny\}} package.

\hypertarget{introduction-to-shiny-through-an-example}{%
\section{Introduction to Shiny through an Example}\label{introduction-to-shiny-through-an-example}}

\hypertarget{what-is-a-shiny-application}{%
\subsection{What is a Shiny application?}\label{what-is-a-shiny-application}}

In case you have already been through the visualization section \ref{data-viz}, you've already been briefly introduced to some sort of dashboard in R through the \texttt{\{esquisse\}} package. In this section, the \texttt{\{shiny\}} package is used to build such dashboard.

\texttt{\{shiny\}} is an R package that allows you to directly create from R interactive web applications. Its goal is to \emph{convert} your R code into an application that can be accessible and used by anyone through their web browser, without having to be familiar with R.

This procedure is made available as \texttt{\{shiny\}} uses some carefully curated set of user interface (UI) functions that generate the HTML, CSS, and JavaScript code needed for most common tasks. In most cases, further knowledge of these languages is not required\ldots unless you want to push your application further. Additionally, it introduces a new way of programming called \emph{reactive programming}, which tracks automatically dependencies between code: When a change in an input is detected, any code that is affected by this change will automatically be updated.

\hypertarget{starting-with-shiny}{%
\subsection{Starting with Shiny}\label{starting-with-shiny}}

To create your very first shiny application, you can click on R studio in the \emph{new page} icon and select \emph{Shiny Web App\ldots{}}
Once you filled in the relevant information (name, author), you can then decide whether you want to create one unique file (\texttt{app.R}) or multiple files (\texttt{ui.R} and \texttt{server.R}).

Both solutions are equivalent and work the same: In both cases, a \texttt{ui()} and a \texttt{server()} function are generated. Due to better readability, and to ease its maintenance over time, we recommend to use the single file for short applications, and to use multiple files for larger applications (larger meaning with more code lines).

For our short application, it seems more convenient to use a single file.

\hypertarget{illustration}{%
\subsection{Illustration}\label{illustration}}

For illustration, let's consider a simple application in which we would import a data set that contains sensory data, as collected from a trained panel. In this example, the data set structure follows the one in \emph{biscuits\_sensory\_profile.xlsx}.

For simplification purposes, the code developed for this application requires that the data set contains one column called \emph{Judge} (containing the panelist information), one column called \emph{Product} (containing the product information), all the other columns being quantitative (corresponding to the sensory attributes).

The goal of the application is then to compute from this raw data set the sensory profiles of the products and to display them on screen. Furthermore, we also represent these sensory profiles graphically in a spider plot or in a circular bar plot.
Since the main goal of shiny application is in its interactivity, the user should have the opportunity to remove/add/reorder the attributes on the table and plots, and to hide/show products to increase visibility.

Once the graphics match our needs, we also propose to download it as a \emph{.png} file to integrate it in our report.

From a learning perspective, this application introduces you specifically to:

\begin{itemize}
\tightlist
\item
  Importing an external file to the application;
\item
  Create options that that are both independent (type of graph to produce) and depend of the file imported (list of attributes and products);
\item
  Run some analyses (compute the means) and display the results as a table and as a plot;
\item
  Export the graph to your computer as a png file.
\end{itemize}

\begin{quote}
The code presented in the next section can be found in \texttt{app.R}. In the next sections, pieces of code are shown for explanation, and should/can not be run on their own. Instead, the entire application should be run.
\end{quote}

\hypertarget{user-interface}{%
\subsubsection{User Interface}\label{user-interface}}

The user interface (UI) is the part of the application that controls what the user sees and controls.
In our example, the UI is separated into two parts:

\begin{itemize}
\tightlist
\item
  The left panel contains the options that the user can manually change;
\item
  The right (or main) panel contains the outputs.
\end{itemize}

In the \emph{app.R} file, this information is stored in the \texttt{ui()} function, and the two panels can be found in \texttt{sidebarPanel()} and in \texttt{mainPanel()} respectively.

In \texttt{sidebarPanel()}, all the options are set up. These options include \texttt{fileInput()} for importing the data set, or \texttt{radioButtons()} to control the type of plot to generate. A large list of options exist including \texttt{numericInput()}, \texttt{sliderInput()}, \texttt{textInput()}, \texttt{passwordInput()}, \texttt{dateInput()}, \texttt{selectInput()}, \texttt{checkboxInput()} etc. Note that this library of options can be extended by adding \texttt{checkboxGroupInput()} from the \texttt{\{shinyjs\}} package\footnote{To use \texttt{\{shinyjs\}}, you need to load the library and add \texttt{useShinyjs(),} at the start of your \texttt{ui()} code.}.

For most of these options, setting them up is quite straightforward, especially when (say) the range of values is already known beforehand (e.g.~p-value ranging from 0 to 1, with default value at 0.05). However, in some cases, the option of interest cannot be defined on the UI side since they depend on the data itself (e.g.~the product or attribute selection in our example). In such situation, these options are created on the server side, and are retrieved on the UI side through \texttt{uiOutput()}.

On \texttt{mainPanel()}, \texttt{tabsetPanel()} and \texttt{tabPanel()} control for the design of the output section. In our example, two tabs (one for the table and one for the graph) are created, although they could have been printed together on one page.

In our simple example, the \texttt{mainPanel()} is only use to export results computed on the server side. Depending on the type of output generated, the correct function used to retrieve the results should be used:

\begin{itemize}
\tightlist
\item
  For tables, \texttt{tableOutput()} is used to retrieve the table generated with \texttt{renderTable()};
\item
  For graphics, \texttt{plotOutput()} is used to retrieve the plot generated with \texttt{renderPlot()};
\item
  For elements to download, \texttt{downloadButton()} is used to retrieve the element (here a plot, but could be an Excel or PowerPoint file for instance) generated with \texttt{downloadHandler()}.
\end{itemize}

\begin{quote}
Note the pattern in the namings of the complementary functions: The \texttt{xxxOutput()} function (UI side) is used to retrieve the output generated (server side) by the corresponding \texttt{renderXxx()} function. This also applies with \texttt{uiOutput()} and \texttt{renderUI()}.
\end{quote}

\hypertarget{server}{%
\subsubsection{Server}\label{server}}

The server side of the application is where all the computations are being performed, including the construction of tables, figures, etc.

Since the options defined on the UI side should affect the computations performed (e.g.~our decision on the type of plot to design should affect the plot generated), we need to communicate these decisions to the server, and use them.
On the server side, any information (or option) passed to the server side is done through \texttt{input\$name\_option}. In our previous example regarding the type of graph to generate, this is shown as \texttt{input\$plottype}, as defined by:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{radioButtons}\NormalTok{(}\StringTok{"plottype"}\NormalTok{, }\StringTok{"Type of Plot to Draw:"}\NormalTok{, }
             \AttributeTok{choices=}\FunctionTok{c}\NormalTok{(}\StringTok{"Spider Plot"}\OtherTok{=}\StringTok{"line"}\NormalTok{, }\StringTok{"Circular Barplot"}\OtherTok{=}\StringTok{"bar"}\NormalTok{), }
             \AttributeTok{selected=}\StringTok{"line"}\NormalTok{, }\AttributeTok{inline=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this case, if the user select \emph{Spider Plot} (resp. \emph{Circular Barplot}), \texttt{input\$plottype} will take the value \emph{line} (resp. \emph{bar}).

Reversely, any information that is being build on the server side and that should be passed on the UI part of the application can either be done via the \texttt{xxxOutput()}/\texttt{renderXxx()} combination presented before (useful for showing results), including the \texttt{renderUI()}/\texttt{uiOutput()} combination (useful for options that are \emph{server-dependent}).

Following a similar communication system than the one from UI to server, the part generated on the server side is stored as \texttt{output\$name\_option} (defined as \texttt{renderUI()}) and is captured on the UI side using \texttt{uiOutput("name\_option")}.

In our example, the latter combination is used for the two options that require reading the data set first, namely the selection of attributes and the selection of products.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# server side:}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{attribute }\OtherTok{\textless{}{-}} \FunctionTok{renderUI}\NormalTok{(\{}
    
    \FunctionTok{req}\NormalTok{(}\FunctionTok{mydata}\NormalTok{())}
    
\NormalTok{    items }\OtherTok{\textless{}{-}} \FunctionTok{mydata}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{pull}\NormalTok{(Attribute) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{as.character}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{unique}\NormalTok{()}
    
    \FunctionTok{selectInput}\NormalTok{(}\StringTok{"attribute"}\NormalTok{, }
                \StringTok{"Select the Attributes (in order) "}\NormalTok{, }
                \AttributeTok{choices=}\NormalTok{items, }\AttributeTok{selected=}\NormalTok{items, }
                \AttributeTok{multiple=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  \})}

\NormalTok{  output}\SpecialCharTok{$}\NormalTok{product }\OtherTok{\textless{}{-}} \FunctionTok{renderUI}\NormalTok{(\{}
    
    \FunctionTok{req}\NormalTok{(}\FunctionTok{mydata}\NormalTok{())}
\NormalTok{    items }\OtherTok{\textless{}{-}} \FunctionTok{mydata}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{pull}\NormalTok{(Product) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{unique}\NormalTok{()}
    
    \FunctionTok{checkboxGroupInput}\NormalTok{(}\StringTok{"product"}\NormalTok{, }
                       \StringTok{"Select the Products to Display:"}\NormalTok{, }
                       \AttributeTok{choices=}\NormalTok{items, }\AttributeTok{selected=}\NormalTok{items)}
\NormalTok{  \})}
  
\CommentTok{\# UI side:}
  \FunctionTok{uiOutput}\NormalTok{(}\StringTok{"attribute"}\NormalTok{)}
  \FunctionTok{uiOutput}\NormalTok{(}\StringTok{"product"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Lastly, we have elements that are only relevant on the server side, namely the computation themselves. In our example, these are results of a function called \texttt{reactive()}.

Reactivity (and its corresponding \texttt{reactive()} function) is a great \emph{lazy} feature of \texttt{\{shiny\}} that was designed so that the computations are only performed when necessary, i.e.~when changes in an input affects computations. This laziness is of great power since only computations that are strictly needed are being performed, hence increasing speed by limiting the computation power required to its minimum.

Let's break this down in a simple example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  mydata }\OtherTok{\textless{}{-}} \FunctionTok{reactive}\NormalTok{(\{}
    \FunctionTok{req}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{datafile)}
\NormalTok{    data }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_xlsx}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{datafile}\SpecialCharTok{$}\NormalTok{datapath, }\AttributeTok{sheet=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Judge, Product), }
                   \AttributeTok{names\_to=}\StringTok{"Attribute"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Score"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Attribute =} \FunctionTok{fct\_inorder}\NormalTok{(Attribute), }
             \AttributeTok{Score =} \FunctionTok{as.numeric}\NormalTok{(Score)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{group\_by}\NormalTok{(Product, Attribute) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Score =} \FunctionTok{mean}\NormalTok{(Score)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{ungroup}\NormalTok{()}
    \FunctionTok{return}\NormalTok{(data)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

In this section, the file selected by the user is read through the \texttt{fileInput()} option called \texttt{datafile} on the UI side. Note that in this case, the path of the file is stored in the object called \texttt{datapath}, meaning that to access this file, we need to read \texttt{input\$datafile\$datapath}.

Once read (here using \texttt{\{readxl\}}), some small transformations to the data are performed before saving its final version in an object called \texttt{mydata}. Since this block of code only depends on \texttt{input\$datafile} (UI side), this part is no longer used unless \texttt{datafile} is being updated or changed.

For the computation of the means, the same procedure applies as well:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  mymean }\OtherTok{\textless{}{-}} \FunctionTok{reactive}\NormalTok{(\{}
    
    \FunctionTok{req}\NormalTok{(}\FunctionTok{mydata}\NormalTok{(), input}\SpecialCharTok{$}\NormalTok{attribute, input}\SpecialCharTok{$}\NormalTok{product)}
\NormalTok{    mymean }\OtherTok{\textless{}{-}} \FunctionTok{mydata}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Product"}\NormalTok{, }\StringTok{"Attribute"}\NormalTok{), as.character)) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{filter}\NormalTok{(Attribute }\SpecialCharTok{\%in\%}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{attribute) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Product =} \FunctionTok{factor}\NormalTok{(Product, input}\SpecialCharTok{$}\NormalTok{product),}
             \AttributeTok{Attribute =} \FunctionTok{factor}\NormalTok{(Attribute, input}\SpecialCharTok{$}\NormalTok{attribute),}
             \AttributeTok{Score =} \FunctionTok{format}\NormalTok{(}\FunctionTok{round}\NormalTok{(Score, }\DecValTok{2}\NormalTok{), }\AttributeTok{nsmall=}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from=}\NormalTok{Product, }\AttributeTok{values\_from=}\NormalTok{Score)}
    
    \FunctionTok{return}\NormalTok{(mymean)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

For this \texttt{reactive()} block, \texttt{mymean} depends on \texttt{mydata}, \texttt{input\$attribute}, and \texttt{input\$product}. This means that if \texttt{datafile} (read \texttt{mydata}), \texttt{input\$attribute} and/or \texttt{input\$product} change, the computations re-run and \texttt{mymean} is getting updated.

For small and simple examples like ours, this domain of reactivity may be sufficient, and would be sufficient in many cases. There are however some few points that require a bit more explanations.

First, we advise that you use \texttt{reactive()} as much as possible: In our example, we could have created the code to build the graph within \texttt{renderPlot()}. However, this way of coding is not efficient since it would always be updated, even when it is not necessary. For small examples such as the one proposed here, this may not make much difference, but for larger applications it would have a larger impact. This is why we prefer to create the graphs in a \texttt{reactive()} instance, and simply retrieve it for display.

Second, and as you may have seen already, the output of a \texttt{reactive()} section can be re-used in other sections. This means that just like in \emph{regular coding}, you can save elements in R object that you can re-use later (e.g.~\texttt{mydata}, \texttt{mymean}, or \texttt{myplot}). However, these elements act like functions, meaning that if you want to call them, you should do it as \texttt{mydata()} for instance. More generally, let's imagine that \texttt{mydata} is a list with two elements (say \texttt{mydata\$element1} and \texttt{mydata\$element2}), we would retrieve \texttt{element1} as \texttt{mydata()\$element1}.

Third, let's introduce the function \texttt{req()} that is used at the start of almost every block of code on the server side. To do so, let's take the example of \texttt{output\$attribute} which starts with \texttt{req(mydata())}. The \texttt{req()} functions aims in requiring the object mentioned (here \texttt{mydata()}) before running: if \texttt{mydata()} doesn't exist, then \texttt{output\$attribute} is set as \texttt{NULL}. This small line of code comes handy as it avoids returning design errors: How to extract a list of attributes from data that do not exist yet?

Finally, the application that we are developing here is \emph{over-reactive} as every change we do will create update results. To highlight this, just remove some attributes from the list and you'll see the mean table or graphic being updated. In our small example, this is not too problematic since the application runs fast, but in other instances in which more computation is required, you may not want to wait that each little change done is being processed. To over come this, you can replace \texttt{reactive()} by \texttt{eventReactive()} combined with a button (e.g.~\emph{Run} or \emph{Apply changes}) that only trigger changes once pressed. This means that changes are only performed on a user action.

\hypertarget{deploying-the-application}{%
\subsection{Deploying the Application}\label{deploying-the-application}}

To run the application, three options exist (within RStudio):

\begin{itemize}
\tightlist
\item
  Push the \emph{Run app} button on the task bar of your script (a menu allows you to run the app in the Viewer window, or as a new window).
\item
  Type directly \texttt{shiny::runApp(\textquotesingle{}code\textquotesingle{})} in R.
\item
  Use the shortcut \emph{CTRL}+\emph{SHIFT}+\emph{ENTER} (on windows).
\end{itemize}

In this case, your computer (and RStudio) will use a virtual local server to build the application.

Unfortunately, this solution is not sufficient in case you really want to share it with colleagues. In such case, you need to publish your app by installing it on a server. Ideally, you have access to a server that can host such application (check with your IT department). If not, you can always create an account on \url{https://www.shinyapps.io/admin/} and install it there. Note that there is a free registration option, which comes with limitations (number of applications to install). Also, before using their services, make sure that their conditions (privacy, protection of the data, etc.) suit you and your company policy.

\hypertarget{to-go-further-2}{%
\section{To go further\ldots{}}\label{to-go-further-2}}

Quickly, \texttt{\{shiny\}} became popular and many researchers developed additional packages and tools to further enhance it. Without being exhaustive, here are a few tools, widgets, and packages that we often use as they provide interesting additional features. But don't hesitate to look online for features that answer your needs!

\hypertarget{personalizing-and-tuning-your-application}{%
\subsection{Personalizing and Tuning your application}\label{personalizing-and-tuning-your-application}}

If you start building many applications in \texttt{\{shiny\}}, you might get tired of its default layout. Fortunately, the overall appearance of applications can be modified easily, especially if you have notion in other programming language such as CSS and HTML. If not, no worries, there are alternative solutions for you, including the \texttt{\{bslib\}} package.
To change the layout, simply load the library, and add the following piece of code at the start of your application (here for using the theme \emph{darkly}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fluidPage}\NormalTok{(}
  \AttributeTok{theme =}\NormalTok{ bslib}\SpecialCharTok{::}\FunctionTok{bs\_theme}\NormalTok{(}\AttributeTok{bootswatch =} \StringTok{"darkly"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{\{bslib\}} also allows you creating your own theme that match your company style. So do not hesitate to take some time to build it once, and to apply it to all your applications.

Besides changing the overall theme of your applications, there are certain details that can make your overall application more appealing and easier to use. In our short example, you may have noticed that each option and each output is a separate line. This is sufficient here since the number of options and outputs are very limited. However, as the application grows in size, this solution is no longer sustainable as you do not want the users to scroll down a long way to adjust all the options.

Instead, you can combine different options (or outputs) on one line. To do so, \texttt{\{shiny\}} works by defining each panel as a table with as many rows as needed and 12 columns. When not specified, the whole width (i.e.~the 12 columns) of the screen is used, but this could be controlled through the \texttt{column()} function.

In our example, we could have position the product selection and the attribute selection on the same line using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fixedRow}\NormalTok{(}
  \FunctionTok{column}\NormalTok{(}\DecValTok{6}\NormalTok{, }\FunctionTok{uiOutput}\NormalTok{(}\StringTok{"product"}\NormalTok{)),}
  \FunctionTok{column}\NormalTok{(}\DecValTok{6}\NormalTok{, }\FunctionTok{uiOutput}\NormalTok{(}\StringTok{"attribute"}\NormalTok{))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{upgrading-tables}{%
\subsection{Upgrading Tables}\label{upgrading-tables}}

The default table built in \texttt{\{shiny\}} using the \texttt{tableOutput()}/\texttt{renderTable()} combination is very handy, yet limited in its layout. Fortunately, many additional packages that create HTML tables have been developed to provide alternative solution to build more flexible tables.

In particular, we recommend the use of the \texttt{\{DT\}} and \texttt{\{rhandsontable\}} packages as they are very simple to use, and yet provides a large variety of powerful design options. Just to name a few, it allows:

\begin{itemize}
\tightlist
\item
  cells or text formatting (e.g.~coloring, rounding, adding currencies or other units, etc.);
\item
  merge rows or columns;
\item
  add search/filter fields to columns of the table;
\item
  provide interactivity, that for instance can be connected to graphics;
\item
  include graphics within cells;
\item
  allows manual editing, giving the user the chance to fill in or modify some cells;
\end{itemize}

To build such table, you will need to load the corresponding packages. For \texttt{\{DT\}} tables, you can generate and retrieve them using the complementary functions \texttt{renderDataTable()} and \texttt{dataTableOutput()}, or its concise forms \texttt{renderDT()} and \texttt{DTOutput()}\footnote{the table should be generated using \texttt{datatable()}.}. For \texttt{\{rhandontable\}} tables, you can generate and retrieve them using \texttt{renderRHandsontable()} and \texttt{rHandsontableOutput()}.

For more information and examples, please look at \url{https://rstudio.github.io/DT/} and \url{https://jrowen.github.io/rhandsontable/}.

\hypertarget{building-dashboard}{%
\subsection{Building Dashboard}\label{building-dashboard}}

The example used here illustrates the power of \texttt{\{shiny\}}. However, it is limited to our own data set, meaning that it is study specific. What if we would want to create a dashboard, that is connected to a database for instance and that updates its results as soon as new data is being collected?

This is of course the next step, and \texttt{\{shiny\}} can handle this thanks to the \texttt{\{shinydashboard\}} package.

In its principle, \texttt{\{shinydashboard\}} works in a similar way to \texttt{\{shiny\}} itself, except for the structure of the UI. For \texttt{\{shinydashboard\}}, the UI contains three sections as shown below (the example below generates an empty dashboard.):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(shiny)}
\FunctionTok{library}\NormalTok{(shinydashboard)}

\NormalTok{ui }\OtherTok{\textless{}{-}} \FunctionTok{dashboardPage}\NormalTok{(}
  \FunctionTok{dashboardHeader}\NormalTok{(),}
  \FunctionTok{dashboardSidebar}\NormalTok{(),}
  \FunctionTok{dashboardBody}\NormalTok{()}
\NormalTok{)}

\NormalTok{server }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input, output) \{ \}}

\FunctionTok{shinyApp}\NormalTok{(ui, server)}
\end{Highlighting}
\end{Shaded}

It is then your task to fill in the dashboard by for instance connecting to your data source (e.g.~a simple data set, a database, etc.) and to fill in your different tabs with the interactivity and outputs of interest.

For more information, including a comprehensive \emph{Get started} section, please visit \url{https://rstudio.github.io/shinydashboard/}

\hypertarget{interactive-graphics}{%
\subsection{Interactive Graphics}\label{interactive-graphics}}

Through its way of working, \texttt{\{shiny\}} creates some interactivity to graphics by updating it when changing some options. This is hence done by replacing a static graph by another static graph.

However, R provides other options that creates interactive graphs directly. This can be done thanks to the \texttt{\{plotly\}} library.

\texttt{\{plotly{]}} is an alternative library to \texttt{\{ggplot2\}} that can be used to build R graphics within the R environment: It is not specific to \texttt{\{shiny\}} and can be used outside Shiny applications. To build \texttt{\{plotly\}} visualizations, you can build it directly from scratch using the \texttt{plot\_ly()} function. It is out of the scope of this book to develop further how \texttt{\{plotly\}} works, mainly because we made the decision to explain in details how \texttt{\{ggplot2\}} works. And fortunately, \texttt{\{plotly\}} provides an easy solution to convert automatically \texttt{\{ggplot2\}} graph to \texttt{\{plotly\}} thanks to the \texttt{ggplotly()} function.

For more information, please visit \url{https://plotly.com/r/}

\hypertarget{interactive-documents}{%
\subsection{Interactive Documents}\label{interactive-documents}}

Ultimately, \texttt{\{shiny\}} can also be combined to other tools such as \texttt{\{rmarkdown\}} to build interactive tutorials, teaching material, etc. This is done by integrating the interactivity of \texttt{\{shiny\}} to propose options and reactive outputs into a text editor through \texttt{\{rmarkdown\}}.

To integrate \texttt{\{shiny\}} in your \texttt{\{rmarkdown\}}, simply add \texttt{runtime:\ shiny} in the YAML metadata of your R markdown document.

\hypertarget{documentation-and-books}{%
\subsection{Documentation and Books}\label{documentation-and-books}}

Thanks to its way of working and its numerous extensions, there is (almost) no limit to applications you can build (except maybe your imagination?). For inspiration, and to get a better idea of the powerful applications that you can build, have a look at the gallery on the official shiny webpage:
\url{https://shiny.rstudio.com/gallery/}

In this section, we just introduced you to the main functions available in \texttt{\{shiny\}}, but if you want to go further, there is a whole world for you to explore. Of course, a lot of material is available online, and we would not know where to start to guide you. However, we strongly recommend you to start with the book from Hadley Wickham entitled \emph{Mastering Shiny} (REF) as it is comprehensive and will give you the kick start that you need\ldots and more.

\hypertarget{next-steps}{%
\chapter{Conclusion and Next Steps}\label{next-steps}}

Congratulations, you've reached the end of this book!

We hope we have motivated you to continue your amazing journey into the emerging computational sensory science field. To give you a little hand, we are listing here some other resources we would recommend and a summary of the main useful packages for sensory and consumer data analysis/visualization, including the ones we used throughout this book.

\hypertarget{other-recommended-resources}{%
\section{Other Recommended Resources}\label{other-recommended-resources}}

\begin{itemize}
\tightlist
\item
  \emph{R for Data Science} by Garrett Grolemund and Hadley Wickham (\url{https://r4ds.had.co.nz/})
\item
  \emph{Analyzing Sensory Data with R} by Sebastien Le and Thierry Worch
\item
  \emph{Rapid Sensory Profiling Techniques and Related Methods} by Julien Delarue, J. Ben Lawlor, and Michel Rogeaux
\item
  \emph{Practical Guide to Cluster Analysis in R} by Alboukadel Kassambara
\item
  \emph{Practical Guide to Principal Component Methods in R} by Alboukadel Kassambara
\item
  \emph{Multiple Factor Analysis by Example Using R} by Jerome Pages
\item
  \emph{Using the flextable R package} by David Gohel (\url{https://ardata-fr.github.io/flextable-book/index.html})
\item
  \emph{Tidy Modelling with R} by Max Kuhn and Julia Silge (\url{https://www.tmwr.org/})
\item
  \emph{Hands-On Machine Learning with R} by Brad Boehmke and Brandon Greenwell (\url{https://bradleyboehmke.github.io/HOML/})
\item
  \emph{Introduction to Statistical and Machine Learning Methods for Data Science} by Carlos Andre Reis Pinheiro and Mike Patetta
\item
  \emph{Supervised Machine Learning for Text Analysis in R} by Emil Hvitfeldt and Julia Sigle
\item
  \emph{R Graphics Cookbook: Practical Recipes for Visualizing Data} by Winston Chang
\item
  \emph{Text Mining with R: A Tidy Approach} by David Robinson and Julia Silge
\item
  \emph{Textual Data Science with R} by Mónica Bécue-Bertaut
\item
  \emph{Design and Analysis of Experiments with R} by John Lawson
\item
  \emph{Mastering Shiny} by Hadley Wickham (\url{https://mastering-shiny.org/})
\end{itemize}

Some interesting book related to story telling, graphical design and data visualization:

\begin{itemize}
\tightlist
\item
  \emph{Storytelling with Data} by Cole Nussbaumer Knaflic
\item
  \emph{Beyond Bullet Points} by Cliff Atkinson
\item
  \emph{Once Upon an Innovation} by Jean Storlie and Mimi Sherlock
\item
  \emph{Show me the Number: Designing Table and Graphs to Enlighten} by Stephen Few
\end{itemize}

\hypertarget{useful-r-packages}{%
\section{Useful R Packages}\label{useful-r-packages}}

\begin{itemize}
\tightlist
\item
  \texttt{FactoMineR}: package dedicated to multivariate Exploratory Data Analysis including Principal Components Analysis (PCA), Correspondence analysis (CA), Multiple Correspondence Analysis (MCA), clustering.
\item
  \texttt{FactoExtra}: package that makes easy to extract and visualize the output of exploratory multivariate data analyses, including Principal Component Analysis (PCA), Correspondence Analysis (CA), Multiple Correspondence Analysis (MCA), Multiple Factor Analysis (MFA), Hierarchical Clustering (HCKUST) and partioning Clustering (E.g. k-means, PAM,CLARA, etc.)
\item
  \texttt{SensR}: package for Thurstonian Models for sensory discrimination methods, including duotrio, tetrad, triangle, 2-AFC, 3-AFC, A-not A, same-different, 2-AC and degree-of-difference. This package enables the calculation of d-primes, standard errors of d-primes, sample size and power computations, and comparisons of different d-primes.
\item
  \texttt{SensoMineR}: package dedicated to the statistical analysis of sensory data. It tackles the characterization of the products, panel performance assessment, links between sensory and instrumental data, consumer's preferences, napping evaluation, optimal designs.
\item
  \texttt{tempR}: package for Analysis and visualization of data from temporal sensory methods, including temporal check-all-that-apply (TCATA) and temporal dominance of sensation.
\item
  \texttt{sensmixed}: package to analyze sensory and consumer data within mixed effects model framework.
\item
  \texttt{corrplot}: package that provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.
\item
  \texttt{stats}: this This package contains functions for statistical calculations and random number generation. The analysis include ANOVA, posthoc tests, Clustering, Correlation, multivariate analysis, among many others.
\end{itemize}

  \bibliography{references.bib}

\backmatter
\printindex

\end{document}

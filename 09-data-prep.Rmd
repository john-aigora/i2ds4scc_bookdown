# Data Preparation {#data-prep}

>After importing the data, the next crucial step is to ensure that the data as it is now available is of good quality and is the correct representation of reality. As an example, during importation, software (such as R) tends to guess (from reading the file) the nature of each variable. If such guess is correct in 99% of the case, there are situations in which it is erroneous, and ignoring such error can have huge consequences on the final results and conclusions. The goal of this section is hence to perform some pre-check of the data and to prepare them for future analyses.

## Introduction

*Data Preparation*, which consists of *data inspection* and *data cleaning*, is a critical step before any further *Data Manipulation* or *Data Analysis*. Having a good data preparation procedure ensures a good understanding of the data, and avoids what could be very critical mistakes.

To illustrate the importance of the later point, let's imagine a study in which the samples are defined by their 3-digits code. 
During importation, R would recognize them as number, and hence defines the *Product* column as numerical. Without inspection and correction, any ANOVA that include the product effect would be replaced by a linear regression (or analysis of covariance) which of course does not provide the results required (although the analysis would run without error). Worst, if this procedure is automated and the *p-value* associated to the product effect is extracted, the conclusions would rely on the wrong analysis! A good data preparation procedure is hence important to avoid such unexpected results. 

So what consists of data preparation, and how does that differ from data manipulation? 
There is clearly a thin line between data preparation (and particularly *data cleaning*) and data manipulation, as both these steps share many procedures in common (same applies to data manipulation and data analysis for instance). Although multiple definitions of each step exist, we decided to follow the following rule:

*Data Preparation* includes all the required steps to ensure that the data is matching its intrinsic nature. These steps include inspecting the data at hand (usually through simple descriptive statistics of the data as a whole) and cleaning the data by eventually correcting importation errors (including the imputation of missing data). Although some descriptive statistics are being produced for data inspection, these analyses have no interpretation value besides ensuring that the data are in the right range, or following the right distribution. For instance, with our sensory data, we would ensure that all our sensory scores are included between 0 and 100 (negative scores would not be permitted), but we would not look at the mean or the distribution of the score per product which would belong to data analyses as it would often lead to interpretation (e.g. P01 is sweeter than P02).

The *Data Manipulation* is an optional step that adjust or convert the data into a structure that is usable for further analysis. This of course may lead to *interpretation* of the results as it may involve some analyses.

The *Data Analysis* step ultimately converts the data into results (through values, graphics, tables, etc.) that provide more insights (through interpretation) about the data.

The data used in this chapter corresponds to the *biscuits_sensory_profile.xlsx* that you already imported in Section \@ref(data-collection) but with few missing values. This new data set is stored in *biscuits_sensory_profile_with_NA.xlsx*.

As usual, we start this chapter by loading the main packages we need and by importing this data set:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)

file_path <- here("data","biscuits_sensory_profile_with_NA.xlsx")
sensory <- read_xlsx(file_path, sheet="Data")
```

## Inspect {#inspect}

### Data Inspection {#data-inspection}

To inspect the data, different steps can be used. 
First, since `read_xlsx()` returns a tibble, let's take advantage of its printing properties to get a fill of the data:

```{r, eval=TRUE, echo=TRUE}
sensory
```

Other informative solutions consists in printing a summary of the data through the `summary()` or `glimpse()`:

```{r, eval=TRUE, echo=TRUE}
summary(sensory)
```

```{r, eval=TRUE, echo=TRUE}
glimpse(sensory)
```

These functions provide relevant yet basic views of each variable present in the data including their types, the range of values, means, and medians, as well as the first values of each variables. 

Such view might be sufficient for some first conclusions (e.g. Are my panelists considered as numerical or nominal data? Do I have missing values?), yet it is not sufficient to fully ensure that the data is ready for analysis. For the latter, more extensive analyses can be performed automatically in different ways. These analyses include looking at the distribution of some variables, or the frequencies of character levels. 

A first solution comes from the `{skimr}` package and its `skim()` function. By applying it to data, an automated extended summary is directly printed on screen by separating `character` type variables from `numeric` type variables:

```{r, eval=FALSE, echo=TRUE}
library(skimr)
skim(sensory)
```

Another approach consists in generating automatically an html report with some pre-defined analyses using `create_report()` from the `{DataExplorer}` package.

```{r, eval=FALSE, echo=TRUE}
library(DataExplorer)
create_report(sensory)
```

Unless specified otherwise through `output_file`, `output_dir`, and `output_format`, the report will be saved as an html file on your active directory as *report.html*. This report provides many statistics on your data, including some simple statistics (e.g. raw counts, percentages), informs you on the structure of your data, as well as on eventual missing data. It also generates graphics to describe your variables (e.g. univariate distribution, correlation and PCA). 

>Note that the analyses performed to build this report can be called directly within R. For instance, `introduce()` and `plot_intro()` generates the first part of the report, whereas `plot_missing()` and `profile_missing()` provide information regarding missing data just to name those.

### Missing Data

In the previous section on \@ref(data-inspection), it can be seen that the data set contain missing values. It concerns for instance the attribute `Light`, for which one missing value has been detected. There are different ways in which we can handle such missing values. But first, let's try to find out where these missing values are, and which impact they may have on our analyses (are they structured or unstructured, etc.)

#### Visualization of Missing Values

A first approach to inspect and visualize where the missing values are is by representing them visually. To do so, the `{visdat}` package provides a neat solution as it represents graphically the data by highlighting where missing values are located. Such visual representation is obtained using the `vis_miss()` function:

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
library(visdat)
sensory %>% 
  vis_miss()
```

As can be seen, missing values are only present in few variables. However, `Sour` contains up to 10% of missing data, which can be quite critical in some situations.

If we would want to dig deeper and assess for which products (say) data are missing, we could re-create the same plots per product. The following code would generate that for you:

```{r, echo=TRUE, eval=FALSE}
sensory %>% 
  split(.$Product) %>% 
  map(function(data){
    vis_miss(data)
  })

```

Of course, such approach could also be applied per panelist for instance.

Once we've investigated where the missing values are located, we can go further by understanding if there are some sorts of relationship between missing values. In other words, are the missing values random? Or are they somewhat structured? 
To answer these questions, the `{naniar}` package provides an interesting function called `gg_miss_upset()` which studies the relationship between missing values:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(naniar)
sensory %>%
  gg_miss_upset()
```

It seems here that the only connection between NAs is observed between `Light` and `Color contrast`.

Such relational structure can also be visualized in a scatter plot using the `geom_miss_point()` function from the same package:

```{r, echo=TRUE, eval=TRUE}
ggplot(sensory, aes(x=Product, y=Sour))+
  geom_miss_point()
```

Here, the relationship between `Product` and `Sour` is shown. Such plot may help decide what to do with missing values, whether it is ignoring, removing, or predicting them. 

#### Ignoring Missing Values

A first solution to handle missing values is to simply *ignore* them, as many analyses handle them well. For instance, an ANOVA could be run for such attribute, and results are being produced:

```{r, eval=TRUE, echo=TRUE}
broom::tidy(aov(Light ~ Product + Judge, data=sensory))
```

This solution may work fine when the number of missing values is small, but be aware that it can also provide erroneous results in case they are not handled the way the analyst is expecting them to be handled.

For some other analyses, *ignoring* the presence of missing values may simply provide unwanted results. To illustrate this, let's compute the simple mean per product for `Light` 

```{r, eval=TRUE, echo=TRUE}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light)) %>% 
  ungroup()
```

As can be seen, since `P04` contains the missing value, its corresponding mean is defined as `NA`.

#### Removing Missing Values

To enforce the mean to be computed, we need to inform R to remove any missing values beforehand. Such procedure can be done manually by simply filtering out any missing data (here for `Sour`) before running the analysis:

```{r, echo=TRUE, eval=TRUE}
sensory %>% 
  filter(!is.na(Sour))
```

However, this latter solution is not always satisfactory as it also deletes real data since the data set went from 99 to 89 rows. This means that for variables that did not have missing values for instance, existing data have been removed. 

Hence, we prefer another alternative which consists in removing missing values within the analysis procedure (here `mean()`) through the parameter `na.rm=TRUE`:

```{r, eval=TRUE, echo=TRUE}
sensory %>% 
  group_by(Product) %>% 
  summarise(Light = mean(Light, na.rm=TRUE)) %>% 
  ungroup()
```

Using `na.rm=TRUE` is equivalent to removing the missing values from the data before performing the analysis, but only for the variable of interest. A similar approach consists in first rotating (using `pivot_longer()`) the data before removing missing values:

```{r, echo=TRUE, eval=TRUE}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores)) %>% 
  group_by(Product, Variables) %>% 
  summarize(Means = mean(Scores)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = Variables, values_from = Means) %>% 
  dplyr::select(Product, Sour, Light)
```

If this solution seems satisfactory as the means were computed without using `na.rm=TRUE` for both `Sour` and `Light` (who contained missing values), its use is limited since converting the data to its original format (i.e. performing `pivot_wider()` after `pivot_longer()` without computing the mean in between) will reintroduce the missing values^[Missing values do not need to be visible to exist: Incomplete designs are a good example showing that although the data do not have empty cells, it does contain a lot of missing data (the samples that were not evaluated by each panelist).].  

It should be noted that removing missing values has the impact of unbalancing the data. By taking the example of `Light` and `Sour`, let's print the number of panelist evaluating each product:

```{r, echo=TRUE, eval=TRUE}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores") %>% 
  filter(!is.na(Scores),
         Variables %in% c("Light","Sour")) %>%
  group_by(Product, Variables) %>% 
  count() %>% 
  ungroup() %>% 
  pivot_wider(names_from=Variables, values_from=n)
```

Here for example, the only missing value detected for `Light` is related to `P04`. For `Sour`, `P02`, `P07`, and `P09` only have 7 observations out of 9. 

The solution of *blindly* removing missing values is a solution that you may sometime use. However, it is not the only strategy, and we can consider other approaches that are more in-line with the nature of the data.

Rather than removing the missing values only, we could consider removing blocks of data, whether it is attributes, products, or panelists that present missing data. This solution is particularly handy when tests are performed in multiple sessions and some respondents did not manage to attend them all. It can then be relevant to remove completely those respondents from your data. 

The procedure presented below show the procedure on how to remove attributes with missing data, but could easily be adapted to panelists or products:

```{r, echo=TRUE, eval=TRUE}
sensory_long <- sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Variables", values_to="Scores")

(attr_rmv <- sensory_long %>% 
  filter(is.na(Scores)) %>% 
  pull(Variables) %>% 
  unique())

sensory_clean <- sensory_long %>% 
  filter(!(Variables %in% attr_rmv)) %>% 
  pivot_wider(names_from=Variables, values_from=Scores)

```

This procedure removed the 7 attributes that contained missing values, leading to a table with 99 rows and 29 columns (instead of 36).

#### Imputing Missing Values

Rather than removing missing data, another strategy consists in imputing missing values. Here again, many strategies can be considered, starting with replacing them with a fixed value. Such approach is usually not the most suitable one, yet it can be relevant in certain cases. For instance, in a CATA task, missing values are often replaced with 0s (not ticked).

To replace missing values with a fixed value, `replace_na()` can be used. When applied to a tibble, this function requires you defining using `list()` the columns to apply it to, and which values to use (each column being treated separately). 

For convenience, let's apply it to `sensory` by replacing missing values for `Sour` by the value `888` and for `Light` with `999` (we use these extreme values to track changes more easily):

```{r, eval=TRUE, echo=TRUE}
sensory %>% 
  replace_na(list(Sour = 888, Light = 999)) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

When dealing with intensity scale, it is more frequent to replace missing values by the mean score for that product and attribute. When the test is duplicated, the mean provided by the *panelist x product x attribute* combination across the different repetitions available is even preferred as it maintains individual variability within the scores. 

Such approach is a 2-steps process:

1. Compute the mean (since we do not have duplicates, we use the mean per product);
2. Combine it to the data

For simplicity, `sensory_long` is used as starting point:

```{r, echo=TRUE, eval=TRUE}
prod_mean <- sensory_long %>% 
  group_by(Product, Variables) %>% 
  summarize(Mean = mean(Scores, na.rm=TRUE)) %>% 
  ungroup()

sensory_long %>% 
  full_join(prod_mean, by=c("Product","Variables")) %>% 
  mutate(Scores = ifelse(is.na(Scores), Mean, Scores)) %>% 
  dplyr::select(-"Mean") %>% 
  pivot_wider(names_from=Variables, values_from=Scores) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, the missing value associated to `J01` for `Light` and `P04` has been replaced by `40.7`. In fact, any missing values related to `P04` and `Light` would automatically be replaced by `40.7` here. For other products (and other attributes), their respective means would be used.

When the model used to impute missing values is fairly simple (here, replacing by the mean correspond to a simple 1-way ANOVA), the imputation can be done directly through the `impute_lm()` function from the `{simputation}` package. To mimic the previous approach, the one-way ANOVA is being used^[It is worth noticing that the individual differences could also be included by simple adding the Judge effect in the model.]. Here, missing data for both `Sour` and `Light` are being imputed independently using the same model:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(simputation)

sensory %>% 
  impute_lm(Sour + Light ~ Product) %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

As can be seen, this procedure provides the same results as before, but in less steps!

In some situations, implementing missing values using such ANOVA (or regression) model can lead to aberrations. It is for instance the case when the imputed values falls outside the scale boundaries. To avoid such situations, `{simputation}` also provides other more advanced alternatives including (amongst others) `impute_rf()` which uses random forest to impute the missing values.

Last but not least, imputation of missing values could also be done in a multivariate way, by using the structure of the data (e.g. correlation) to predict the missing values. This is the approach proposed in the `{missMDA}` package. Since our data are numeric, the imputation is done through PCA with the `imputePCA()` function. Note that here, the imputed values are stored in the object `.$completeObs` (here, `sensory` is used):

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(missMDA)

imputePCA(sensory, quali.sup=1:4, method="EM")$completeObs %>% 
  dplyr::select(Judge, Product, Sour, Light)
```

In this case, it can be seen that the missing value for `J01`*x*`P04`*x*`Light` has been replaced by the value `33.7`.

#### Limitations

As we have seen, there are different ways to implement missing values, and the different algorithms will likely impute them with different values. Hence, the overall results can be affected, and there is no way to know which solution is the most suitable for our study. Still, it is recommended to treat the missing values, and to chose the right strategy that is the most adapted to the data. 

However, since most imputation methods involve modeling, applying them to variables with a high missing values rate can introduce bias in the data. Let's consider a situation in which assessors are evaluating half the product set using a BIB. Hence, half of the data are missing. By imputing the missing values, each prediction is proportionally based on one unique value. And ultimately, any further analyses on this data would be based on half measured and half *fictive* data.

### Design Inspection

The next point of interest - quite specific to sensory and consumer data - is to ensure that the design is well balanced, and handles correctly the first-order and carry-over effects. This step is particularly important for those who analyze the data but were not involved from the start in that study (and hence were not involved during the test set-up).

Let's show a simple procedure that would check part of the quality of a design. Since our data set stored in *biscuits_sensory_profile.xlsx* does not contain any information regarding the experimental design, let's use `sensochoc` from `{SensoMineR}` instead. 

To load (and clean) the data, let's run these lines of code:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(SensoMineR)

data(chocolates)

dataset <- sensochoc %>% 
  as_tibble() %>% 
  mutate(across(c(Panelist, Session, Rank, Product), as.character))
```

The data consist of 6 products (`Product`) evaluated by 29 panelists (`Panelist`) in duplicates (`Session`). The presentation order is stored in `Rank`. 

To evaluate whether the products have been equally presented at each position, a simple cross-count between `Product` and `Rank` is done. This can be done using the `xtabs()` function:

```{r, eval=TRUE, echo=TRUE}
xtabs(~Product + Rank, data=dataset)
```

Such table can also be obtained using `group_by()` and `count()` to get the results in a tibble:

```{r, echo=TRUE, eval=FALSE}
dataset %>% 
  group_by(Product) %>% 
  count(Rank) %>% 
  ungroup() %>% 
  pivot_wider(names_from=Rank, values_from=n)
```

As we can see, the design is not perfectly balanced, as `choc2` is evaluated 11 times in the 1st, 4th, and 6th position, but only 7 times in the 5th position.

To make sure that the design is well balanced in terms of carry-over effect, we need to count how often each product is tested before each of the other products. Since this information is not directly available in the data, it needs to be added. 

Let's start with extracting the information available, i.e. the order of each product for each panelist and session:

```{r, echo=TRUE, eval=TRUE}
current <- dataset %>% 
  dplyr::select(Panelist, Product, Session, Rank) %>% 
  mutate(Rank = as.numeric(Rank))

```

An easy way to add the `Previous` product information as a new column in the data is by replacing `Rank` by `Rank + 1` in `current` (all new positions larger than the number of products are filtered). 

```{r, eval=TRUE, echo=TRUE}
previous <- current %>% 
  rename(Previous = Product) %>% 
  mutate(Rank = Rank + 1) %>% 
  filter(Rank <= length(unique(dataset$Product)))

```

This new data is merged to `current` by `Panelist`, `Session`, and `Rank`:

```{r, eval=TRUE, echo=TRUE}
cur_prev <- current %>% 
  left_join(previous, by=c("Panelist", "Session", "Rank"))
```

As can be seen, the products that are evaluated first get `NA` in `Previous`, and for each rank r (r > 1), `Previous` gets the product that was evaluated at rank r-1.

To evaluate whether the carry-over effect is well balance, the only thing left to do is cross-count `Product` and `Previous` (here, the results are split per `Session`):

```{r, eval=TRUE, echo=TRUE}
cur_prev %>% 
  group_by(Session, Product, Previous) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(Product = factor(Product, levels=paste0("choc",1:6)),
         Previous = factor(Previous, levels=c("NA",paste0("choc",1:6)))) %>%
  arrange(Previous) %>% 
  pivot_wider(names_from=Previous, values_from=n, values_fill=0) %>% 
  arrange(Product) %>% 
  split(.$Session)

```

As expected, the table shows that a product is never evaluated twice in a row (the diagonal contains 0s). Here again, the design is not optimal since `choc1` has been evaluated 3 times before `choc2` and 6 times before `choc5` in the first session.

>The last column defined as `NA` refers to the number of time that products did not have a product tested before, in other words that they were evaluated first.

## Clean {#clean}

As mentioned in the introduction of this chapter, there is a thin line between *Data Inspection* and *Data Manipulation*, as both steps share many common practices. Here, we are limiting ourselves on handling variables and their type. For a full overview, we encourage the readers to look at Section \@ref(data-manip) to see other practices on how to handle data.

### Handling Data Type

The data used in this section is stored in *bisuits_traits.xlsx*. So let's start with importing it in R:

```{r, echo=TRUE, eval=TRUE}
file_path <- here("Data", "biscuits_traits.xlsx")

demo_var <- read_xlsx(file_path, sheet="Variables") %>% 
  dplyr::select(Code, Name)

demo_lev <- read_xlsx(file_path, sheet="Levels") %>% 
  dplyr::select(Question, Code, Levels) %>% 
  inner_join(demo_var, by=c("Question"="Code")) %>% 
  dplyr::select(-Question)

demographic <- read_xlsx(file_path, sheet="Data", skip=1, col_names=unlist(demo_var$Name))

```

In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another.

Remember that when your data set is stored in a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables' type can be accessed at any moment. In case the data is not in a tibble, the use of the `str()` function becomes handy as it provides this information (here we limit ourselves to the first 5 columns).

```{r, echo=TRUE, eval=TRUE}
str(demographic[,1:5])
```

In sensory and consumer research, the four most common types are:

* Numerical (incl. integer [`int`], decimal [`dcl`],  and double [`dbl`]);
* Logical [`lgl`];
* Character [`char`];
* Factor [`fct`].

R still has plenty of other types, for more information please visit: https://tibble.tidyverse.org/articles/types.html

#### Numerical Data

Since a large proportion of the research done is quantitative, it is no surprise that our data are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g. 1, 2, -16, etc.), or decimal value (or double, e.g. 1.6, 2.333333, -3.2 etc.).

By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double.

#### Binary Data

Another common type that seems to be numerical in appearance, but that has additional properties is the binary type. 
Binary data is data that takes two possible values (`TRUE` or `FALSE`), and are often the results of a *test* (e.g. is `x>3`? Or is `MyVar` numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires.

>Intrinsically, binary data is *numerical*, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the `sum()` function, as shown in the simple example below:
```{r example_logical, echo=TRUE, eval=FALSE}
# Generate 10 random values between 1 and 10 using the uniform distribution
x <- runif(10, 1, 10)
x
# Test whether the values generated are strictly larger than 5
test <- x>5
test
# Counting the number of values strictly larger than 5
sum(test)
```

#### Nominal Data

Nominal data is any data that are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes. But since the software cannot guess that those numbers are *identifiers* rather than *numbers*, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables is explained in the next section. 

For nominal data, two particular types of data are of interest: 

* Character or `char`;
* Factor or `fct`.
  
Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels: 

* For `character`, there are no particular structure, and the variables can take any values (e.g. open-ended question);
* For `factor`, the inputs of the variables are structured into `levels`.
 
To evaluate the number of levels, different procedures are required:

* For `character`, one should count the number of unique element using `length()` and `unique()`;
* For `factor`, the levels and the number of levels are directly provided by `levels()` and `nlevels()`.
 
Let's compare a variable set as `factor` and `character` by using a simple hand-made example:

```{r char_vs_fctr1, eval=TRUE, echo=TRUE}
example <- demographic %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge))

summary(example)

unique(example$Judge)
length(unique(example$Judge))

levels(example$Judge_fct)
nlevels(example$Judge_fct)
```

>Although `Judge` and `Judge_fct` look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or for building tables and graphs.

When set as `character`, the number of levels of a variable is directly read from the data, and its levels' order matches the way they appear in the data (or sometimes are re-arranged in alphabetical order). This means that any data collected using a structured scale will often lose its natural order. 

When set as `factor`, the factor levels (including their order) are informed, and does not depend necessarily on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. In our case, the levels are read from the data and are reordered alphabetically (note that `J10` and `J100` appear before `J2` for instance.)

To illustrate this, let's re-arrange the levels from `Judge_fct` by ordering them numerically in such a way `J2` follows `J1` rather than `J10`.

```{r sorting_judge, echo=TRUE, eval=TRUE}
example <- demographic %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = factor(Judge, str_sort(Judge, numeric=TRUE)))
levels(example$Judge_fct)

```

Now the levels are sorted, let's filter respondents by only keeping J1 to J20. We then re-run the previous code that count the number of elements in each variables:

```{r char_vs_fctr2, eval=TRUE, echo=TRUE}
example_reduced <- example %>%  
  filter(Judge %in% paste0("J",1:20))

unique(example_reduced$Judge)
length(unique(example_reduced$Judge))

levels(example_reduced$Judge_fct)
nlevels(example_reduced$Judge_fct)
```

After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements (`J18` doesn't exist in the data), whereas the column set as factor still contains the 107 entries (most of them not having any recordings). 

```{r, echo=TRUE, eval=TRUE}
example_reduced %>% 
  count(Judge, .drop=FALSE)

example_reduced %>% 
  count(Judge_fct, .drop=FALSE)
```

This property can be seen as an advantage or a disadvantage depending on the situation:

* For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of `factor`).
* For hypothesis testing (e.g. ANOVA) on subset of data, the `Judge` variable set as `character` would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would still use the original count (so 106 here)!

The latter point is particularly critical since the analysis is incorrect and will either return an error or (worse!) return erroneous results!

Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily. 
Let's consider the `Living area` variable from `demographic` as an example. From the original excel file, it can be seen that it has three levels, `1` corresponding to *urban area*, `2` to *rurban area*, and `3` to *rural area*. Let's start by renaming its levels:

```{r area_recode1, echo=TRUE, eval=TRUE}
example = demographic %>% 
  mutate(Area = factor(`Living area`, levels=c(1,2,3), labels=c("Urban", "Rurban", "Rural")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```

As can be seen, the variable `Area` is the factor version (including its labels) of `Living area`.
Let's now regroup `Rurban` and `Rural` together under `Rural`, and change the order by ensure that `Rural` appears before `Urban`:

```{r area_recode2, echo=TRUE, eval=TRUE}
example = demographic %>% 
  mutate(Area = factor(`Living area`, levels=c(2,3,1), labels=c("Rural", "Rural", "Urban")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```

This approach of renaming and re-ordering factor levels is very important as it can simplify the readability of tables and figures.
Some other transformations can be applied to factors thanks to the `{forcats}` package. Particular attention should be given to the following functions:

* `fct_reorder()`/`fct_reorder2()` and `fct_relevel()` reorder the levels of a factor;
* `fct_recode()` renames the factor levels (as an alternative to `factor()` used in the previous example);
* `fct_collapse()` and `fct_lump()` aggregate different levels together (`fct_lump()` regroups automatically all the rare levels);
* `fct_inorder()` uses the order read in the data (particularly useful with `pivot_longer()` for instance);
* `fct_rev()` reverses the order of the levels (particularly useful in graphs).

Although it hasn't been done here, manipulating strings is also possible through the `{stringr}` package, which provides interesting functions such as:

* `str_to_upper()`/`str_to_lower()` to convert strings to uppercase or lowercase;
* `str_c()`, `str_sub()` combine or subset strings;
* `str_trim()` and `str_squish()` remove white spaces;
* `str_extract()`, `str_replace()`, `str_split()` extract, replace, or split strings or part of the strings;
* `str_sort()` to order alphabetically (or by respecting numbers, as shown previously) its elements.

Many of these functions will be used later in Section \@ref(text-analysis)
 
### Converting between Types

Since each variable type has its own properties, it is important to be able to switch from one to another if needed. This can be critical (converting from numerical to character or factor and reversely) or purely practical (converting from character to factor and reversely). 

In the previous section, we have already seen how to convert from character to factor. Let's now consider two other conversions, namely:

* from numerical to character/factor;
* from character/factor to numerical.
 
The conversion from numerical to character or factor is simply done using `as.character()` and `as.factor()` respectively. An example in the use of `as.character()` and `as.factor()` was provided in the previous section when we converted the `Respondent` variables to character and factor. The use of `factor()` was also used earlier when the variable `Living area` was converted from numerical to factor (called `Area`) with labels.

>`as.factor()` only converts into factors without allowing to chose the order of the levels, nor to rename them. Instead, `factor()` should be used as it allows specifying the `levels` (and hence the order of the levels) and their corresponding `labels`. 

To illustrate the conversion from character to numeric, let's start with creating a tibble with two variables, one containing strings made of numbers, and one containing strings made of text.

```{r simple_example, eval=TRUE, echo=TRUE}
example <- tibble(Numbers = c("2","4","9","6","8","12","10"),
                  Text = c("Data","Science","4","Sensory","and","Consumer","Research"))
```

The conversion from character to numerical is straight forward and requires the use of the function `as.numeric()`:

```{r char_2_num, echo=TRUE, eval=TRUE}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```

As can be seen, when strings are made of numbers, the conversion works fine. However, any non-numerical string character cannot be converted and hence returns NAs.

Now let's apply the same principle to a variable of the type factor. To do so, the same example in which the variables are now defined as factor is used:

```{r char_2_fctr, echo=TRUE, eval=TRUE}
example <- example %>% 
  mutate(Numbers = as.factor(Numbers)) %>% 
  mutate(Text = factor(Text, levels=c("Data","Science","4","Sensory","and","Consumer","Research")))
```

Let's apply as.numeric() to these variables:

```{r fctr_2_num, echo=TRUE, eval=TRUE}
example %>% 
  mutate(NumbersN = as.numeric(Numbers), TextN = as.numeric(Text))
```

We can notice here that the outcome is not really as expected as the numbers 2-4-9-6-8-12-10 becomes 3-4-7-5-6-2-1, and Data-Science-4-Sensory-and-Consumer-Research becomes 1-2-3-4-5-6-7. The rationale behind this conversion is that the numbers do not reflects the string itself, but the position of that level in the factor level order.

To convert properly numerical factor levels to number, the variable should first be converted into character:

```{r fctr_2_char_2_num, echo=TRUE, eval=TRUE}
example %>%
  mutate(Numbers = as.numeric(as.character(Numbers)))
```

As can be seen, it is very important to verify the type of each variable (and convert if needed) to ensure that the data is processed as it should be. Since each type has its own advantages and drawbacks, it is convenient to regularly transit from one to another. Don't worry, you will get quickly familiarized with this as we will be doing such conversions regularly in the next sections.

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```


# Data Manipulation {#data-manip}


In sensory science, different data collection tools (e.g. different devices, software, methodologies, etc.) may provide the same data in different ways. Also, different statistical analyses may require having the data structured in different formats. 

A simple example to illustrate this later point is the analysis of liking data.
Let C consumers provide their hedonic assessments of P samples. To evaluate if samples have been liked differently, an ANOVA is performed on a long thin table with (PxC) rows x 3 columns (consumer, sample, and the liking scores). 


<!-- Insert Figure illustrating the data structure -->


However, to assess whether consumers have the same preference patterns at the individual level, internal preference mapping or cluster analysis would be performed, and both these analyses require as input a short and large table with P rows and C columns. 

<!-- Insert Figure illustrating the data structure -->


Another example of data manipulation consists in summarizing data, by for instance computing the mean by product for each sensory attribute (hence creating the so-called sensory profiles), or to generate frequency tables (e.g. proportions of male/female, distribution of the liking scores by sample, contingency table for CATA data, etc.)


<!-- Insert Figure illustrating the data structure -->


For these reasons, it is hence essential to learn to manipulate data and transition from one structure to another. After presenting many different ways to transform your data, application through simple examples will be presented[^1].

[^1]: Most of the examples presented in this chapter have no scientific meaning. This has been done on purpose to emphasize the "how to?", not the "why?"


## Tidying Data


Hadley Wickham defined 'tidy data' as "data sets that are arranged such that each variable is a column and each observation (or case) is a row." Depending on the statistical unit to consider, and the analyses to perform, data hence need to be manipulated. 


### Simple Manipulations


The notion of 'simple manipulations' is completely arbitrary and consists in simple data transformation that could easily be performed in other software such as Excel (although we strongly recommend performing any sorts of transformation in R, not in Excel, see...)


#### Handling Columns


##### Renaming Variables


The first simple transformation that one could consider consists in renaming one or multiple variables.
This procedure can easily be done using the `rename()` function from the `{dplyr}` package.

In our sensory file, let's recode 'Judge' into 'Panellist', and 'Product' into 'Sample' (here, we apply transformations without saving the results, so without altering the original dataset):


```{r}
sensory %>% 
  rename(Panellist = Judge, Sample = Product)
```

If this procedure of renaming variables should be applied on many variables following a structured form (e.g. transforming names into snake_case, camelCase, etc.), the use of the `{janitor}` package comes handy thanks to its `clean_names()` function:


```{r}
library(janitor)
sensory %>% 
  clean_names()
```


Note that the `{janitor}` package offers many options, and although the transformation was performed to all the variables, it is possible to ignore certain variables for the transformation.


##### Re-Organizing Columns


Another simple transformation consists in re-organizing the dataset, either by re-ordering (incl. removing) the columns, or by selecting some rows based on a certain criteria. 

For re-ordering columns, `relocate()` is being used. This function allows re-positioning a variable before or after another variable. By re-using the `sensory` dataset, let's position `Code` between `Product` and `Shiny`. This can be specified into two different ways:


```{r}
sensory %>% 
  relocate(Code, .after=Product)

sensory %>% 
  relocate(Code, .before=Shiny)
```


Last but not least (regarding columns transformation) the `select()` function (from the `{dplyr}` package[^2]) allows selecting a set of variables, by simply informing the variables that should be kept in the dataset. Let's limit ourselves in selecting `Judge`, `Product`, and `Shiny`:

[^2]: Many other packages include a function called `select()`, hence creating conflicts: To avoid any risks of errors, we recommend calling the `select()` function using `dplyr::select()` as it formally calls the `select()` function from `{dplyr}`, hence avoiding any risks of error! Of course, the same procedure applies to any other functions that may suffer the same issue.


```{r}
sensory %>% 
  dplyr::select(Judge, Product, Shiny)
```


When a long series of variables should be kept in the same order, the use of the `:` is used.
Here, we consider keeping the entire set of variables except for the variable `Code`:


```{r}
sensory %>% 
  dplyr::select(Judge, Product, Shiny:Sticky)
```


However, when only one (or few) variable needs to be removed, it is easier to specify which variable to remove rather than informing all the variables to keep. Such solution is then done using the `-` sign. The previous example can then be obtained using the following code:


```{r}
sensory %>% 
  dplyr::select(-Code)
```


The selection process of variables can be further informed through functions such as `starts_with()`, `ends_with()`, and `contains()`, which all select variables that either starts, ends, or contains a certain character or sequence of character. To illustrate this, let's only keep the variables that starts with 'Qtt':


```{r}
sensory %>% 
  dplyr::select(starts_with("Qtt"))
```


Rather than selecting variables based on their names, we can also select them based on a certain 'rule' with the `where()` function. In that case, let's consider all the variables that are numerical, which automatically removes `Judge`, `Code`, and `Product` columns:


```{r}
sensory %>% 
  dplyr::select(where(is.numeric))
```


**Remark**: `dplyr::select()` is a very powerful function that facilitates complex variables' selection through very intuitive functions. Ultimately, it can also be used to `relocate()` and even `rename()` variables, as shown in the example below:


```{r}
sensory %>% 
  dplyr::select(Panellist = Judge, Sample = Product, Code, Shiny:Sticky, -starts_with("Qtt"))
```


More examples illustrating the use of `select()` are provided throughout the book.


##### Creating Columns


In some cases, new variables need to be created based on existing variables. Examples of such situations include taking the quadratic term of a sensory attribute to test for curvature, or simply considering a new variables as the sum or the subtraction between two (or more) variables. Such creation of a variable is through the `mutate()` function from the `{dplyr}` package. This function takes as inputs the name of the variable to create, and the formula to consider.
Let's create two new variables, one called Shiny2 which corresponds to Shiny squared up, and one StiMelt which corresponds to Sticky + Melting. Since we will only be using these three variables, let's reduce the dataset to these three variables with `select()` first to improve readability:


```{r}
sensory %>% 
  dplyr::select(Shiny, Sticky, Melting) %>% 
  mutate(Shiny2 = Shiny^2, StiMelt = Sticky + Melting)
```


Tip: If you want to transform a variable, say by changing its type, or re-writing its content, you can use `mutate()` and assign to the new variable the same name as the original one. This will overwrite the existing column with the new one. To illustrate this, let's transform the sample codes (stored in `Code`) from upper case to lower case only. This can be done by mutating `Code` into the lowercase version of `Code` (`tolower(Code)`):


```{r}
sensory %>% 
  mutate(Code = tolower(Code))
```


`mutate()` being one of the most important function from the `{dplyr}` package, it will be used extensively throughout this book.

Since performing mathematical computation on non-numerical columns is not possible, conditions can easily be added through variants of the `mutate()` function such as `mutate_if()`, `mutate_at()`, or `mutate_all()`. However, we prefer the use of `mutate()` combined with `across()` to perform such operations.


##### Mergeing and Separating columns


It can happen that some columns of a data set contain information (strings) that cover different types of information. For instance, we could imagine coding the name of our panelists as FirstName_LastName or Gender_Name, and we would want to separate them into two columns to make the distinction between the different information, i.e. FirstName and LastName or Gender and Last Name respectively. 

In other situations, we may want to merge information present in multiple columns in one. In our `sensory` data set, we can see that the sample information is spread through two columns, one containing the code (`Code`) and one the names (`Product`). To start, let's merge these two columns so that both information is stored in one column (called here `NewName`) as *code-name*. 
To do so, we use the `unite()` function from the `{tidyr}` package, which takes as first element the name of the new variables, followed by all the columns to *unite*, and by providing the separation between the elements (here *-*):


```{r}
sensory2 <- sensory %>% 
  unite(NewName, Code, Product, sep="-")
sensory2

```


By default, `unite()` removes from the data set the individual variables that have been merged. To keep these original variables, the parameter `remove` should be set to `FALSE`. 

To reverse the changes (saved here in `sensory2`) and to separate a column into different variables, the function `separate()` from the `{tidyr}` package is required. Similarly to `unite()`, `separate()` takes as first parameter the name of the variable to split, followed by the names for the different segments generated, and of course the separated defined by `sep`. In our example, this would be done as following:


```{r}
sensory2 %>% 
  separate(NewName, c("Code","Product"), sep="-")
```


#### Handling Rows


The manipulation of rows is done through three aspects, 

1. by re-arranging the rows in a logical way, 
2. by filtering entries based on a given variables,
3. splitting the data in sub-groups based on the entries of a variable.


##### Re-arranging Rows


The first step of re-arranging rows is done through the `arrange()` function from the `{dplyr}` package. This function allows sorting the data in the ascending order[^3]. To arrange them in a descending order, the function `desc()` is required.

[^3]: For numerical order, this is simply re-arranging the values from the lowest to the highest. For strings, the entries are then sorted alphabetically unless the variable is of type factor in which case the order of the levels for that factors are being used.

Let's re-arrange the data by Judge and Product, the Judge being sorting in an ascending order whereas the product are being sorted in a descending order:


```{r}
sensory %>% 
  arrange(Judge, desc(Product))
```


##### Filtering Data


To define sub-set of data, the `filter()` function is being used. This function requires providing an argument that is expressed as a test, meaning that the outcome should either be TRUE (keep the value) or FALSE (discard the value). In R, this is expressed by the double '=' sign `==`. Let's filter the data to only keep the data related to sample `29PPK_p`:


```{r}
sensory %>% 
  filter(Product == "29PPK_p")
```


Other relevant test characters are the following:

 - `!Product == "29PPK_p"` or `Product != "29PPK_p"` means different from, and will keep all samples except `29PPK_p`;
 - `%in% my_vector` keeps any value included within the vector `my_vector` (e.g. `Product %in% c("12GP_f","12GP16PSL_m","12GP23P_m")`);
 - for multiple conditions: 
  - `&` (read 'and') is multiplicative, meaning that all the conditions need to be true (`Product == "12GP_f" & Shiny>40`);
  - `|` (read 'or') is additive, meaning that only one condition needs to be true (`Product == "12GP_f" | Shiny>40`)
  
As we will see later, this option is particularly useful when you have missing values as you could remove all the rows that contain missing values for a given variable. Since we do not have missing values here, let's create some by replacing all the evaluations for Shiny that are larger than 40 by missing values. In a second step, we can filter out all missing values from Shiny:


```{r}
sensory_na <- sensory %>% 
  dplyr::select(Judge, Product, Shiny) %>% 
  mutate(Shiny = ifelse(Shiny > 40, NA, Shiny))

sensory_na

sensory_na %>% 
  filter(!is.na(Shiny))

```


As we can see, this procedure removed 20 rows since the original table had 99 rows and 3 columns, whereas the 'clean' table has 79 rows and 3 columns.


##### Splitting Data


For researchers who want to split their data into logical subgroup (e.g. splitting a panel by gender, hence performing an analysis for the males and females separately), the `filter()` function is a good alternative. This is to some extent what we did earlier when we only filtered data from sample "29PPK_p". To get sub-groups of data for each sample, we could repeat the same procedure for all the other samples. However, this procedure becomes tedious as the number of samples increases. 
For such task, we prefer the use of the function `split()`, which takes as argument the column to split from:


```{r}
sensory %>% 
  split(.$Product)
```


This function creates a list of *n* elements (*n* being the number of samples here), each element being the data related to one sample. 
From there, automated analyses can be performed to each of the sub-data through the `map()` function, as it will be illustrated later. 


### Reshaping the Data


Reshaping the data itself is done through pivoting, hence either creating a longer and thinner table (CREATE FIGURE), or a shorter and wider table (CREATE FIGURE). This is done through the `pivot_longer()` and `pivot_wider()` functions from the `{tidyr}` package.


<!-- Insert Figure illustrating pivot_wider -->


<!-- Insert Figure illustrating pivot_longer -->


#### Pivotting Longer


Currently, our `sensory` data table is a table in which we have as many rows as Judge x Product, the different attributes being spread across multiple columns. However, in certain situations, it is relevant to have all the attributes stacked vertically, meaning that the table will have Judge x Product x Attributes rows. Such simple transformation can be done through the `pivot_longer()` function from the `{dplyr}` package, which takes as inputs the attributes to pivot, the name of the variables that will contain these names (`names_to`), and the name of the column that will contain their entries (`values_to`)


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Score")
```


This transformation converts a table of 99 rows and 35 columns into a table with 3168 (99*32) rows and 5 columns in this example.

TIPS: With `pivot_longer()` and any other function that requires selecting variables, it is often easier to deselect variables that we do not want to include rather than selecting all the variables of interest. Throughout the book, both solutions will be used.

In case the attribute names are following a standard structure, say "attribute_name modality" as is the case in `sensory` for some attributes, an additional parameter of `pivot_longer()` becomes handy as it splits the Attribute variable just created into say 'Attribute' and 'Modality.' To illustrate this, let's reduce sensory to Judge, Product, and all the variables that end with odor or flavor (for clarity, all the other variables are being discarded). After pivoting the subset of columns, we automatically split the attribute names into attribute and modality by informing the separator between names (here, a space):


```{r}
sensory %>% 
  dplyr::select(Judge, Product, ends_with("odor"), ends_with("flavor")) %>% 
  pivot_longer(-c(Judge,Product), names_to=c("Attribute","Modality"), values_to="Score", names_sep=" ")
```


This parameter combines both the power of `pivot_longer()` and `separate()` in one unique process. Note that more complex transformations through the use of regular expressions and `names_pattern` can be considered. More information on this topic is provided in REF CHAPTER TEXTUAL.


#### Pivotting Wider


The complementary/opposite function to `pivot_longer()` is called `pivot_wider()`. This function pivots data horizontally, hence reducing the number of rows and increasing the number of columns. In this case, the two main parameters to provide is which column will provide the new column names to create (`name_from`), and what are the corresponding values to use (`values_from`). 

From the previous example, we could set `names_from=Attribute` and `values_from=Score` to return to the original format of sensory. However, let's reduce the dataset to `Product`, `Judge`, and `Shiny` only, and let's pivot the `Judge` and `Shiny` columns:


```{r}
sensory %>% 
  dplyr::select(Judge, Product, Shiny) %>% 
  pivot_wider(names_from = Judge, values_from = Shiny)
```

 
This procedure creates a table with as many rows as there are products, and as many columns as there are panelists (+1 since the product information is in a column, not defined as row names).

These functions are particularly useful in consumer studies, since `pivot_longer()` and `pivot_wider()` allows restructuring the data for analysis such as ANOVA (`pivot_longer()` output) and preference mapping or clustering (`pivot_wider()` structure).

Important remarks: Let's imagine the sensory test was performed following an incomplete design, meaning that each panelist did not evaluate all the samples. Although the long and thin dataset would not show missing values (the entire rows being removed), the shorter and larger version would contain missing values for the products that each panelist did not evaluate. If the user wants to automatically replace these missign values with a fixed value, say, it is possible through the parameter `values_fill` (e.g. `values_fill=0` would replace each missing value with a 0). Additionally, after pivoting the data, if multiple entries exist for a combination row-column, `pivot_wider()` will return a list of elements. In the next Section, an example illustrating such situation and its solution will be presented.


### Transformation that Alters the Data


In some cases, the final table to generate requires altering the data, by (say) computing the mean across multiple values, or counting the number of occurrences of factor levels for instance. In other words, we summarize the information, which also tend to reduce the size of the table. It is hence no surprise that the function used for such data reduction is called `summarise()` (`{dplyr}` package).


#### Introduction to Summary Statistics


In practice, `summarise()` applies a function (whether it is the `mean()`, or a simple count using `n()`) on a set of values. Let's compute the mean on all numerical variables of `sensory`:


```{r}
sensory %>% 
  summarise(across(where(is.numeric), mean))
```


As can be seen, the grand mean is computed for each attribute.
If multiple functions should be applied, we could perform all the transformation simultaneously as following:


```{r}
sensory %>% 
  summarise(across(where(is.numeric), list(min=min, max=max)))
```


In this example, each attribute is duplicated with "_min" and "_max" to provide the minimum and maximum value for each attribute. By using a combination of `pivot_longer()` with `names_sep` and pivot_wider, we could easily restructure such table by showing for each attribute (presented in rows) the minimum and the maximum in two different columns.

By following the same principles, many other functions can be performed, whether they are built-in R or created by the user. 
Here is a recommendation of interesting descriptive functions to consider with `summarise()`:

 - `mean()`, `median()` (or more generally `quantile()`) for the mean and median (or any other quantile);
 - `sd()` and `var()` for the standard deviation and the variance;
 - `min()`, `max()`, `range()` (provides both the min and max) or `diff(range())` (for the difference between min and max); 
 - `n()` and `sum()` for the number of counts and the sum respectively. 


It can appear that the interest is not in the grand mean, say, but in mean per product, or per product and panelist in case the test has been duplicated. In such cases, the `summary()` should aggregate set of values per product, or per product x panelist respectively. Such information can be passed on through the `group_by()`. 


```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(across(where(is.numeric), mean))
```


This procedure creates a tibble with 11 rows (product) and 33 columns (32 sensory attributes + 1 column including the product information) which contains the mean per attribute for each sample, also known as the sensory profiles of the products.


#### Illustrations of Data Manipulation


Let's review the different transformations presented earlier by generating the sensory profiles of the samples through different approaches[^4].

[^4]: It is important to realize that each 'data manipulation challenge' can be solved in many different ways, so don't be afraid to think out of the box when solving them...

In the previous example, we've seen how to obtain the sensory profile using `summarise()` `across()` all numerical variables. In case a selection of the attributes should have been done, we could use the same process by simply informing which attributes to transform:


```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(across(Shiny:Melting, mean))
```


The list of attributes to include can also be stored in an external vector:


```{r}
sensory_attr <- colnames(sensory)[4:ncol(sensory)]
sensory %>% 
  group_by(Product) %>% 
  summarise(across(all_of(sensory_attr), mean))

```


Remark: It is important to notice that when `group_by()` is being called, the software will remember the groups unless stated otherwise. This means that any subsequent transformation performed on the previous table will be done by product. Such property can be causing unexpected results in case transformations should be performed across all samples. To avoid such behavior, we strongly recommend you to apply `ungroup()` as soon as the results per group has been generated.

A different approach consists in combining `summarise()` to `pivot_longer()` and `pivot_wider()`. This process requires summarising only one column by Product and Attribute:


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  group_by(Product, Attribute) %>% 
  summarise(Scores = mean(Scores)) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores) %>% 
  ungroup()
```


One can notice that through this procedure, the order of the attributes are no longer following the same sequence, and have been ordered in alphabetical order. To maintain the original order, the Attribute column should be transformed into a factor in which the levels are ordered following the original order.

What would happen if we would omit to `summarise()` the data in between the two pivoting functions? In that case, we also remove Judge and Code which were lost in the process...


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  dplyr::select(-c(Judge, Code)) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores)
```


As can be seen, each cell contains `dbl [9]` corresponding to the scores provided by the 9 panelists to that product and that attribute. Since we would ultimately want the mean of these 9 values to generate the sensory profiles, a solution comes directly from `pivot_wider()` through the parameter `values_fn` which applies the function provided here on each set of values:


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  dplyr::select(-c(Judge, Code)) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores, values_fn=mean)
```


### Combining Data from Different Sources


It often happens that the data to analyze is stored in different files, and need to be combined or merged. Depending on the situations, different solutions are required.

Let's start with a simple example where the tables match in terms of variables, and should be combined vertically. 
To do so, we use the file *excel-scrap.xlsx* which contains a fake example in which 12 assessors evaluated 2 samples on 3 attributes in triplicate, each replication being stored in a different sheet.

To combine the tables vertically, we could use the basic R function `rbind()`. However, we prefer the use of `bind_rows()` from the `{dplyr}` package since it better controls for the columns by ensuring that the order is well respected (in case one table contains a variable that the other tables do not, it will keep the variables and allocate NAs when this information is missing). To keep the distinction between the three tables, the parameter `.id` is used. This will create a column called `Session` in this example that will assign a 1 to the first table, a 2 to the second one, and a 3 to the third one (we do this here since this information was not available within the tables: If it were, the parameter `.id` could have been ignored).


```{r}
library(here)
library(readxl)

path <- file.path("data", "excel_scrap.xlsx")

session1 <- read_xlsx(path, sheet=1)
session2 <- read_xlsx(path, sheet=2) 
session3 <- read_xlsx(path, sheet=3)

all_data <- bind_rows(session1, session2, session3, .id = "Session")

```


Although this solution works fine, another neater and tidier solution will be presented in \@ref(import-mult-sheet).

Similarly, tables can be combined horizontally using the corresponding function `cbind()` (`{base}`) and/or `bind_cols()` (`{dplyr}`). In this case, it is better to ensure that the rows' order is identical before combining them to ensure no mishaps.

Alternatively it is possible to merge tables using `merge()` from `{base}`, or the different `*_join()` functions from 
the `{dplyr}` package. In that case, the tables do not need to be in the same order, nor from the same size, since the function will handle it.




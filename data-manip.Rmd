---
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Data Manipulation {#data-manip}

<!-- 
MENTIONING SOMEWHERE THAT SOME OF THE TRANSFORMATIONS MADE ARE TEMPORARY AS THEY ARE NOT BEING SAVED, SO RE-USING THE SAME DATA WILL STILL BE BACK TO ITS ORIGINAL VERSION...
WE ARE ALSO MISSING A SECTION EXPLAINING PIPES...AND ADDING THAT INTO APERITIFS MIGHT BE CONFUSING AS APERITIFS IS AT THE END OF THE BOOK...SO SHOULD WE EXPLAIN IT IN THE INTRODUCTION? 

TW: not completely sure where we are now with the re-organization of the book.
I've added a sentence to explain here that readers need to look at the pre-requisite (how to read this book)
Also, the section "pre-requisite" (or could be called "summary") is something I think John suggested which provides information about what are we going to use (which data set), which main packages, and what are the learning points. I think this is a good thing to have for readers to quickly see what's in there. But for printing, maybe this should be written in a box, or in italic, to separate it from the rest.
-->

## Chapter Overview

In this section, you will learn how to manipulate and transform your data. This goes from very simple transformation (e.g. renaming columns, sorting tables, relocating variables, etc.) to more complex work (transposing parts of the table, combining/merging tables, etc.). 

Such transformations are done through simple examples, including the computation of the sensory profiles (mean scores per product and attribute) of products by following different transformations path. The data set used include *Sensory Profile.xlsx* mainly, as well as made up data set (*Consumer Test.xlsx*) to illustrate how to join different data set.

Most of the functions we use are accessible through the `{tidyverse}`. For importing the data, the usual `{here}` and `{readxl}` are used.

```{r}
library(tidyverse)
library(here)
library(readxl)
```

Note that in many cases, we are not saving the results of the transformation that we do. If you want to apply these changes to your data set, please visit section (APERITIF).
<!-- Vanessa: Would the section be "Getting Started"? Is there a way to write as hyperlink? --> 

## Why Manipulating Data?

In sensory science, different data collection tools (e.g. different devices, software, methodologies, etc.) may provide the same data in different ways. Also, different statistical analyses may require having the data structured in different formats. 

A simple example to illustrate this latter point is the analysis of liking data.
Let C consumers provide their hedonic assessments on P samples. To evaluate if samples have received different liking means at the population level, an ANOVA is performed on a long thin table with 3 columns (consumer, sample, and the liking scores), where the combination of CxP is spread in rows.

<!-- Insert Figure illustrating the data structure -->

However, to assess whether consumers have the same preference patterns at the individual level, internal preference mapping or cluster analysis is performed, both these analyses requiring as input a short and large table with P rows and C columns. 

<!-- Insert Figure illustrating the data structure -->

Another example of data manipulation consists in summarizing data, by for instance computing the mean by product for each sensory attribute (hence creating the so-called sensory profiles), or to generate frequency tables (e.g. proportions of male/female, distribution of the liking scores by sample, contingency table for CATA data, etc.)

<!-- Insert Figure illustrating the data structure -->

For these reasons, it is essential to learn to manipulate data and transition from one structure to another. 

Different ways to transform your data are illustrated through several simple examples^[Note that some of the examples presented in this chapter emphasize the "how to?," not the "why?," and are not necessarily chosen to convey scientific meaning].

## Tidying Data {#tidy-data}

Hadley Wickham (@Wickham2014) defined 'tidy data' as "data sets that are arranged such that each variable is a column and each observation (or case) is a row." Depending on the statistical unit to consider and the analyses to perform, data may need to be manipulated to be presented in a tidy form. 

### Simple Manipulations

The notion of 'simple manipulations' proposed here consists in data transformations that could easily be performed in other software such as Excel (using copy-paste, sorting and filtering, creating a pivot table, etc.). However, we strongly recommend performing any sorts of transformation in R as this will reduce the risk of errors, typically be faster, more reliable, and will be reusable if you need to perform the same operations on similar data in the future (including updated versions of the current data set). Moreover, these operations will become easier and more natural for you to use as you familiarize yourself with them.

#### Handling Columns

##### Renaming Variables

The first simple transformation we consider consists of renaming one or multiple variables. This procedure can easily be done using the `rename()` function from the `{dplyr}` package.  In each of the examples below, we use the `names()` function to show just the names of the resulting data set.

In our sensory file^[The *sensory* data are imported from the *Sensory Profile.xlsx* file, see Section \@ref(data-import) for more on how to import data sets], let's rename 'Judge' into 'Panellist', and 'Product' into 'Sample' (here we apply transformations without saving the results, so the original data set remains unchanged).

<!-- Vanessa: Maybe we can read and define sensory data in here and only mention chapter 8 for further details on data import? This will avoid the user to have to stop and go check in another chapter how sensory was imported/defined -->

First, here are the original names:

```{r}
sensory %>% 
  names()
```

To do so, we indicate in `rename()` that `newname` is replacing `oldname` as following `rename(newname = oldname)`. Additionally, we can apply multiple changes by simply separating them with a `,`:

```{r}
sensory %>% 
  rename(Panellist = Judge, Sample = Product) %>% 
  names()
```

If this procedure of renaming variables should be applied on many variables following a structured form (e.g. transforming names into snake_case, CamelCase, etc.) (see [https://en.wikipedia.org/wiki/Letter_case#Use_within_programming_languages](https://en.wikipedia.org/wiki/Letter_case#Use_within_programming_languages) for more information), the use of the `{janitor}` package comes handy thanks to its `clean_names()` function and the `case` parameter:

```{r}
library(janitor)
sensory %>% 
  clean_names(case="snake") %>% 
  names()
```

Note that the `{janitor}` package offers many options, and although the transformation is performed here on all the variables, it is possible to apply it on certain variables only.

##### Re-Organizing Columns

Another simple transformation consists in re-organizing the data set, either by re-ordering (including removing) columns, or by selecting some rows based on a certain criteria. 

For re-ordering columns, `relocate()` is being used. This function allows re-positioning a (set of) variable(s) before or after another variable. By re-using the `sensory` data set, let's position all the variables starting with 'Qty' between `Product` and `Shiny`. This can be specified into two different ways:

```{r}
sensory %>% 
  relocate(starts_with("Qty"), .after=Product) %>% 
  names()

sensory %>% 
  relocate(starts_with("Qty"), .before=Shiny) %>% 
  names()
```

Another very important function regarding columns transformation is the `select()` function (from the `{dplyr}` package^[Note that many other packages include a function called `select()`, which could create conflicts. To avoid any risks of errors, we recommend calling it as `dplyr::select()` to formally call `select()` from `{dplyr}`. This avoids any risks of error! Of course, the same procedure applies to any other functions that may suffer from the same issue.]) allows selecting a set of variables, by simply informing the variables that should be kept in the data. Let's limit ourselves in selecting `Judge`, `Product`, and `Shiny`:

```{r}
sensory %>% 
  dplyr::select(Judge, Product, Shiny)
```

When a long series of variables should be kept in the same order, the use of the `:` is used.
Let's only keep the variables related to Flavor, hence going from `Cereal flavor` to `Warming`:

```{r}
sensory %>% 
  dplyr::select(Judge, Product, `Cereal flavor`:Warming)
```

<!-- why does 'Cereal flavor' has the '' and not Warming? Is it because of the blank between Cereal and flavor? If so, it might be worth adding a note about that. 

TW: added in the aperitif
-->

However, when only one (or few) variable needs to be removed, it is easier to specify which one to remove rather than informing all the ones to keep. Such solution is then done using the `-` sign. The previous example can then be obtained using the following code:

```{r}
sensory %>% 
  dplyr::select(-c(Shiny, Melting))
```

The selection process of variables can be further informed through functions such as `starts_with()`, `ends_with()`, and `contains()`, which all select variables that either starts, ends, or contains a certain character or sequence of character. To illustrate this, let's only keep the variables that starts with 'Qty':

```{r}
sensory %>% 
  dplyr::select(starts_with("Qty"))
```

Rather than selecting variables based on their names, we can also select them based on their position (e.g. `dplyr::select(2:5)` to keep the variables that are at position 2 to 5), or following a certain 'rule' using the `where()` function. In that case, let's consider all the variables that are numerical, which automatically removes `Judge` and `Product`:

```{r}
sensory %>% 
  dplyr::select(where(is.numeric))
```

**Remark**: `dplyr::select()` is a very powerful function that facilitates selection of complex variables through very intuitive functions. Ultimately, it can also be used to `relocate()` and even `rename()` variables, as shown in the example below:

```{r}
sensory %>% 
  dplyr::select(Panellist = Judge, Sample = Product, Shiny:Sticky, -starts_with("Qty"))
```

More examples illustrating the use of `dplyr::select()` are provided throughout the book.

##### Creating Columns

In some cases, new variables need to be created from existing ones. Examples of such situations include taking the quadratic term of a sensory attribute to test for curvature, or simply considering a new variables as the sum or the subtraction between two (or more). Such creation of a variable is processed through the `mutate()` function from the `{dplyr}` package. This function takes as inputs the name of the variable to create, and the *formula* to consider.
Let's create two new variables, one called `Shiny2` which corresponds to `Shiny` squared up, and one `StiMelt` which corresponds to `Sticky` + `Melting.` Since we only use these three variables, let's first reduce the data set to these three variables with `select()` to improve readability:

```{r}
sensory %>% 
  dplyr::select(Shiny, Sticky, Melting) %>% 
  mutate(Shiny2 = Shiny^2, 
         StiMelt = Sticky + Melting)
```

Tip: If you want to transform a variable, say by changing its type, or re-writing its content, you can use `mutate()` and assign to the new variable the same name as the original one. This will overwrite the existing column with the new one. To illustrate this, let's transform `Product` from upper case to lower case only. This can be done by mutating `Product` into the lowercase version of `Product` (`tolower(Product)`):

```{r}
sensory %>% 
  mutate(Product = tolower(Product))
```

`mutate()` being one of the most important function from the `{dplyr}` package, more examples of its use are presented throughout this book.

Since performing mathematical computations on non-numerical columns is not possible, conditions can easily be added through `mutate()` combined with `across()`. Let's imagine we want to round all variables to 0 decimal, which can only be applied to numerical variables. 
To do so, we `mutate()` `across()` all variables that are considered `as.numeric` (through `where()`):

```{r}
# round(sensory, 0) returns an error because Judge and Product are characters

sensory %>% 
  mutate(across(where(is.numeric), round, 0))
```

In case only a selection of numerical variables should be rounded, we could also replace `where(is.numeric)` by a vector (using `c()`) with the names of the variables to round.

##### Merging and Separating columns

It can happen that some columns of a data set contain information (strings) that cover different types of information. For instance, we could imagine coding the name of our panelists as *FirstName_LastName* or *Gender_Name*, and we would want to separate them into two columns to make the distinction between the different information, i.e. *FirstName* and *LastName* or *Gender* and *LastName* respectively. In other situations, we may want to merge information present in multiple columns in one. 

For illustration, let's consider the information stored in the *Product Info* sheet from *Sensory Profile.xlsx*. This table includes information regarding the cookies, and more precisely their Protein and Fiber content (Low or High). 

After importing the data, let's merge these two columns so that both information is stored in one column called `ProtFib`. 
To do so, we use the `unite()` function from the `{tidyr}` package, which takes as first element the name of the new variables, followed by all the columns to *unite*, and by providing the separation between these elements (here *-*):

```{r}
file_path <- here("data","Sensory Profile.xlsx") 
prodinfo <- read_xlsx(file_path, sheet="Product Info") %>%  
  unite(ProtFib, Protein, Fiber, sep="-")
prodinfo
```

By default, `unite()` removes from the data set the individual variables that have been merged. To keep these original variables, the parameter `remove = FALSE` can be used. 

Although it is not relevant for combining columns, it is interesting to mention an additional package that can be used to combine elements together. This package is called `{glue}` and provides interesting alternatives to the usual `paste()` and `paste0()` functions. 

To reverse the changes (saved here in `prodinfo`) and to separate a column into different variables, the function `separate()` from the `{tidyr}` package is used. Similarly to `unite()`, `separate()` takes as first parameter the name of the variable to split, followed by the names for the different segments generated, and of course the separator defined by `sep`. 

In our example, this would be done as following:

```{r}
prodinfo %>% 
  separate(ProtFib, c("Protein","Fiber"), sep="-")
```

##### Conditions

In some cases, the new column to create depend directly on the value(s) of one or more columns present in the data. 
An examples of such situations consists in categorizing a continuous variable into groups by converting the age (in year) of the participants into age range. For such simple examples, some pre-existing functions (e.g. `cut()` in this situation) can be used.
However, in other situations, pre-defined functions do not exist and the transformation should be done *manually* using conditions.

Let's illustrate this by converting the `Shiny` variable (from `sensory`) from numeric to classes.
Since the scale used is a 60pt scale, let's start by creating a class called *Low* if the score is lower than 30, and *High* otherwise. 
To do so, pre-defined functions such as `cut()` are not being used intentionally as a manual transformation is preferred. Instead, `mutate()` is associated to `ifelse()`, which works as following: `ifelse(condition, results if condition is TRUE , results if condition is not TRUE)`

```{r}
sensory %>% 
  dplyr::select(Shiny) %>% 
  mutate(ShinyGroup = ifelse(Shiny < 30, "Low", "High"))
```

Let's imagine the same variable should now be split into three levels: *Low*, *Medium*, and *High*. Such additional group could be obtained by adding an `ifelse()` condition within the existing `ifelse()` condition:

```{r}
sensory %>% 
  dplyr::select(Shiny) %>% 
  mutate(ShinyGroup = ifelse(Shiny < 20, "Low", 
                             ifelse(Shiny < 40, "Medium", "High")))
```

Since there are only 3 conditions in total here, only two entangled `ifelse()` are required. This makes the code still manageable. But let's imagine more complex situations in which say 10 different conditions are required. Such solution would quickly become tedious to read, to track, and to debug if errors are being made. So instead, for more complex solutions, the use of an alternative function called `case_when()` is preferred. In the previous case, the same conditions would be written as follow:

```{r}
sensory %>% 
  dplyr::select(Shiny) %>% 
  mutate(ShinyGroup = case_when(Shiny < 20 ~ "Low",
                                between(Shiny, 20, 40) ~ "Medium",
                                Shiny > 40 ~ "High"))
```

This provides the same results as previous, except for the exact value 40 which was assigned as *High* in the `ifelse()` example and to *Medium* in the `case_when()` example. This is due to the fact that the `between()`^[By default, `between(x, value1, value2)` considers value1 <= x <= value2.] function was introduced, although it was not necessary here.

#### Handling Rows

After manipulating columns, the next logical step is to handle rows. Such operations include three aspects, 

1. Re-arranging the rows in a logical way, 
2. Selecting certain rows,
3. Filtering entries based on given variables,
4. Splitting the data in sub-groups based on the entries of a variable.

##### Re-arranging Rows

The first step of re-arranging rows is done through the `arrange()` function from the `{dplyr}` package. This function allows sorting the data in the ascending order^[For numerical order, this is simply re-arranging the values from the lowest to the highest. For strings, the entries are then sorted alphabetically unless the variable is a factor in which case the order of the levels for that factors are being used.]. To arrange them in a descending order, the function `desc()` is also required.

Let's re-arrange the data by Judge and Product, the Judge being sorting in an ascending order whereas the product are being sorted in a descending order:

```{r}
sensory %>% 
  arrange(Judge, desc(Product))
```

##### Selecting Rows

The next step is to select a subset of the data by keeping certain rows only. If the position of the rows to keep is know, this information can be used directly using the `slice()` function. Let's select from `sensory` all the data that is related to `P01`. A quick look at the data informs us that it corresponds to rows 1 to 89, with a step of 11:

```{r}
sensory %>% 
  slice(seq(1,89,11))
```

This is a manual way to select data. However, this procedure may generate an erroneous subset in the case that the row order in the data changes. To avoid mistakes, a more *stable* procedure of filtering data is proposed in the next section. 

##### Filtering Data

To define sub-set of data, the `filter()` function is being used. This function requires providing an argument that is expressed as a *test*, meaning that the outcome should either be TRUE (keep the value) or FALSE (discard the value) when the condition is verified or not respectively. In R, this is expressed by the double '=' sign `==`. 

Let's filter the data to only keep the data related to sample `P02`:


```{r}
sensory %>% 
  filter(Product == "P02")
```


Other relevant test characters are the following:
 - `!Product == "P02"` or `Product != "P02"` means different from, and will keep all samples except `P02`;
 - `%in% my_vector` keeps any value included within the vector `my_vector` (e.g. `Product %in% c("P01","P02","P03")`)
 
In some cases, the tests to perform are more complex as they require multiple conditions. 
There are two forms of conditions:

 - `&` (read *and*) is multiplicative, meaning that all the conditions need to be true (`Product == "P02" & Shiny > 40`);
 - `|` (read *or*) is additive, meaning that only one of the conditions needs to be true (`Product == "P03" | Shiny > 40`)
  
As we will see later, this option is particularly useful when you have missing values as you could remove all the rows that contain missing values for a given variable. Since we do not have missing values here, let's create some by replacing all the evaluations for `Shiny` that are larger than 40 by missing values. This is done here through the `ifelse()`, which takes three arguments (in this order): the test to perform (here `Shiny > 40`), the instruction if the test passes (here replace with `NA`), and the instruction if the test doesn't pass (here keep the value stored in `Shiny`).


```{r}
(sensory_na <- sensory %>% 
  dplyr::select(Judge, Product, Shiny) %>% 
  mutate(Shiny = ifelse(Shiny > 40, NA, Shiny)))

```


In a second step, we filter out all missing values from `Shiny`. In practice, this is done by keeping all the values that are not missing:


```{r}
sensory_na %>% 
  filter(!is.na(Shiny))

```


As we can see, this procedure removed 20 rows since the original table had 99 rows and 3 columns, whereas the filtered table only has 79 rows and 3 columns.


##### Splitting Data


After filtering data, the next logical step is to split data into subsets based on a given variable (e.g. by gender). For such purpose, one could consider using `filter()` by applying it to each subgroup. In a previous example, this is what we have done when we filtered data for sample `P02` only. 

Of course, we could get sub-groups of data for each sample by repeating the same procedure for all the other samples. However, this procedure becomes tedious as the number of samples increases. Instead, we prefer to use `split()` which takes as arguments the data and the column to split from:


```{r}
split(sensory, sensory$Product)

# In a pipe, it would be written as (the '.' calls the data):
#
# sensory %>% split(.$Product)

```


This function creates a list of *n* elements (*n* being the number of samples), each element corresponding to the data related to one sample. Such list can then be used in automated analyses by performing on each sub-data through the `map()` function, as it will be illustrated later. 

### Reshaping the Data


Reshaping the data itself is done through pivoting, hence either creating a long and thin table (CREATE FIGURE), or a short and wide table (CREATE FIGURE). This is done through `pivot_longer()` and `pivot_wider()` from `{tidyr}`.


<!-- Insert Figure illustrating pivot_wider -->

<!-- Insert Figure illustrating pivot_longer -->


#### Pivotting Longer

<!-- Add a section on melt() as special situation of pivotlonger in which all the columns should be pivotted... -->

Currently, our `sensory` data table is a table in which we have as many rows as Judge x Product, the different attributes being spread across multiple columns. However, in certain situations, it is relevant to have all the attributes stacked vertically, meaning that the table will have Judge x Product x Attributes rows. Such simple transformation can be done through the `pivot_longer()` function from the `{dplyr}` package, which takes as inputs the attributes to pivot, the name of the variables that will contain these names (`names_to`), and the name of the column that will contain their entries (`values_to`)


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Score")
```


This transformation converts a table of 99 rows and 34 columns into a table with 3168 (99*32) rows and 4 columns.
In the pivoted table, the names of the variables (stored here in *Attribute*) are in the same order as presented in the original table.

TIPS: With `pivot_longer()` and any other function that requires selecting variables, it is often easier to deselect variables that we do not want to include rather than selecting all the variables of interest. Throughout the book, both solutions are being considered.

In case the attribute names are following a standard structure, say "attribute_name modality" as is the case in `sensory` for some attributes, an additional parameter of `pivot_longer()` becomes handy as it splits the Attribute variable just created into say 'Attribute' and 'Modality.' 

To illustrate this, let's reduce sensory to Judge, Product, and all the variables that end with odor or flavor (all other variables being discarded). After pivoting the subset of columns, we automatically split the attribute names into *Attribute* and *Modality* by informing the separator between names (here, a space):


```{r}
sensory %>% 
  dplyr::select(Judge, Product, ends_with("odor"), ends_with("flavor")) %>% 
  pivot_longer(-c(Judge,Product), names_to=c("Attribute","Modality"), values_to="Score", names_sep=" ")
```


This parameter combines both the power of `pivot_longer()` and `separate()` in one unique process. 
Note that more complex transformations through the use of regular expressions (and the `names_pattern` option) can be considered. More information on this topic is provided in REF CHAPTER TEXTUAL (TO CHECK IF IT IS THE CASE!).

Note that the `{reshape2}` package provides a similar function called `melt()`, which comes particularly handy if the entire set of numerical variables should be pivoted. In case qualitative variables are present in the data set, they will be kept as *id variables*. If performed on a matrix with row names, then the new table will have two columns containing the row and column names.

```{r}
melt(sensory)
```


#### Pivotting Wider


The complementary/opposite function to `pivot_longer()` is `pivot_wider()`. This function pivots data horizontally, hence reducing the number of rows and increasing the number of columns. In this case, the two main parameters to provide is which column will provide the new column names to create (`name_from`), and what are the corresponding values to use (`values_from`). 

From the previous example, we could set `names_from = Attribute` and `values_from = Score` to return to the original format of sensory. However, let's reduce the data set to `Product`, `Judge`, and `Shiny` only, and let's pivot the `Judge` and `Shiny` columns:


```{r}
sensory %>% 
  dplyr::select(Judge, Product, Shiny) %>% 
  pivot_wider(names_from = Judge, values_from = Shiny)
```

 
This procedure creates a table with as many rows as there are products, and as many columns as there are panelists (+1 since the product information is in a column, not defined as row names).

These procedures are particularly useful in consumer studies, since `pivot_longer()` and `pivot_wider()` allows restructuring the data for analysis such as ANOVA (`pivot_longer()` output) and preference mapping or clustering (`pivot_wider()` structure).

Important remarks: Let's imagine the sensory test was performed following an incomplete design, meaning that each panelist did not evaluate all the samples. Although the long and thin structure would not show missing values (the entire rows being removed), the shorter and larger version would contain missing values for the products that each panelist did not evaluate. If the user wants to automatically replace these missing values with a fixed value, say, it is possible through the parameter `values_fill` (e.g. `values_fill=0` would replace each missing value with a 0). Additionally, after pivoting the data, if multiple entries exist for a combination row-column, `pivot_wider()` will return a list of elements. In the next Section, an example illustrating such situation and its solution will be presented.

In previous versions of `pivot_wider()`, the order of the new columns did not match the original order as it would re-organize them alphabetically (mainly if the variable specified in `names_from` was of type character). If this situation is fixed with later updates (if this should still happen to you, try to update` {tidyr}`), there was a work around which consists in transforming this variable as a factor (use `fct_inorder()` to maintain the order as shown in the data) before pivoting it. Indeed, if the variable is of type factor, the new columns are re-ordered based on its levels' order. Such trick is great to keep in mind as it can help in many other situations (such as ordering elements on a graphic for instance!)


### Transformation that Alters the Data


In some cases, the final table to generate requires altering the data, by (say) computing the mean across multiple values, or counting the number of occurrences of factor levels for instance. In other words, we summarize the information, which also tend to reduce the size of the table. It is hence no surprise that the function used for such data reduction is called `summarise()` (`{dplyr}` package).


#### Introduction to Summary Statistics


In practice, `summarise()` applies a function (whether it is the `mean()`, or a simple count using `n()`) on a set of values. Let's compute the mean on all numerical variables of `sensory`:


```{r}
sensory %>% 
  summarise(across(where(is.numeric), mean))
```


As can be seen, the grand mean is computed for each attribute.
If multiple functions should be applied, we could perform all the transformation simultaneously as following:


```{r}
sensory %>% 
  summarise(across(where(is.numeric), list(min=min, max=max)))
```


In this example, each attribute is duplicated with "_min" and "_max" to provide the minimum and maximum value for each attribute. By using a combination of `pivot_longer()` (or `reshape2::melt()`) with `names_sep` followed by `pivot_wider()`, we could easily restructure such table by showing for each attribute (presented in rows) the minimum and the maximum in two different columns.

By following the same principles, many other functions can be performed, whether they are built-in R or created by the user. 
Here is a recommendation of interesting descriptive functions to consider with `summarise()`:

 - `mean()`, `median()` (or more generally `quantile()`) for the mean and median (or any other quantile);
 - `sd()` and `var()` for the standard deviation and the variance;
 - `min()`, `max()`, `range()` (provides both the min and max) or `diff(range())` (for the difference between min and max); 
 - `n()` and `sum()` for the number of counts and the sum respectively. 

#### Introduction to *grouping*

It can appear that the interest is not in the grand mean, but in the mean per product (say), or per product and panelist in case the test has been duplicated. In such cases, `summarize()` should aggregate set of values per product, or per product by panelist respectively. Such information can be passed on through `group_by()`^[We strongly recommend you to `ungroup()` blocks of code that includes `group_by()` once the computations are done to avoid any unexpected results.]. 


```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(across(where(is.numeric), mean)) %>% 
  ungroup()
```


This procedure creates a tibble with 11 rows (product) and 33 columns (32 sensory attributes + 1 column including the product information) which contains the mean per attribute for each sample, also known as the sensory profiles of the products.


#### Illustrations of Data Manipulation


Let's review the different transformations presented earlier by generating the sensory profiles of the samples through different approaches^[It is important to realize that any *data manipulation challenge* can be solved in many different ways, so don't be afraid to think out of the box when solving them...].

In the previous example, we've seen how to obtain the sensory profile using `summarise()` `across()` all numerical variables. In case a selection of the attributes should have been done, we could use the same process by simply informing which attributes to transform:


```{r}
sensory %>% 
  group_by(Product) %>% 
  summarise(across(Shiny:Melting, mean)) %>% 
  ungroup()
```


The list of attributes to include can also be stored in an external vector:


```{r}
sensory_attr <- colnames(sensory)[5:ncol(sensory)]
sensory %>% 
  group_by(Product) %>% 
  summarise(across(all_of(sensory_attr), mean)) %>% 
  ungroup()

```


Remark: It is important to notice that when `group_by()` is being called, the software will remember the groups unless stated otherwise. This means that any subsequent transformation performed on the previous table will be done by product. Such property can be causing unexpected results in case transformations should be performed across all samples. To avoid such behavior, we strongly recommend you to apply `ungroup()` as soon as the results per group have been generated.

A different approach consists in combining `summarise()` to `pivot_longer()` and `pivot_wider()`. This process requires summarizing only one column by Product and Attribute:


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  mutate(Attribute = fct_inorder(Attribute)) %>% 
  group_by(Product, Attribute) %>% 
  summarise(Scores = mean(Scores)) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores) %>% 
  ungroup()
```


Note that through this procedure, `Attribute` was transformed into a factor using `fct_inorder()` to ensure that the double pivoting procedure maintains the original attributes' order. Without this line of code, the final table would have the columns reordered alphabetically.

Another comment concerns the message that R provides here: *`summarise()` has grouped output by 'Product'. You can override using the `.groups` argument.* This message is just informative, and can be hidden by adding the following code at the start of your script:

```{r}
options(dplyr.summarise.inform = FALSE)
```

What would happen if we would omit to `summarise()` the data in between the two pivoting functions? In that case, we also remove Judge which were lost in the process...

```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  dplyr::select(-Judge) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores)
```


As can be seen, each variable is of type list in which each cell contains `dbl [9]`: This corresponds to the scores provided by the 9 panelists to that product and that attribute. Since we would ultimately want the mean of these 9 values to generate the sensory profiles, a solution comes directly within `pivot_wider()` through the parameter `values_fn` which applies the function provided here on each set of values:


```{r}
sensory %>% 
  pivot_longer(Shiny:Melting, names_to="Attribute", values_to="Scores") %>% 
  dplyr::select(-Judge) %>% 
  pivot_wider(names_from=Attribute, values_from=Scores, values_fn=mean)
```


### Combining Data from Different Sources


It often happens that the data to analyze is stored in different files, and need to be combined or merged. Depending on the situations, different solutions are required.

#### Binding by Rows

Let's start with a simple example where the tables match in terms of variables, and should be combined vertically. 
To do so, we use the file *excel-scrap.xlsx* which contains a fake example in which 12 assessors evaluated 2 samples on 3 attributes in triplicate, each replication being stored in a different sheet.

The goal here is to read the data stored in the different sheets, and to combine them in one unique file.

To combine the tables vertically, we could use the basic R function `rbind()`. However, we prefer to use `bind_rows()` from `{dplyr}` since it better controls for the columns by ensuring that the order is respected. And in case one table contains a variable that the other tables don't, this variable will be kept and filled in with NAs when the information is missing.

To keep the distinction between the three tables, the parameter `.id` is used. This will create a column called `Session` in this example that will assign a 1 to the first table, a 2 to the second one, and a 3 to the third one (This process is used to avoid losing this information as it was not directly available within the data: If it were, the parameter `.id` could have been ignored).


```{r}
path <- file.path("data", "excel_scrap.xlsx")

session1 <- read_xlsx(path, sheet=1)
session2 <- read_xlsx(path, sheet=2) 
session3 <- read_xlsx(path, sheet=3)

all_data <- bind_rows(session1, session2, session3, .id = "Session")

```


Although this solution works fine, another neater and tidier solution will be presented in \@ref(import-mult-sheet).

#### Combining Tables by column(s)

Similarly, tables can be combined horizontally using the corresponding function `cbind()` (`{base}`) and/or `bind_cols()` (`{dplyr}`). In this case, it is better to ensure that the rows' order is identical before combining to avoid mishaps, and the tables should be of the same size.

If that is not the case, mergeing tables should be done using `merge()` (`{base}`), or preferably through the different `*_join()` functions from (`{dplyr}`). Depending on the *merging degree* to consider between tables X and Y, there are four different `*_join()` versions to consider:

 - `full_join()` keeps all the cases from X and Y regardless whether they are present in the other table or not (in case they are not present, NAs will be introduced) [it corresponds to `merge(all=TRUE)`];
 - `inner_join()` only keeps the common cases, i.e. cases that are present in both X and Y [corresponds to `merge(all=FALSE)`];
 - `left_join()` keeps all the cases from X [corresponds to `merge(all.x=TRUE, all.y=FALSE)`];
 - `right_join()` keeps all the cases from Y [corresponds to `merge(all.x=FALSE, all.y=TRUE)`];
 - `anti_join()` only keeps the elements from X that are not present in Y (this is particularly useful if you have a tibble Y of elements that you would like to remove from X). 


<!-- Add examples with X=ABC, Y=ABD and full=ABCD, inner=AB, left=ABC, right=ABD, anti=CD  -->


The merging procedure requires the users to provide a *key*, i.e. a (set of) variable(s) used to combine the tables. For each unique element defined by the key, a line is being created. When needed, rows of a table are being duplicated. Within the different `*_join()` functions, the key is informed by the `by` parameter, which may contain one or more variables with the same or different names.

To illustrate, let's use the data set called *Consumer Test.xlsx*, which contains three tabs:


```{r}
file_path <- here("data","Consumer Test.xlsx")

excel_sheets(file_path)

```


The three sheets contain the following information, which need to be combined:

 - *Biscuits*: The consumers' evaluation of the 10 products and their assessment on liking, hunger, etc. at different moments of the test.
 - *Time Consumption*: The amount of cookies and the time required to evaluate them in each sitting.
 - *Weight*: The weight associated to each cookie.


Let's start by combining *Time Consumption* and *Weight* so that we can compute the total weight of biscuits eaten by each respondent in each sitting. In this case, the joining procedure is done by `Product` since the weight is only provided for each product. The total weight eaten (`Amount`) is then computed by multiplying the number of cookies eaten (`Nb biscuits`) by `Weight`


```{r}
time <- read_xlsx(file_path, sheet="Time Consumption")
weight <- read_xlsx(file_path, sheet="Weight")

(consumption <- time %>% 
  full_join(weight, by="Product") %>% 
  mutate(Amount = `Nb biscuits`*Weight))
```


As can be seen, the `Weight` information stored in the *Weight* sheet has been replicated every time each sample has been evaluated by another respondent. 

The next step is then to merge this table to `Biscuits`. In this case, since both data set contain the full evaluation of the cookies (each consumer evaluating each product), the joining procedure needs to be done by `Judge` and `Product` simultaneously. A quick look at the data shows two important things:

 - In *Biscuits*, the consumer names only contains the numbers whereas in `consumption`, they also contain a `J` in front of the name: This needs to be fixed as the names need to be identical to be merged, else they will be considered separately and NAs will be introduced. In practice, this will be done by mutating `Consumer` and by pasting a `J` in front of the number using the function `paste0()`.
 - The names that contain the product (`Samples` and `Product`) and consumers (`Consumer` and `Judge`) information are different in both data set. We could rename these columns in one data set to match the other, but instead we will keep the two names and inform it within `full_join()`. This is done through the `by` parameter as following: `"name in dataset 1" = "name in dataset 2"`
 
 
```{r}
biscuits <- read_xlsx(file_path, sheet="Biscuits") %>% 
  mutate(Consumer = str_c("J",Consumer)) %>% 
  full_join(consumption, by=c("Consumer"="Judge", "Samples"="Product"))

biscuits
```


The three data sets are now flawlessly joined into one and can be further analysed.
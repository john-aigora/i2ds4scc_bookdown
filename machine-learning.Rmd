```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

```{r load_utilities, message=FALSE, warning=FALSE}

# Install and load libraries ----------------------------------------------

library(tidyverse)
library(rattle) # wine dataset
require(factoextra)
library(tidymodels)
library(ranger)
library(ggfortify) # PCA visualization
library(cluster)

# load data

wine.fl <- "http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.fl,header = F)
# Names of the variables
wine.names=c("Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium",
             "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins",
             "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
colnames(wine)[2:14]=wine.names
colnames(wine)[1]="Type"

```

# Machine Learning {#machine-learning}
 

## Methods Overview

### Unsupervised learning

```{r prepare_data}

# We drop type, to present possibility of generating the target using unsupervised methods

set.seed(123)
wine_dataset <- tibble(wine) %>% 
  select(-Type) 

```
#### Cluster analysis

<!-- what's the role of cluster analysis in ML -->
<!-- cluster analysis in consumer analysis as semisupervised learning -->

Aim of cluster analysis is to discover in high-dimensional data groups of
observations that are similar to each other in the group and significantly
different from the rest of observations. Common application in consumer science
is to cluster consumers based on their taste preference to understand and target
specific segments.


<!-- #### Factor analysis -->
<!-- #### Principle components analysis -->

<!-- Tian comments: Why do we need PCA before cluster analysis? Or in what situation -->
<!-- a PCA is beneficial before clustering? -->


comparing PCA with and without scaling

```{r pca_without_scaling}
pca_results_ws <- wine_dataset %>%
  prcomp()

summary(pca_results_ws)
autoplot(pca_results_ws)

pca_ws_plot <- autoplot(pca_results_ws, data = wine_dataset,
                   loadings = TRUE, loadings.colour = 'red',
                   loadings.label = TRUE, loadings.label.size = 3) +
  theme_minimal()

pca_ws_plot

```

```{r pca_with_scaling}
# scaling data + PCA -----------------------------------------------------------

pca_results <- wine_dataset %>%
  prcomp(scale. = TRUE)

summary(pca_results)
plot(pca_results)

pca_plot <- autoplot(pca_results, data = wine_dataset,
               loadings = TRUE, loadings.colour = 'red',
               loadings.label = TRUE, loadings.label.size = 3) +
  theme_minimal()

pca_plot

reduced_dataset <- data.frame(pca_results$x[, 1:2]) %>%
  tibble()
# we can see that clustering should be easy

```

<!-- Clustering  -->
<!-- Using Elbow method to find the optimum number of cluster numbers -->

<!-- kmeans vs hierarchical clustering, is nbclust only valid for kmeans? -->

```{r find_opt_cluster_number}

wcss <- tibble()
for (i in 1:10) {
  
  wcss <- wcss %>% 
    bind_rows(tibble(n_clusters = i,
                     wcss = kmeans(reduced_dataset, i)$tot.withinss))
  
}

ggplot(data=wcss, aes(x=n_clusters, y=wcss, group=1)) +
  geom_line(size = 1.5)+
  geom_point(shape=21, fill="blue", color="darkred", size=5) +
  theme_minimal() +
  xlab("Number of clusters") +
  ylab("Total within-cluster sum of squares")

# Finding number of clusters - silhouette method
fviz_nbclust(reduced_dataset, kmeans, method = "silhouette")

# 3 clusters seems to be good fit

```

<!-- Tian comments:from sensory consumer data analysis point of view, when to  -->
<!-- use kmeans, when to use hierarchical? etc. -->
```{r kmeans_cluster_analysis}

km_dw <- kmeans(reduced_dataset, 3, nstart = 20)

km_dw

fviz_cluster(
  list(data = reduced_dataset, cluster = km_dw$cluster),
  ellipse.type = "norm",
  geom = "point",
  stand = FALSE
)

```


<!-- #### t-SNE -->
<!-- ### Semisupervised learning -->
<!-- #### PLS regression -->
<!-- #### Cluster Characterization -->
### Supervised learning

#### Regression

Regression methods approximate the target variable^[This is a bit of simplification. 
In some cases it is some transformation of combination of predictors that 
approximates target variable. An example of this is logistic regression.]
with (usually linear) combination of predictor variables. 

There are multiple regression algorithms varying by type of data they can handle,
type of target variable and additional aspects like ability to perform 
dimensionality reduction. We will take a walk through the ones most relevant for 
sensory and consumer science.

##### Linear regression

The simplest and most popular variant is linear regression in which continuous 
target variable is approximated as linear combination of predictors in a way that 
minimizes sum of squared estimates of errors (SSE). It can be for example used to 
predict consumer liking of a product based on it's sensory profile, but user has
to keep in mind that linear regression can in some cases return value outside 
reasonable range of target values. This can be addressed by capping predictions 
to desired range. Functions in R to apply linear regression are: `lm()` and `glm()`
or `parsnip::linear_reg() %>% parsnip::set_engine("lm")` when using `tidymodels` workflow.

##### Logistic regression

Logistic regression is an algorithm which by use of logistic transformation allows
to apply the same approach as linear regression to cases with binary target variables.
It can be used in R with `glm(family = "binomial")` or
`parsnip::logistic_reg() %>% parsnip::set_engine("glm")` when using `tidymodels` workflow.

##### Penalized regression 

It is often that the data we want to use for modeling have a lot of predictor 
variables, possibly with lot of high correlations. In such cases linear/logistic
regression may become unstable and produce unreasonable predictions. This can be
addressed by use of so called penalized regression. It is a special case where 
instead of minimizing pure error term, algorithm minimizes both error and regression 
coefficients at the same time. This leads to more stable predictions.

There are three variations of penalized regression and all of them can be accessed
via function `glmnet::glmnet()` ($\beta$ is set of regression coefficients and 
$\lambda$ is a parameter to be set by user or determined from cross-validation):

+ Ridge regression (L2 penalty) minimizes $SSE + \lambda \sum|\beta|^2$ and drives
the coefficients to smaller values
+ Lasso regression (L1 penalty) $SSE + \lambda \sum|\beta|$ and forces some of the
coefficients to vanish what can be used for variable selection
+ Elastic-net regression is a combination of the two previous variants 
$SSE + \lambda_1 \sum|\beta| + \lambda_2 \sum|\beta|^2$.

Penalized regression can be also runned in `tidymodels` workflow with 
or `parsnip::linear_reg() %>% parsnip::set_engine("glmnet")`.

##### MARS

One limitation of all mentioned so far methods is that they assume linear relationship
between predictor and target variables. Multivariate adaptive regression spline (MARS)
addresses this by modeling nonlinearities with piecewise linear function. This
gives a nice balance between simplicity and ability to fit complex data, for example
$\Lambda$-shaped once where there is a maximal point from which function decreases 
in both directions. In R this model can be accesed via `earth::earth()` function.

##### PLS

In case of multiple target variables one can apply partial least squares (PLS) regression
which, similarly to PCA looks for components that maximizes explained variance of
the predictors, but at the same time also maximizes their correlation to target variables.
PLS can be applied with `lm()` specifying multiple targets or in `tidymodels` workflow
with `plsmod::pls() %>% parsnip::set_engine("mixOmics")`.

#### K-nearest neighbors

A very simple, yet useful and robust algorithm that works for both numeric and nominal
target variables is K-nearest neighbors. The idea is that for every new observation
we want to predict the algorithms finds K closest points in training set and use
either their mean value (for numeric targets) or most frequent value (for nominal targets)
as prediction. This algorithm can be used with `kknn::kknn()` function or in `tidymodels` workflow
with `parsnip::nearest_neighbor() %>% parsnip::set_engine("kknn")`.

#### Decision trees

Decision tree models the data by splitting the training set in smaller subsets 
in a way that each split is done by a predictor variable so that it maximizes 
the difference in target variable between the subsets. One important advantage 
of decision trees is that they can model complex relationships and interactions
between predictors. To use decision tree in R one can use `rpart::rpart()` or
in `tidymodels` workflow with `parsnip::decision_tree() %>% parsnip::set_engine("rpart")`.

#### Black boxes

So called black boxes are class of models that have too complex structure to directly
interpret relationship between predictor variables and a value predicted by the model.
Their advantage usually is ability to model more complicated data than in case of
interpretable models, but they have greater risk of overfitting (fitting to
noise in training data). Also, lack of clear interpretation may be not acceptable
in some business specific use cases. The later problem can be addressed by use of 
explanation algorithms that will be discussed in later part of this chapter.

##### Random forests

A random forest is a set of decision trees, each one trained on random subset 
of observations and/or predictors. The final prediction is an average of individual
trees' predictions. This way 





##### SVMs
##### Neural networks
##### Computer vision

## Key Topics

### Model Validation

### Interpretability 
#### LIME
#### DALEX
#### IML

## Common Applications
### Predicting sensory profiles from instrumental data
### Predicting consumer response from sensory profiles
### Characterizing consumer clusters

## Code


<!-- introduce tidymodel -->

```{r classification_Aigora_ml_training}


# Classification ----------------------------------------------------------

# Dataset preparation
wine_classification_dataset <- reduced_dataset %>% 
  bind_cols(tibble(Wine_type = km_dw$cluster)) %>% # Our classification label is the clustering output
  mutate(Wine_type = as.factor(Wine_type))

initial_split <- initial_split(data = wine_classification_dataset, strata = "Wine_type", prop = 0.7)

wine_train <- training(initial_split)
wine_testing <- testing(initial_split)

wine_cv <- wine_train %>% vfold_cv(5,strata = Wine_type)

# Random forest model definition
model_recipe <- wine_train %>% 
  recipe(Wine_type ~ .)

rf_spec <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()) %>%
  set_mode("classification") %>% 
  set_engine(engine = "ranger")

rf_wf <- workflow() %>%
  add_recipe(model_recipe) %>% 
  add_model(rf_spec)


# Best parameters searching

params_grid <- rf_spec %>%
  parameters() %>%
  update(mtry = mtry(range = c(1, 2)),
         trees = trees(range = c(10, 200))) %>% 
  grid_regular(levels = 5)

tuning <- tune_grid(
  rf_wf,
  resamples = wine_cv,
  grid = params_grid
)

autoplot(tuning)

params_best <- select_best(tuning, "roc_auc")

# Finalize model 

final_model <- rf_wf %>%
  finalize_workflow(params_best) %>%
  fit(wine_train)

validation_data_pred <- wine_testing %>%
  bind_cols(predict(final_model, .))

cm <- conf_mat(validation_data_pred, Wine_type, .pred_class)

autoplot(cm, type = "heatmap")
```

```{r regression_data_generation_Aigora_ml_training}
library(tidyverse)

n_rows <- 100

with(set.seed(1),
     data <- tibble(
       id = sprintf("product_%d", 1:n_rows)
     ) %>%
       mutate(
         sensory_1 = runif(n(), 0, 6),
         sensory_2 = runif(n(), 4, 8),
         sensory_3 = pmax(0, pmin(10, rnorm(n(), 6, 1.5))),
         sensory_4 = runif(n(), 0, 2),
         sensory_5 = runif(n(), 0, 2),
         sensory_6 = pmax(0, pmin(10, rnorm(n(), 3, 1))),
         sensory_7 = runif(n(), 3, 10)
       ) %>%
       mutate(
         liking = 
           - 0.2 * abs(sensory_1-2.5)
         + 0.6 * sensory_2
         + 0.4 * sensory_3
         + 0.2 * abs(sensory_4-1)
         - 0.3 * sensory_5
         + 0.1 * sensory_6
         + 0.2 * sensory_7
       ) %>%
       mutate(liking = pmin(9.74, rnorm(n(), 1, 0.1) * liking))
)

summary(data)

# write_rds(data, "data/regression_data.rds")

```

```{r regression_Aigora_ml_training}
# Tutorial content:
#   1. Split data
#   2. Define model
#   3. Tune hyperparameters
#   4. Visualize results
#   5. Explore model with DALEX and modelStudio


# Install and load libraries ----------------------------------------------

# install.packages(c("tidyverse", "tidymodels", "remotes",
#                    "DALEXtra", " modelStudio", "r2d3"))
# remotes::install_github("aigorahub/aigoraOpen")
library(tidyverse)
library(tidymodels)
library(aigoraOpen)


# Load data ---------------------------------------------------------------

# data <- read_rds("data/regression_data.rds")

set.seed(123)

data_split <- initial_split(data)

training_data <- training(data_split)
validation_data <- testing(data_split)

resampling <- vfold_cv(training_data, v = 10)

# Define model ------------------------------------------------------------

model_recipe <- training_data %>%
  select(-id) %>% # id shouldn't be used by the model
  recipe(liking ~ .) %>%
  step_earth(all_predictors(), outcome = "liking")

model_spec <- linear_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

model_workflow <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(model_spec)


# Tune model hyperparameters (grid search) ---------------------------------

set.seed(124)

params_grid <- model_spec %>%
  parameters() %>%
  grid_regular(levels = 10)

tuning <- tune_grid(
  model_workflow,
  resamples = resampling,
  grid = params_grid
)

autoplot(tuning)

params_best <- select_best(tuning, "rmse")


# Zoom in grid search -----------------------------------------------------

set.seed(125)

penalty_levels <- sort(unique(params_grid$penalty))
mixture_levels <- sort(unique(params_grid$mixture))

penalty_new_range <- c(
  penalty_levels[lead(penalty_levels, default = 0) == params_best$penalty],
  penalty_levels[lag(penalty_levels, default = 0) == params_best$penalty]
)
mixture_new_range <- c(
  mixture_levels[lead(mixture_levels, default = 0) == params_best$mixture],
  mixture_levels[lag(mixture_levels, default = 0) == params_best$mixture]
)

params_grid_2 <- model_spec %>%
  parameters() %>%
  update(
    penalty = penalty(log10(penalty_new_range)),
    mixture = mixture(mixture_new_range)
  ) %>%
  grid_regular(levels = 10)

tuning_2 <- tune_grid(
  model_workflow,
  resamples = resampling,
  grid = params_grid_2
)

autoplot(tuning_2)

params_best_2 <- select_best(tuning_2, "rmse")


# Finalize model ----------------------------------------------------------

final_model <- model_workflow %>%
  finalize_workflow(params_best_2) %>%
  fit(training_data)

validation_data_pred <- validation_data %>%
  bind_cols(predict(final_model, .))

metric_set(rmse, rsq)(validation_data_pred, liking, .pred)

validation_data_pred %>%
  ggplot(aes(liking, .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = 2)


# Model Studio ------------------------------------------------------------

library(DALEXtra)
library(modelStudio)

explainer <- explain_tidymodels(
  final_model, 
  data = training_data %>% 
    column_to_rownames("id") %>%
    select(-liking),
  y = training_data$liking
)

resid <- model_performance(explainer)
resid
plot(resid)

var_imp <- variable_importance(explainer)
plot(var_imp)

ms <- modelStudio(
  explainer,
  new_observation = validation_data %>%
    column_to_rownames("id") %>%
    select(-liking),
  new_observation_y = round(validation_data$liking, 3)
)

# r2d3::save_d3_html(ms, "output/modelstudio.html")

```

